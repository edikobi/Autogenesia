# ü§ñ AI Code Agent - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –¢–µ—Å—Ç

**–î–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** 17.12.2025 17:10:20
**–ü—Ä–æ–µ–∫—Ç:** `C:\Users\Admin\AI_Assistant_Pro`
**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** 200.10 —Å–µ–∫.

---

## üìù –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

> –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ–∞–π–ª general_web_search.py –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏, –≤ —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –µ—Å—Ç—å, –Ω–∞–ø–∏—à–∏ –∫–æ–¥ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–π, –≥–¥–µ –ø—Ä–æ–±–ª–µ–º–∞

---

## üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

- **Orchestrator:** Claude Opus 4.5 (RouterAI)
- **Code Generator:** deepseek-chat

---

## üîç –ê–Ω–∞–ª–∏–∑ –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞

[–ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω]

---

## üìã –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è Code Generator

**Task:** –ò—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å—é –≤ —Ñ—É–Ω–∫—Ü–∏–∏ `general_web_search_tool`, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ –≤ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º async-–∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.

**File:** `app/tools/general_web_search.py`

**Location:** –§—É–Ω–∫—Ü–∏—è `general_web_search_tool` (—Å—Ç—Ä–æ–∫–∏ 39-73)

**Changes:**

1. **–î–æ–±–∞–≤–∏—Ç—å –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é `_run_async` –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–æ–≤ (–ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ 13, –ø–µ—Ä–µ–¥ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞–º–∏ –Ω–∞ —Å—Ç—Ä–æ–∫–µ 15)**:
   
   –°–æ–∑–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `_run_async(coro)`, –∫–æ—Ç–æ—Ä–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ—Ä—É—Ç–∏–Ω—É:
   - –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π event loop —á–µ—Ä–µ–∑ `asyncio.get_running_loop()`
   - –ï—Å–ª–∏ loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω (`RuntimeError` –Ω–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `concurrent.futures.ThreadPoolExecutor` –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∫–æ—Ä—É—Ç–∏–Ω—ã –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Å –Ω–æ–≤—ã–º event loop
   - –ï—Å–ª–∏ loop –Ω–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `asyncio.run(coro)` (Python 3.7+) ‚Äî —ç—Ç–æ –±–æ–ª–µ–µ —á–∏—Å—Ç—ã–π —Å–ø–æ—Å–æ–±, —á–µ–º —Ä—É—á–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ/–∑–∞–∫—Ä—ã—Ç–∏–µ loop

   –ö–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏:
   ```python
   def _run_async(coro):
       """
       –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ—Ä—É—Ç–∏–Ω—É –∏–∑ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
       –†–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∫–∞–∫ –∏–∑ –æ–±—ã—á–Ω–æ–≥–æ –∫–æ–¥–∞, —Ç–∞–∫ –∏ –∏–∑ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ event loop.
       """
       try:
           loop = asyncio.get_running_loop()
       except RuntimeError:
           # –ù–µ—Ç –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ loop ‚Äî –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.run()
           return asyncio.run(coro)
       
       # –ï—Å–ª–∏ loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –≤—ã–ø–æ–ª–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
       with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
           future = executor.submit(asyncio.run, coro)
           return future.result()
   ```

2. **–ó–∞–º–µ–Ω–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ—É–Ω–∫—Ü–∏–∏ `general_web_search_tool` (—Å—Ç—Ä–æ–∫–∏ 39-73)**:
   
   –ó–∞–º–µ–Ω–∏—Ç—å –±–ª–æ–∫ try —Å —Ä—É—á–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º event loop –Ω–∞ –≤—ã–∑–æ–≤ `_run_async`:
   
   ```python
   def general_web_search_tool(query: str, max_results: int = 10, time_limit: str = "w", region: str = "ru-ru") -> str:
       """
       –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –¥–ª—è –æ–±—â–∏—Ö, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
       
       Args:
           query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
           max_results: –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–¥–æ 10).
           time_limit: –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏ ('d' - –¥–µ–Ω—å, 'w' - –Ω–µ–¥–µ–ª—è, 'm' - –º–µ—Å—è—Ü, 'y' - –≥–æ–¥, None - –≤—Å–µ –≤—Ä–µ–º—è).
           region: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'ru-ru' –¥–ª—è –†–§).
       """
       if not query:
           return format_error("Query is required")

       max_results = min(max_results, 10)
       
       try:
           result = _run_async(async_general_web_search(query, max_results, time_limit, region))
               
           if not result.success:
               return format_error(result.error or "Search failed")
               
           if not result.pages:
               return format_no_results(query)
               
           return format_results_xml(result)
           
       except Exception as e:
           logger.error(f"General web search error: {e}")
           return format_error(f"Search failed: {e}")
   ```

**Why:** –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π event loop –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –µ–≥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ `asyncio.set_event_loop()`, —á—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –∏–∑ —É–∂–µ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ async-–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ FastAPI, aiohttp, –∏–ª–∏ async-—Ñ—É–Ω–∫—Ü–∏–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞). –ù–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –∫–æ—Ä—É—Ç–∏–Ω—ã.

---

## üíª –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥

**–§–∞–π–ª:** `app/tools/general_web_search.py`

```python
from __future__ import annotations
import asyncio
import concurrent.futures
import logging
import re
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from urllib.parse import urlparse, unquote
import httpx
from app.utils.token_counter import TokenCounter
from config.settings import cfg

logger = logging.getLogger(__name__)

def _run_async(coro):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ—Ä—É—Ç–∏–Ω—É –∏–∑ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    –†–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∫–∞–∫ –∏–∑ –æ–±—ã—á–Ω–æ–≥–æ –∫–æ–¥–∞, —Ç–∞–∫ –∏ –∏–∑ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ event loop.
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        # –ù–µ—Ç –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ loop ‚Äî –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.run()
        return asyncio.run(coro)
    
    # –ï—Å–ª–∏ loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –≤—ã–ø–æ–ª–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(asyncio.run, coro)
        return future.result()

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–º–∏—Ç –∏–∑ settings –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
MAX_TOTAL_TOKENS = 25000  
REQUEST_TIMEOUT = 15.0
MAX_CONTENT_PER_PAGE = 4000 # –ß—É—Ç—å –º–µ–Ω—å—à–µ –¥–ª—è —Å—Ç–∞—Ç–µ–π, —á—Ç–æ–±—ã –≤–º–µ—Å—Ç–∏—Ç—å –±–æ–ª—å—à–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

@dataclass
class GeneralWebPage:
    url: str
    title: str
    snippet: str
    content: str = ""
    tokens: int = 0
    relevance_score: float = 0.0
    error: Optional[str] = None
    published_date: Optional[str] = None # –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π

@dataclass
class GeneralWebSearchResult:
    success: bool
    query: str
    pages: List[GeneralWebPage] = field(default_factory=list)
    total_tokens: int = 0
    error: Optional[str] = None

def general_web_search_tool(query: str, max_results: int = 10, time_limit: str = "w", region: str = "ru-ru") -> str:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –¥–ª—è –æ–±—â–∏—Ö, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        max_results: –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–¥–æ 10).
        time_limit: –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏ ('d' - –¥–µ–Ω—å, 'w' - –Ω–µ–¥–µ–ª—è, 'm' - –º–µ—Å—è—Ü, 'y' - –≥–æ–¥, None - –≤—Å–µ –≤—Ä–µ–º—è).
        region: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'ru-ru' –¥–ª—è –†–§).
    """
    if not query:
        return format_error("Query is required")

    max_results = min(max_results, 10)
    
    try:
        result = _run_async(async_general_web_search(query, max_results, time_limit, region))
            
        if not result.success:
            return format_error(result.error or "Search failed")
            
        if not result.pages:
            return format_no_results(query)
            
        return format_results_xml(result)
        
    except Exception as e:
        logger.error(f"General web search error: {e}")
        return format_error(f"Search failed: {e}")

async def async_general_web_search(query: str, max_results: int, time_limit: str, region: str) -> GeneralWebSearchResult:
    # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ DDG —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ–≥–∏–æ–Ω–∞
    search_results = await duckduckgo_search(query, max_results * 2, time_limit, region)
    
    if not search_results:
        return GeneralWebSearchResult(success=False, query=query, error="No search results found")

    # 2. –°–∫–∞—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    pages = await fetch_pages_parallel(search_results, max_results)

    # 3. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (Pseudo-Semantic Jaccard)
    pages = calculate_relevance_scores(pages, query)
    
    # 4. –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
    pages.sort(key=lambda p: p.relevance_score, reverse=True)

    # 5. –û—Ç–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ, –ø–æ–∫–∞ –≤–ª–µ–∑–∞–µ–º –≤ –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
    selected_pages = select_within_token_limit(pages, MAX_TOTAL_TOKENS)
    
    total_tokens = sum(p.tokens for p in selected_pages)
    
    return GeneralWebSearchResult(
        success=True,
        query=query,
        pages=selected_pages,
        total_tokens=total_tokens
    )

async def duckduckgo_search(query: str, num_results: int, time_limit: str, region: str) -> List[Dict[str, str]]:
    """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ HTML –≤–µ—Ä—Å–∏—é DDG —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
    search_url = "https://html.duckduckgo.com/html/"
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã DDG
    # df: w (week), m (month), d (day), y (year)
    params = {
        'q': query,
        'kl': region, # region settings (ru-ru)
    }
    if time_limit and time_limit in ['d', 'w', 'm', 'y']:
        params['df'] = time_limit

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7" # –í–∞–∂–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    }

    try:
        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
            response = await client.post(search_url, data=params, headers=headers)
            
            if response.status_code != 200:
                logger.warning(f"DDG returned status {response.status_code}")
                return []

            return parse_ddg_html(response.text, num_results)
    except Exception as e:
        logger.error(f"DDG search error: {e}")
        return []

def parse_ddg_html(html: str, max_results: int) -> List[Dict[str, str]]:
    results = []
    # –ß—É—Ç—å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π regex –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    result_pattern = re.compile(r'<a[^>]*class="[^"]*result__a[^"]*"[^>]*href="([^"]+)"[^>]*>(.*?)</a>', re.IGNORECASE)
    snippet_pattern = re.compile(r'<a[^>]*class="[^"]*result__snippet[^"]*"[^>]*>(.*?)</a>', re.IGNORECASE)
    
    matches = result_pattern.findall(html)
    snippets = snippet_pattern.findall(html)
    
    for i, (url, title) in enumerate(matches):
        if i >= max_results:
            break
            
        actual_url = extract_actual_url(url)
        if not is_valid_url(actual_url):
            continue
            
        snippet = snippets[i] if i < len(snippets) else ""
        
        # –û—á–∏—Å—Ç–∫–∞ HTML —Ç–µ–≥–æ–≤ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ —Å–Ω–∏–ø–ø–µ—Ç–∞
        title = remove_html_tags(title)
        snippet = remove_html_tags(snippet)
        
        results.append({
            "url": actual_url,
            "title": title.strip(),
            "snippet": snippet.strip()
        })
        
    return results

def remove_html_tags(text: str) -> str:
    return re.sub(r'<[^>]+>', '', text)

def extract_actual_url(ddg_url: str) -> str:
    if "uddg=" in ddg_url:
        match = re.search(r'uddg=([^&]+)', ddg_url)
        if match:
            return unquote(match.group(1))
    return ddg_url

def is_valid_url(url: str) -> bool:
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return False
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º PDF –∏ –±–∏–Ω–∞—Ä–Ω–∏–∫–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —è–≤–Ω–æ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if any(url.lower().endswith(ext) for ext in ['.pdf', '.doc', '.docx', '.xls', '.zip']):
            return False 
        return True
    except:
        return False

async def fetch_pages_parallel(search_results: List[Dict[str, str]], max_results: int) -> List[GeneralWebPage]:
    tasks = [fetch_single_page(r) for r in search_results]
    pages = await asyncio.gather(*tasks)
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –∏ –æ—à–∏–±–æ—á–Ω—ã–µ
    valid_pages = [p for p in pages if p.content and not p.error]
    return valid_pages[:max_results]

async def fetch_single_page(result: Dict[str, str]) -> GeneralWebPage:
    url = result['url']
    try:
        content = await fetch_page_content(url)
        if not content:
            return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error="Empty content")
            
        counter = TokenCounter()
        tokens = counter.count(content)
        
        # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ, –æ–±—Ä–µ–∑–∞–µ–º, –Ω–æ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ
        if tokens > MAX_CONTENT_PER_PAGE:
            content = truncate_content(content, MAX_CONTENT_PER_PAGE, counter)
            tokens = MAX_CONTENT_PER_PAGE
            
        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], content=content, tokens=tokens)
    except Exception as e:
        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error=str(e))

async def fetch_page_content(url: str) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8"
    }
    try:
        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            if response.status_code != 200:
                return ""
            return extract_text_from_html(response.text)
    except:
        return ""

def extract_text_from_html(html: str) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ç–∞—Ç–µ–π"""
    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏, –º–µ—Ç—Ä–∏–∫–∏
    html = re.sub(r'<(script|style|svg|nav|footer|header|aside)[^>]*>.*?</\1>', ' ', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)
    
    # –ó–∞–º–µ–Ω—è–µ–º –±–ª–æ—á–Ω—ã–µ —Ç–µ–≥–∏ –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    html = re.sub(r'</?(p|div|br|h[1-6]|li|tr)[^>]*>', '\n', html, flags=re.IGNORECASE)
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏
    text = re.sub(r'<[^>]+>', '', html)
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML —Å—É—â–Ω–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç–æ –±–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞)
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&quot;', '"').replace('&lt;', '<').replace('&gt;', '>')
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    
    return text.strip()

def truncate_content(content: str, max_tokens: int, counter: TokenCounter) -> str:
    # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–µ–∑–∫–∞, –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –¥–æ –æ–±—Ä–µ–∑–∫–∏ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
    # –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–æ—Å—Ç–æ —Ä–µ–∂–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω) –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
    approx_chars = max_tokens * 4
    if len(content) > approx_chars:
        content = content[:approx_chars]
    return content

def calculate_relevance_scores(pages: List[GeneralWebPage], query: str) -> List[GeneralWebPage]:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Jaccard Similarity (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤ —Å–ª–æ–≤).
    –≠—Ç–æ –ª—É—á—à–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ–π count, —Ç–∞–∫ –∫–∞–∫ —É—á–∏—Ç—ã–≤–∞–µ—Ç '–ø–æ–∫—Ä—ã—Ç–∏–µ' –∑–∞–ø—Ä–æ—Å–∞ —Ç–µ–∫—Å—Ç–æ–º.
    """
    query_words = set(re.findall(r'\w+', query.lower()))
    if not query_words:
        return pages

    for page in pages:
        score = 0.0
        
        # 1. Title Score (–û—á–µ–Ω—å –≤–∞–∂–Ω–æ)
        title_words = set(re.findall(r'\w+', page.title.lower()))
        if title_words:
            intersection = query_words.intersection(title_words)
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            score += (len(intersection) / len(query_words)) * 3.0 # –í–µ—Å 3.0

        # 2. Snippet Score
        snippet_words = set(re.findall(r'\w+', page.snippet.lower()))
        if snippet_words:
            intersection = query_words.intersection(snippet_words)
            score += (len(intersection) / len(query_words)) * 1.5 # –í–µ—Å 1.5

        # 3. Content Score (–ø–ª–æ—Ç–Ω–æ—Å—Ç—å –≤ –Ω–∞—á–∞–ª–µ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∂–Ω–µ–µ)
        content_preview = page.content[:1000].lower() # –°–º–æ—Ç—Ä–∏–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
        content_words = set(re.findall(r'\w+', content_preview))
        if content_words:
            intersection = query_words.intersection(content_words)
            score += (len(intersection) / len(query_words)) * 1.0

        page.relevance_score = score

    return pages

def select_within_token_limit(pages: List[GeneralWebPage], limit: int) -> List[GeneralWebPage]:
    selected = []
    current_tokens = 0
    for page in pages:
        if current_tokens + page.tokens <= limit:
            selected.append(page)
            current_tokens += page.tokens
        else:
            # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –≤–ª–µ–∑–∞–µ—Ç —Ü–µ–ª–∏–∫–æ–º, –Ω–æ –º–µ—Å—Ç–æ –µ—â–µ –µ—Å—Ç—å (>500 —Ç–æ–∫–µ–Ω–æ–≤), –±–µ—Ä–µ–º –∫—É—Å–æ–∫
            remaining = limit - current_tokens
            if remaining > 500:
                # –¢—É—Ç –Ω—É–∂–µ–Ω truncate, –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ
                # –í —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ –ª—É—á—à–µ –≤—ã–∑–≤–∞—Ç—å truncate_content
                page.content = page.content[:remaining * 4] 
                page.tokens = remaining
                selected.append(page)
            break
    return selected

def format_results_xml(result: GeneralWebSearchResult) -> str:
    parts = []
    parts.append(f"<!-- Web search results for '{result.query}' -->")
    parts.append(f"<websearch query='{result.query}' tokens='{result.total_tokens}'>")
    for i, page in enumerate(result.pages, 1):
        parts.append(f"  <page rank='{i}' relevance='{page.relevance_score:.2f}' url='{page.url}'>")
        parts.append(f"    <title>{page.title}</title>")
        parts.append(f"    <content><![CDATA[{page.content}]]></content>")
        parts.append(f"  </page>")
    parts.append("</websearch>")
    return "\n".join(parts)

def format_error(msg: str) -> str:
    return f"<!-- ERROR -->\n<error message='{msg}'/>"

def format_no_results(query: str) -> str:
    return f"<!-- No results -->\n<websearch query='{query}'><message>No results found</message></websearch>"
```

---

## üìñ –ü–æ—è—Å–Ω–µ–Ω–∏—è –∫ –∫–æ–¥—É

–í–Ω–µ—Å–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å—é –≤ —Ñ—É–Ω–∫—Ü–∏–∏ `general_web_search_tool`. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å—Ç–∞—Ä–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–∫–ª—é—á–∞–ª–∞—Å—å –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∞ —Å–æ–∑–¥–∞–≤–∞–ª–∞ –Ω–æ–≤—ã–π event loop –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª–∞ –µ–≥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ `asyncio.set_event_loop()`, —á—Ç–æ –≤—ã–∑—ã–≤–∞–ª–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –∏–∑ —É–∂–µ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ async-–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ FastAPI, aiohttp –∏–ª–∏ –¥—Ä—É–≥–∏—Ö async-—Ñ—É–Ω–∫—Ü–∏–π).

**–ß—Ç–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ:**

1. **–î–æ–±–∞–≤–ª–µ–Ω–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è `_run_async(coro)`**:
   - –§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–º–µ—â–µ–Ω–∞ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–æ–≤, –ø–µ—Ä–µ–¥ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞–º–∏ (–∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
   - –û–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∑–∞–ø—É—â–µ–Ω –ª–∏ —É–∂–µ event loop –≤ —Ç–µ–∫—É—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
   - –ï—Å–ª–∏ loop –Ω–µ –∑–∞–ø—É—â–µ–Ω (RuntimeError), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `asyncio.run(coro)` ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ø–æ—Å–æ–±
   - –ï—Å–ª–∏ loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `ThreadPoolExecutor` –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∫–æ—Ä—É—Ç–∏–Ω—ã –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Å –Ω–æ–≤—ã–º event loop

2. **–ò–∑–º–µ–Ω–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è `general_web_search_tool`**:
   - –£–¥–∞–ª–µ–Ω –±–ª–æ–∫ —Å —Ä—É—á–Ω—ã–º —Å–æ–∑–¥–∞–Ω–∏–µ–º –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º event loop (`asyncio.new_event_loop()`, `asyncio.set_event_loop()`, `loop.run_until_complete()`, `loop.close()`)
   - –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–∑–æ–≤ `_run_async(async_general_web_search(...))`
   - –í—Å—è –æ—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ (–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤) –æ—Å—Ç–∞

---

## üõ†Ô∏è –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

1. ‚úÖ **read_file**
   - –ê—Ä–≥—É–º–µ–Ω—Ç—ã: `file_path=app/tools/general_web_search.py`

---

*–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: 2025-12-17T17:10:20.303181*