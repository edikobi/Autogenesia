{
  "metadata": {
    "timestamp": "2025-12-15T01:15:28.523547",
    "project_dir": "C:\\Users\\Admin\\AI_Assistant_Pro",
    "user_query": "# ЗАДАЧА: Исправить ошибки в скрипте тестирования scripts/test_history_manager.pyСкрипт падает из-за неправильных имен аргументов при вызове функций. Код приложения (`app/history/...`) менять нельзя, нужно исправить только сам тест.**Исправления:**1.  **В функции `test_history_compression`:**    Найди вызов `compress_history_if_needed`.    Замени аргумент `token_threshold` на `threshold` (так он называется в compressor.py).2.  **В функции `test_prune_context`:**    Найди вызов `prune_irrelevant_context`.    Замени аргумент `messages` на `history`.    Замени аргумент `user_query` на `current_query`.3.  **В функции `cleanup_test_database`:**    Перед удалением файла добавь:    ```    import gc    gc.collect() # Force garbage collection to close DB handles    await asyncio.sleep(0.5) # Give Windows time to release file lock    ```    (Не забудь добавить `async` к функции `cleanup_test_database` и `await` при её вызове в `main`).**Выдай полный исправленный код файла `scripts/test_history_manager.py`.**",
    "duration_seconds": 148.2471888065338,
    "models_used": {
      "orchestrator": "Claude Sonnet 4.5 (RouterAI)",
      "code_generator": "deepseek-chat"
    }
  },
  "orchestrator": {
    "analysis": "The test script `scripts/test_history_manager.py` has three critical issues:\n\n1. **Line 258-261**: `compress_history_if_needed` is called with incorrect parameter names:\n   - Uses `messages=` instead of `history=`\n   - Uses `token_threshold=` instead of `threshold=`\n\n2. **Line 319-322**: `prune_irrelevant_context` is called with incorrect parameter names:\n   - Uses `messages=` instead of `history=`\n   - Uses `user_query=` instead of `current_query=`\n\n3. **Line 356-372**: The `cleanup_test_database` function attempts to delete a database file that may still be locked by SQLite connections on Windows. This requires:\n   - Converting the function to `async` \n   - Adding `gc.collect()` to force garbage collection\n   - Adding `await asyncio.sleep(0.5)` to give Windows time to release the file lock\n   - Updating the call in `main()` to use `await`",
    "instruction": "**Task:** Fix incorrect function argument names and add async cleanup for database file handling\n\n**File:** `scripts/test_history_manager.py`\n\n**Changes:**\n\n1. **Line 1 (imports section)**: Add `gc` import after `asyncio`:\n   ```python\n   import gc\n   ```\n\n2. **Line 258-261 (in `test_history_compression` function)**: Replace the `compress_history_if_needed` call with corrected parameter names:\n   - Change `messages=messages` to `history=messages`\n   - Change `token_threshold=2000` to `threshold=2000`\n   \n   The corrected call should be:\n   ```python\n   compression_result = await compress_history_if_needed(\n       history=messages,\n       threshold=2000\n   )\n   ```\n\n3. **Line 319-322 (in `test_prune_context` function)**: Replace the `prune_irrelevant_context` call with corrected parameter names:\n   - Change `messages=messages` to `history=messages`\n   - Change `user_query=user_query` to `current_query=user_query`\n   \n   The corrected call should be:\n   ```python\n   pruned_messages = prune_irrelevant_context(\n       history=messages,\n       current_query=user_query\n   )\n   ```\n\n4. **Line 356 (function signature)**: Change `cleanup_test_database()` to async:\n   - Change `def cleanup_test_database() -> bool:` to `async def cleanup_test_database() -> bool:`\n\n5. **Line 362-364 (in `cleanup_test_database` function, before `db_path.unlink()`)**: Add garbage collection and sleep to release file locks:\n   ```python\n   gc.collect()  # Force garbage collection to close DB handles\n   await asyncio.sleep(0.5)  # Give Windows time to release file lock\n   db_path.unlink()\n   ```\n\n6. **Line 435 (in `main` function)**: Change the cleanup call to use `await`:\n   - Change `test_results[\"cleanup\"] = cleanup_test_database()` to `test_results[\"cleanup\"] = await cleanup_test_database()`\n\n**Why:** \n- The function parameter names in the test don't match the actual signatures in `app/history/compressor.py`, causing runtime errors\n- Windows file locking requires explicit garbage collection and a delay to properly release SQLite database connections before deletion\n- Making `cleanup_test_database` async allows proper use of `await asyncio.sleep()` for the file lock delay"
  },
  "code_generator": {
    "code_blocks": [
      {
        "filepath": "scripts/test_history_manager.py",
        "language": "python",
        "code": "import asyncio\nimport logging\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Tuple\nimport traceback\nimport gc\n\n# Add project root to sys.path for imports\nproject_root = Path(__file__).resolve().parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\n# Project-specific imports\nfrom app.history.manager import HistoryManager\nfrom app.history.storage import HistoryStorage, Thread, Message\nfrom app.history.compressor import compress_history_if_needed, prune_irrelevant_context\nfrom app.utils.token_counter import TokenCounter\nfrom config.settings import Config\n\n\ndef setup_logging() -> None:\n    \"\"\"Configure colored logging with timestamps.\"\"\"\n    # Custom log level for success\n    SUCCESS_LEVEL_NUM = 25\n    logging.addLevelName(SUCCESS_LEVEL_NUM, \"SUCCESS\")\n\n    def success(self, message, *args, **kwargs):\n        if self.isEnabledFor(SUCCESS_LEVEL_NUM):\n            self._log(SUCCESS_LEVEL_NUM, message, args, **kwargs)\n\n    logging.Logger.success = success\n\n    # Color codes\n    COLORS = {\n        'INFO': '\\033[96m',      # Cyan\n        'WARNING': '\\033[93m',   # Yellow\n        'ERROR': '\\033[91m',     # Red\n        'SUCCESS': '\\033[92m',   # Green\n        'RESET': '\\033[0m',      # Reset\n    }\n\n    class ColoredFormatter(logging.Formatter):\n        def format(self, record):\n            levelname = record.levelname\n            if levelname in COLORS:\n                levelname_color = f\"{COLORS[levelname]}{levelname}{COLORS['RESET']}\"\n                record.levelname = levelname_color\n            return super().format(record)\n\n    # Configure handler\n    handler = logging.StreamHandler()\n    formatter = ColoredFormatter(\n        '[%(asctime)s] [%(levelname)s] %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    handler.setFormatter(formatter)\n\n    # Configure root logger\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(handler)\n\n    logging.info(\"Logging configured successfully\")\n\n\ndef test_imports() -> bool:\n    \"\"\"Test all required imports.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing imports...\")\n\n    imports_to_test = [\n        (\"app.history.manager\", \"HistoryManager\"),\n        (\"app.history.storage\", \"HistoryStorage, Thread, Message\"),\n        (\"app.history.compressor\", \"compress_history_if_needed, prune_irrelevant_context\"),\n        (\"app.utils.token_counter\", \"TokenCounter\"),\n        (\"config.settings\", \"Config\"),\n    ]\n\n    all_successful = True\n\n    for module_path, import_names in imports_to_test:\n        try:\n            __import__(module_path)\n            logger.info(f\"✓ Import successful: {module_path} -> {import_names}\")\n        except ImportError as e:\n            logger.error(f\"✗ Import failed for {module_path}: {e}\")\n            all_successful = False\n        except Exception as e:\n            logger.error(f\"✗ Unexpected error importing {module_path}: {e}\")\n            all_successful = False\n\n    if all_successful:\n        logger.success(\"All imports successful\")\n    else:\n        logger.error(\"Some imports failed\")\n\n    return all_successful\n\n\nasync def test_api_connection() -> bool:\n    \"\"\"Test API connection for history compression.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing API connection for history compression...\")\n\n    try:\n        # Import here to avoid circular dependencies\n        from app.llm.api_client import call_llm\n\n        # Get model from config\n        model = getattr(Config, 'COMPRESSOR_MODEL', 'deepseek-chat')\n\n        # Prepare test message\n        messages = [{\"role\": \"user\", \"content\": \"test\"}]\n\n        logger.debug(f\"Calling LLM API with model: {model}\")\n\n        # Call API\n        response = await call_llm(\n            model=model,\n            messages=messages,\n            temperature=0.1,\n            max_tokens=50\n        )\n\n        # Extract response content\n        if hasattr(response, 'content'):\n            content = response.content\n        elif isinstance(response, dict) and 'content' in response:\n            content = response['content']\n        else:\n            content = str(response)\n\n        preview = content[:50] + \"...\" if len(content) > 50 else content\n        logger.success(f\"API connection successful. Response preview: {preview}\")\n        return True\n\n    except ImportError as e:\n        logger.error(f\"Failed to import api_client: {e}\")\n        return False\n    except ConnectionError as e:\n        logger.error(f\"Connection error: {e}\")\n        return False\n    except TimeoutError as e:\n        logger.error(f\"Timeout error: {e}\")\n        return False\n    except Exception as e:\n        logger.exception(f\"Unexpected error during API test: {e}\")\n        return False\n\n\ndef test_create_history() -> Tuple[str, HistoryManager]:\n    \"\"\"Create a new conversation thread with test messages.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Creating new conversation thread...\")\n\n    try:\n        # Ensure tests directory exists\n        db_path = Path(\"tests/test_history.db\")\n        db_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Initialize HistoryManager with test database\n        manager = HistoryManager(db_path=str(db_path))\n\n        # Create new thread\n        thread = manager.create_thread(\n            user_id=\"test_user\",\n            project_path=\"test_project\",\n            project_name=\"Test Project\"\n        )\n\n        thread_id = thread.id\n        logger.info(f\"Thread created with ID: {thread_id}\")\n\n        # Assert thread_id is not None\n        assert thread_id is not None, \"Thread ID should not be None\"\n        logger.debug(f\"Assertion passed: thread_id = {thread_id}\")\n\n        # Add test messages\n        test_messages = [\n            {\"role\": \"user\", \"content\": \"Hello, can you help me with my project?\"},\n            {\"role\": \"assistant\", \"content\": \"Of course! I'd be happy to help. What's your project about?\"},\n            {\"role\": \"user\", \"content\": \"It's a web application using FastAPI and React.\"},\n            {\"role\": \"assistant\", \"content\": \"Great choice! FastAPI is excellent for backend APIs.\"},\n            {\"role\": \"user\", \"content\": \"I need help with authentication setup.\"},\n        ]\n\n        for i, msg_data in enumerate(test_messages, 1):\n            message = manager.add_message(\n                thread_id=thread_id,\n                role=msg_data[\"role\"],\n                content=msg_data[\"content\"]\n            )\n            logger.info(f\"Added message {i}/{len(test_messages)}: {message.id}\")\n\n        # Verify message count\n        messages = manager.get_session_history(thread_id)\n        assert len(messages) == len(test_messages), \\\n            f\"Expected {len(test_messages)} messages, got {len(messages)}\"\n        logger.debug(f\"Assertion passed: {len(messages)} messages added\")\n\n        # Get thread statistics\n        stats = manager.get_thread_statistics(thread_id)\n        logger.info(f\"Thread statistics: {stats.get('total_messages', 0)} messages, \"\n                   f\"{stats.get('total_tokens', 0)} tokens\")\n\n        logger.success(\"Thread creation test completed successfully\")\n        return thread_id, manager\n\n    except FileNotFoundError as e:\n        logger.error(f\"Database file not found: {e}\")\n        raise\n    except ValueError as e:\n        logger.error(f\"Invalid value: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in thread creation: {e}\")\n        raise\n\n\nasync def test_history_compression(manager: HistoryManager, thread_id: str) -> Dict[str, Any]:\n    \"\"\"Test dynamic history compression.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing dynamic history compression...\")\n\n    try:\n        # Get initial messages\n        messages = manager.get_session_history(thread_id)\n        initial_count = len(messages)\n\n        # Calculate initial tokens (simplified)\n        token_counter = TokenCounter()\n        initial_tokens = sum(token_counter.count(msg.content) for msg in messages)\n\n        logger.info(f\"Initial state: {initial_count} messages, ~{int(initial_tokens)} tokens\")\n\n        # Add more messages to trigger compression\n        logger.info(\"Adding messages to trigger compression threshold...\")\n        long_content = \"This is a long message \" * 20  # ~200 tokens\n\n        for i in range(1, 11):\n            role = \"user\" if i % 2 == 1 else \"assistant\"\n            manager.add_message(\n                thread_id=thread_id,\n                role=role,\n                content=f\"{long_content} - Message {i}\"\n            )\n            logger.debug(f\"Added long message {i}/10\")\n\n        # Get updated messages\n        messages = manager.get_session_history(thread_id)\n        before_count = len(messages)\n        before_tokens = sum(token_counter.count(msg.content) for msg in messages)\n\n        logger.info(f\"Before compression: {before_count} messages, ~{int(before_tokens)} tokens\")\n\n        # Apply compression\n        compression_result = await compress_history_if_needed(\n            history=messages,\n            threshold=2000\n        )\n\n        after_count = len(compression_result.get(\"compressed_messages\", []))\n        after_tokens = compression_result.get(\"total_tokens_after\", 0)\n        pruned_count = compression_result.get(\"pruned_count\", 0)\n\n        logger.info(f\"After compression: {after_count} messages, ~{int(after_tokens)} tokens\")\n        logger.info(f\"Pruned {pruned_count} messages\")\n\n        # Verify compression occurred\n        compression_happened = (after_tokens < before_tokens) or (after_count < before_count)\n        assert compression_happened, \"Compression should reduce tokens or message count\"\n\n        if compression_happened:\n            logger.success(\"Compression test passed successfully\")\n        else:\n            logger.warning(\"No compression happened - might be below threshold\")\n\n        return {\n            \"before_count\": before_count,\n            \"before_tokens\": before_tokens,\n            \"after_count\": after_count,\n            \"after_tokens\": after_tokens,\n            \"pruned_count\": pruned_count,\n            \"compression_happened\": compression_happened\n        }\n\n    except AssertionError as e:\n        logger.error(f\"Assertion failed: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in compression test: {e}\")\n        raise\n\n\ndef test_prune_context() -> Dict[str, Any]:\n    \"\"\"Test context pruning for irrelevant tool results.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing context pruning for irrelevant tool results...\")\n\n    try:\n        # Create test messages with tool results\n        messages = [\n            Message(id=\"1\", thread_id=\"test\", role=\"user\", content=\"Show me the auth service code\", tokens=0),\n            Message(id=\"2\", thread_id=\"test\", role=\"tool\", content=\"File: app/services/auth.py\\nCode: def authenticate(): ...\", tokens=0),\n            Message(id=\"3\", thread_id=\"test\", role=\"tool\", content=\"File: app/utils/logger.py\\nCode: def log(): ...\", tokens=0),\n            Message(id=\"4\", thread_id=\"test\", role=\"tool\", content=\"File: app/models/user.py\\nCode: class User: ...\", tokens=0),\n            Message(id=\"5\", thread_id=\"test\", role=\"assistant\", content=\"Here's the auth service code...\", tokens=0),\n            Message(id=\"6\", thread_id=\"test\", role=\"user\", content=\"Now modify only app/services/auth.py\", tokens=0),\n        ]\n\n        original_count = len(messages)\n        logger.info(f\"Original messages: {original_count}\")\n\n        # User query mentioning specific file\n        user_query = \"Modify the authenticate function in app/services/auth.py\"\n\n        # Apply pruning\n        pruned_messages = prune_irrelevant_context(\n            history=messages,\n            current_query=user_query\n        )\n\n        pruned_count = len(pruned_messages)\n        logger.info(f\"After pruning: {pruned_count} messages\")\n        logger.info(f\"Removed {original_count - pruned_count} irrelevant messages\")\n\n        # Verify irrelevant tool results were removed\n        tool_messages_before = [m for m in messages if m.role == \"tool\"]\n        tool_messages_after = [m for m in pruned_messages if m.role == \"tool\"]\n\n        # Check that only auth.py tool result remains\n        auth_tools = [m for m in tool_messages_after if \"auth.py\" in m.content]\n        other_tools = [m for m in tool_messages_after if \"auth.py\" not in m.content]\n\n        assert len(other_tools) == 0, \"Irrelevant tool results should be removed\"\n        assert len(auth_tools) > 0, \"Relevant tool results should remain\"\n\n        logger.success(\"Context pruning test passed successfully\")\n\n        return {\n            \"original_count\": original_count,\n            \"pruned_count\": pruned_count,\n            \"removed_count\": original_count - pruned_count,\n            \"files_mentioned\": [\"app/services/auth.py\"]\n        }\n\n    except AssertionError as e:\n        logger.error(f\"Assertion failed: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in pruning test: {e}\")\n        raise\n\n\nasync def cleanup_test_database() -> bool:\n    \"\"\"Clean up test database file.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Cleaning up test database...\")\n\n    try:\n        db_path = Path(\"tests/test_history.db\")\n        if db_path.exists():\n            gc.collect()  # Force garbage collection to close DB handles\n            await asyncio.sleep(0.5)  # Give Windows time to release file lock\n            db_path.unlink()\n            logger.success(f\"Removed test database: {db_path}\")\n            return True\n        else:\n            logger.info(\"Test database does not exist, nothing to clean up\")\n            return True\n    except Exception as e:\n        logger.error(f\"Failed to clean up test database: {e}\")\n        return False\n\n\nasync def main() -> int:\n    \"\"\"Main test runner.\"\"\"\n    print(\"\\n\" + \"=\" * 40)\n    print(\"=== HISTORY MANAGEMENT TEST SUITE ===\")\n    print(\"=\" * 40 + \"\\n\")\n\n    # Setup logging\n    setup_logging()\n    logger = logging.getLogger(__name__)\n\n    # Test results tracker\n    test_results = {}\n\n    try:\n        # Test 1: Imports\n        logger.info(\"Starting test: Imports\")\n        test_results[\"imports\"] = test_imports()\n\n        if not test_results[\"imports\"]:\n            logger.error(\"Import test failed. Skipping further tests.\")\n            return 1\n\n        # Test 2: API Connection\n        logger.info(\"Starting test: API Connection\")\n        test_results[\"api_connection\"] = await test_api_connection()\n\n        if not test_results[\"api_connection\"]:\n            logger.warning(\"API connection test failed. Some tests may be limited.\")\n\n        # Test 3: Create History\n        logger.info(\"Starting test: Create History\")\n        try:\n            thread_id, manager = test_create_history()\n            test_results[\"create_history\"] = True\n        except Exception:\n            test_results[\"create_history\"] = False\n            thread_id, manager = None, None\n\n        # Test 4: History Compression (if history created successfully)\n        if test_results.get(\"create_history\") and thread_id and manager:\n            logger.info(\"Starting test: History Compression\")\n            try:\n                compression_stats = await test_history_compression(manager, thread_id)\n                test_results[\"history_compression\"] = compression_stats.get(\"compression_happened\", False)\n            except Exception:\n                test_results[\"history_compression\"] = False\n        else:\n            logger.warning(\"Skipping compression test - history creation failed\")\n            test_results[\"history_compression\"] = False\n\n        # Test 5: Prune Context\n        logger.info(\"Starting test: Prune Context\")\n        try:\n            prune_stats = test_prune_context()\n            test_results[\"prune_context\"] = prune_stats.get(\"removed_count\", 0) > 0\n        except Exception:\n            test_results[\"prune_context\"] = False\n\n        # Cleanup\n        logger.info(\"Starting cleanup\")\n        test_results[\"cleanup\"] = await cleanup_test_database()\n\n    except KeyboardInterrupt:\n        logger.error(\"Test suite interrupted by user\")\n        return 1\n    except Exception as e:\n        logger.exception(f\"Unexpected error in main test runner: {e}\")\n        return 1\n\n    # Print summary\n    print(\"\\n\" + \"=\" * 40)\n    print(\"=== TEST RESULTS ===\")\n    print(\"=\" * 40)\n\n    passed = sum(1 for result in test_results.values() if result is True)\n    total = sum(1 for result in test_results.values() if isinstance(result, bool))\n\n    print(f\"\\nSummary: {passed}/{total} tests passed\")\n\n    for test_name, result in test_results.items():\n        status = \"✓ PASS\" if result else \"✗ FAIL\"\n        color = '\\033[92m' if result else '\\033[91m'\n        reset = '\\033[0m'\n        print(f\"  {color}{status}{reset}: {test_name}\")\n\n    print(\"\\n\" + \"=\" * 40)\n\n    # Return exit code\n    if passed == total:\n        logger.success(\"All tests passed!\")\n        return 0\n    else:\n        logger.error(f\"{total - passed} test(s) failed\")\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit_code = asyncio.run(main())\n    sys.exit(exit_code)",
        "context": null
      }
    ],
    "explanation": "Этот код вносит несколько важных исправлений в тестовый скрипт для менеджера истории:\n\n1. **Добавлен",
    "frontend_json": {
      "success": true,
      "code_blocks": [
        {
          "code": "import asyncio\nimport logging\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Tuple\nimport traceback\nimport gc\n\n# Add project root to sys.path for imports\nproject_root = Path(__file__).resolve().parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\n# Project-specific imports\nfrom app.history.manager import HistoryManager\nfrom app.history.storage import HistoryStorage, Thread, Message\nfrom app.history.compressor import compress_history_if_needed, prune_irrelevant_context\nfrom app.utils.token_counter import TokenCounter\nfrom config.settings import Config\n\n\ndef setup_logging() -> None:\n    \"\"\"Configure colored logging with timestamps.\"\"\"\n    # Custom log level for success\n    SUCCESS_LEVEL_NUM = 25\n    logging.addLevelName(SUCCESS_LEVEL_NUM, \"SUCCESS\")\n\n    def success(self, message, *args, **kwargs):\n        if self.isEnabledFor(SUCCESS_LEVEL_NUM):\n            self._log(SUCCESS_LEVEL_NUM, message, args, **kwargs)\n\n    logging.Logger.success = success\n\n    # Color codes\n    COLORS = {\n        'INFO': '\\033[96m',      # Cyan\n        'WARNING': '\\033[93m',   # Yellow\n        'ERROR': '\\033[91m',     # Red\n        'SUCCESS': '\\033[92m',   # Green\n        'RESET': '\\033[0m',      # Reset\n    }\n\n    class ColoredFormatter(logging.Formatter):\n        def format(self, record):\n            levelname = record.levelname\n            if levelname in COLORS:\n                levelname_color = f\"{COLORS[levelname]}{levelname}{COLORS['RESET']}\"\n                record.levelname = levelname_color\n            return super().format(record)\n\n    # Configure handler\n    handler = logging.StreamHandler()\n    formatter = ColoredFormatter(\n        '[%(asctime)s] [%(levelname)s] %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    handler.setFormatter(formatter)\n\n    # Configure root logger\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(handler)\n\n    logging.info(\"Logging configured successfully\")\n\n\ndef test_imports() -> bool:\n    \"\"\"Test all required imports.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing imports...\")\n\n    imports_to_test = [\n        (\"app.history.manager\", \"HistoryManager\"),\n        (\"app.history.storage\", \"HistoryStorage, Thread, Message\"),\n        (\"app.history.compressor\", \"compress_history_if_needed, prune_irrelevant_context\"),\n        (\"app.utils.token_counter\", \"TokenCounter\"),\n        (\"config.settings\", \"Config\"),\n    ]\n\n    all_successful = True\n\n    for module_path, import_names in imports_to_test:\n        try:\n            __import__(module_path)\n            logger.info(f\"✓ Import successful: {module_path} -> {import_names}\")\n        except ImportError as e:\n            logger.error(f\"✗ Import failed for {module_path}: {e}\")\n            all_successful = False\n        except Exception as e:\n            logger.error(f\"✗ Unexpected error importing {module_path}: {e}\")\n            all_successful = False\n\n    if all_successful:\n        logger.success(\"All imports successful\")\n    else:\n        logger.error(\"Some imports failed\")\n\n    return all_successful\n\n\nasync def test_api_connection() -> bool:\n    \"\"\"Test API connection for history compression.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing API connection for history compression...\")\n\n    try:\n        # Import here to avoid circular dependencies\n        from app.llm.api_client import call_llm\n\n        # Get model from config\n        model = getattr(Config, 'COMPRESSOR_MODEL', 'deepseek-chat')\n\n        # Prepare test message\n        messages = [{\"role\": \"user\", \"content\": \"test\"}]\n\n        logger.debug(f\"Calling LLM API with model: {model}\")\n\n        # Call API\n        response = await call_llm(\n            model=model,\n            messages=messages,\n            temperature=0.1,\n            max_tokens=50\n        )\n\n        # Extract response content\n        if hasattr(response, 'content'):\n            content = response.content\n        elif isinstance(response, dict) and 'content' in response:\n            content = response['content']\n        else:\n            content = str(response)\n\n        preview = content[:50] + \"...\" if len(content) > 50 else content\n        logger.success(f\"API connection successful. Response preview: {preview}\")\n        return True\n\n    except ImportError as e:\n        logger.error(f\"Failed to import api_client: {e}\")\n        return False\n    except ConnectionError as e:\n        logger.error(f\"Connection error: {e}\")\n        return False\n    except TimeoutError as e:\n        logger.error(f\"Timeout error: {e}\")\n        return False\n    except Exception as e:\n        logger.exception(f\"Unexpected error during API test: {e}\")\n        return False\n\n\ndef test_create_history() -> Tuple[str, HistoryManager]:\n    \"\"\"Create a new conversation thread with test messages.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Creating new conversation thread...\")\n\n    try:\n        # Ensure tests directory exists\n        db_path = Path(\"tests/test_history.db\")\n        db_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Initialize HistoryManager with test database\n        manager = HistoryManager(db_path=str(db_path))\n\n        # Create new thread\n        thread = manager.create_thread(\n            user_id=\"test_user\",\n            project_path=\"test_project\",\n            project_name=\"Test Project\"\n        )\n\n        thread_id = thread.id\n        logger.info(f\"Thread created with ID: {thread_id}\")\n\n        # Assert thread_id is not None\n        assert thread_id is not None, \"Thread ID should not be None\"\n        logger.debug(f\"Assertion passed: thread_id = {thread_id}\")\n\n        # Add test messages\n        test_messages = [\n            {\"role\": \"user\", \"content\": \"Hello, can you help me with my project?\"},\n            {\"role\": \"assistant\", \"content\": \"Of course! I'd be happy to help. What's your project about?\"},\n            {\"role\": \"user\", \"content\": \"It's a web application using FastAPI and React.\"},\n            {\"role\": \"assistant\", \"content\": \"Great choice! FastAPI is excellent for backend APIs.\"},\n            {\"role\": \"user\", \"content\": \"I need help with authentication setup.\"},\n        ]\n\n        for i, msg_data in enumerate(test_messages, 1):\n            message = manager.add_message(\n                thread_id=thread_id,\n                role=msg_data[\"role\"],\n                content=msg_data[\"content\"]\n            )\n            logger.info(f\"Added message {i}/{len(test_messages)}: {message.id}\")\n\n        # Verify message count\n        messages = manager.get_session_history(thread_id)\n        assert len(messages) == len(test_messages), \\\n            f\"Expected {len(test_messages)} messages, got {len(messages)}\"\n        logger.debug(f\"Assertion passed: {len(messages)} messages added\")\n\n        # Get thread statistics\n        stats = manager.get_thread_statistics(thread_id)\n        logger.info(f\"Thread statistics: {stats.get('total_messages', 0)} messages, \"\n                   f\"{stats.get('total_tokens', 0)} tokens\")\n\n        logger.success(\"Thread creation test completed successfully\")\n        return thread_id, manager\n\n    except FileNotFoundError as e:\n        logger.error(f\"Database file not found: {e}\")\n        raise\n    except ValueError as e:\n        logger.error(f\"Invalid value: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in thread creation: {e}\")\n        raise\n\n\nasync def test_history_compression(manager: HistoryManager, thread_id: str) -> Dict[str, Any]:\n    \"\"\"Test dynamic history compression.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing dynamic history compression...\")\n\n    try:\n        # Get initial messages\n        messages = manager.get_session_history(thread_id)\n        initial_count = len(messages)\n\n        # Calculate initial tokens (simplified)\n        token_counter = TokenCounter()\n        initial_tokens = sum(token_counter.count(msg.content) for msg in messages)\n\n        logger.info(f\"Initial state: {initial_count} messages, ~{int(initial_tokens)} tokens\")\n\n        # Add more messages to trigger compression\n        logger.info(\"Adding messages to trigger compression threshold...\")\n        long_content = \"This is a long message \" * 20  # ~200 tokens\n\n        for i in range(1, 11):\n            role = \"user\" if i % 2 == 1 else \"assistant\"\n            manager.add_message(\n                thread_id=thread_id,\n                role=role,\n                content=f\"{long_content} - Message {i}\"\n            )\n            logger.debug(f\"Added long message {i}/10\")\n\n        # Get updated messages\n        messages = manager.get_session_history(thread_id)\n        before_count = len(messages)\n        before_tokens = sum(token_counter.count(msg.content) for msg in messages)\n\n        logger.info(f\"Before compression: {before_count} messages, ~{int(before_tokens)} tokens\")\n\n        # Apply compression\n        compression_result = await compress_history_if_needed(\n            history=messages,\n            threshold=2000\n        )\n\n        after_count = len(compression_result.get(\"compressed_messages\", []))\n        after_tokens = compression_result.get(\"total_tokens_after\", 0)\n        pruned_count = compression_result.get(\"pruned_count\", 0)\n\n        logger.info(f\"After compression: {after_count} messages, ~{int(after_tokens)} tokens\")\n        logger.info(f\"Pruned {pruned_count} messages\")\n\n        # Verify compression occurred\n        compression_happened = (after_tokens < before_tokens) or (after_count < before_count)\n        assert compression_happened, \"Compression should reduce tokens or message count\"\n\n        if compression_happened:\n            logger.success(\"Compression test passed successfully\")\n        else:\n            logger.warning(\"No compression happened - might be below threshold\")\n\n        return {\n            \"before_count\": before_count,\n            \"before_tokens\": before_tokens,\n            \"after_count\": after_count,\n            \"after_tokens\": after_tokens,\n            \"pruned_count\": pruned_count,\n            \"compression_happened\": compression_happened\n        }\n\n    except AssertionError as e:\n        logger.error(f\"Assertion failed: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in compression test: {e}\")\n        raise\n\n\ndef test_prune_context() -> Dict[str, Any]:\n    \"\"\"Test context pruning for irrelevant tool results.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing context pruning for irrelevant tool results...\")\n\n    try:\n        # Create test messages with tool results\n        messages = [\n            Message(id=\"1\", thread_id=\"test\", role=\"user\", content=\"Show me the auth service code\", tokens=0),\n            Message(id=\"2\", thread_id=\"test\", role=\"tool\", content=\"File: app/services/auth.py\\nCode: def authenticate(): ...\", tokens=0),\n            Message(id=\"3\", thread_id=\"test\", role=\"tool\", content=\"File: app/utils/logger.py\\nCode: def log(): ...\", tokens=0),\n            Message(id=\"4\", thread_id=\"test\", role=\"tool\", content=\"File: app/models/user.py\\nCode: class User: ...\", tokens=0),\n            Message(id=\"5\", thread_id=\"test\", role=\"assistant\", content=\"Here's the auth service code...\", tokens=0),\n            Message(id=\"6\", thread_id=\"test\", role=\"user\", content=\"Now modify only app/services/auth.py\", tokens=0),\n        ]\n\n        original_count = len(messages)\n        logger.info(f\"Original messages: {original_count}\")\n\n        # User query mentioning specific file\n        user_query = \"Modify the authenticate function in app/services/auth.py\"\n\n        # Apply pruning\n        pruned_messages = prune_irrelevant_context(\n            history=messages,\n            current_query=user_query\n        )\n\n        pruned_count = len(pruned_messages)\n        logger.info(f\"After pruning: {pruned_count} messages\")\n        logger.info(f\"Removed {original_count - pruned_count} irrelevant messages\")\n\n        # Verify irrelevant tool results were removed\n        tool_messages_before = [m for m in messages if m.role == \"tool\"]\n        tool_messages_after = [m for m in pruned_messages if m.role == \"tool\"]\n\n        # Check that only auth.py tool result remains\n        auth_tools = [m for m in tool_messages_after if \"auth.py\" in m.content]\n        other_tools = [m for m in tool_messages_after if \"auth.py\" not in m.content]\n\n        assert len(other_tools) == 0, \"Irrelevant tool results should be removed\"\n        assert len(auth_tools) > 0, \"Relevant tool results should remain\"\n\n        logger.success(\"Context pruning test passed successfully\")\n\n        return {\n            \"original_count\": original_count,\n            \"pruned_count\": pruned_count,\n            \"removed_count\": original_count - pruned_count,\n            \"files_mentioned\": [\"app/services/auth.py\"]\n        }\n\n    except AssertionError as e:\n        logger.error(f\"Assertion failed: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in pruning test: {e}\")\n        raise\n\n\nasync def cleanup_test_database() -> bool:\n    \"\"\"Clean up test database file.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Cleaning up test database...\")\n\n    try:\n        db_path = Path(\"tests/test_history.db\")\n        if db_path.exists():\n            gc.collect()  # Force garbage collection to close DB handles\n            await asyncio.sleep(0.5)  # Give Windows time to release file lock\n            db_path.unlink()\n            logger.success(f\"Removed test database: {db_path}\")\n            return True\n        else:\n            logger.info(\"Test database does not exist, nothing to clean up\")\n            return True\n    except Exception as e:\n        logger.error(f\"Failed to clean up test database: {e}\")\n        return False\n\n\nasync def main() -> int:\n    \"\"\"Main test runner.\"\"\"\n    print(\"\\n\" + \"=\" * 40)\n    print(\"=== HISTORY MANAGEMENT TEST SUITE ===\")\n    print(\"=\" * 40 + \"\\n\")\n\n    # Setup logging\n    setup_logging()\n    logger = logging.getLogger(__name__)\n\n    # Test results tracker\n    test_results = {}\n\n    try:\n        # Test 1: Imports\n        logger.info(\"Starting test: Imports\")\n        test_results[\"imports\"] = test_imports()\n\n        if not test_results[\"imports\"]:\n            logger.error(\"Import test failed. Skipping further tests.\")\n            return 1\n\n        # Test 2: API Connection\n        logger.info(\"Starting test: API Connection\")\n        test_results[\"api_connection\"] = await test_api_connection()\n\n        if not test_results[\"api_connection\"]:\n            logger.warning(\"API connection test failed. Some tests may be limited.\")\n\n        # Test 3: Create History\n        logger.info(\"Starting test: Create History\")\n        try:\n            thread_id, manager = test_create_history()\n            test_results[\"create_history\"] = True\n        except Exception:\n            test_results[\"create_history\"] = False\n            thread_id, manager = None, None\n\n        # Test 4: History Compression (if history created successfully)\n        if test_results.get(\"create_history\") and thread_id and manager:\n            logger.info(\"Starting test: History Compression\")\n            try:\n                compression_stats = await test_history_compression(manager, thread_id)\n                test_results[\"history_compression\"] = compression_stats.get(\"compression_happened\", False)\n            except Exception:\n                test_results[\"history_compression\"] = False\n        else:\n            logger.warning(\"Skipping compression test - history creation failed\")\n            test_results[\"history_compression\"] = False\n\n        # Test 5: Prune Context\n        logger.info(\"Starting test: Prune Context\")\n        try:\n            prune_stats = test_prune_context()\n            test_results[\"prune_context\"] = prune_stats.get(\"removed_count\", 0) > 0\n        except Exception:\n            test_results[\"prune_context\"] = False\n\n        # Cleanup\n        logger.info(\"Starting cleanup\")\n        test_results[\"cleanup\"] = await cleanup_test_database()\n\n    except KeyboardInterrupt:\n        logger.error(\"Test suite interrupted by user\")\n        return 1\n    except Exception as e:\n        logger.exception(f\"Unexpected error in main test runner: {e}\")\n        return 1\n\n    # Print summary\n    print(\"\\n\" + \"=\" * 40)\n    print(\"=== TEST RESULTS ===\")\n    print(\"=\" * 40)\n\n    passed = sum(1 for result in test_results.values() if result is True)\n    total = sum(1 for result in test_results.values() if isinstance(result, bool))\n\n    print(f\"\\nSummary: {passed}/{total} tests passed\")\n\n    for test_name, result in test_results.items():\n        status = \"✓ PASS\" if result else \"✗ FAIL\"\n        color = '\\033[92m' if result else '\\033[91m'\n        reset = '\\033[0m'\n        print(f\"  {color}{status}{reset}: {test_name}\")\n\n    print(\"\\n\" + \"=\" * 40)\n\n    # Return exit code\n    if passed == total:\n        logger.success(\"All tests passed!\")\n        return 0\n    else:\n        logger.error(f\"{total - passed} test(s) failed\")\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit_code = asyncio.run(main())\n    sys.exit(exit_code)",
          "filepath": "scripts/test_history_manager.py",
          "language": "python",
          "context": null
        }
      ],
      "combined_code": "\n\n# ==================================================# filepath: scripts/test_history_manager.py\n\nimport asyncio\nimport logging\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Tuple\nimport traceback\nimport gc\n\n# Add project root to sys.path for imports\nproject_root = Path(__file__).resolve().parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\n# Project-specific imports\nfrom app.history.manager import HistoryManager\nfrom app.history.storage import HistoryStorage, Thread, Message\nfrom app.history.compressor import compress_history_if_needed, prune_irrelevant_context\nfrom app.utils.token_counter import TokenCounter\nfrom config.settings import Config\n\n\ndef setup_logging() -> None:\n    \"\"\"Configure colored logging with timestamps.\"\"\"\n    # Custom log level for success\n    SUCCESS_LEVEL_NUM = 25\n    logging.addLevelName(SUCCESS_LEVEL_NUM, \"SUCCESS\")\n\n    def success(self, message, *args, **kwargs):\n        if self.isEnabledFor(SUCCESS_LEVEL_NUM):\n            self._log(SUCCESS_LEVEL_NUM, message, args, **kwargs)\n\n    logging.Logger.success = success\n\n    # Color codes\n    COLORS = {\n        'INFO': '\\033[96m',      # Cyan\n        'WARNING': '\\033[93m',   # Yellow\n        'ERROR': '\\033[91m',     # Red\n        'SUCCESS': '\\033[92m',   # Green\n        'RESET': '\\033[0m',      # Reset\n    }\n\n    class ColoredFormatter(logging.Formatter):\n        def format(self, record):\n            levelname = record.levelname\n            if levelname in COLORS:\n                levelname_color = f\"{COLORS[levelname]}{levelname}{COLORS['RESET']}\"\n                record.levelname = levelname_color\n            return super().format(record)\n\n    # Configure handler\n    handler = logging.StreamHandler()\n    formatter = ColoredFormatter(\n        '[%(asctime)s] [%(levelname)s] %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    handler.setFormatter(formatter)\n\n    # Configure root logger\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(handler)\n\n    logging.info(\"Logging configured successfully\")\n\n\ndef test_imports() -> bool:\n    \"\"\"Test all required imports.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing imports...\")\n\n    imports_to_test = [\n        (\"app.history.manager\", \"HistoryManager\"),\n        (\"app.history.storage\", \"HistoryStorage, Thread, Message\"),\n        (\"app.history.compressor\", \"compress_history_if_needed, prune_irrelevant_context\"),\n        (\"app.utils.token_counter\", \"TokenCounter\"),\n        (\"config.settings\", \"Config\"),\n    ]\n\n    all_successful = True\n\n    for module_path, import_names in imports_to_test:\n        try:\n            __import__(module_path)\n            logger.info(f\"✓ Import successful: {module_path} -> {import_names}\")\n        except ImportError as e:\n            logger.error(f\"✗ Import failed for {module_path}: {e}\")\n            all_successful = False\n        except Exception as e:\n            logger.error(f\"✗ Unexpected error importing {module_path}: {e}\")\n            all_successful = False\n\n    if all_successful:\n        logger.success(\"All imports successful\")\n    else:\n        logger.error(\"Some imports failed\")\n\n    return all_successful\n\n\nasync def test_api_connection() -> bool:\n    \"\"\"Test API connection for history compression.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing API connection for history compression...\")\n\n    try:\n        # Import here to avoid circular dependencies\n        from app.llm.api_client import call_llm\n\n        # Get model from config\n        model = getattr(Config, 'COMPRESSOR_MODEL', 'deepseek-chat')\n\n        # Prepare test message\n        messages = [{\"role\": \"user\", \"content\": \"test\"}]\n\n        logger.debug(f\"Calling LLM API with model: {model}\")\n\n        # Call API\n        response = await call_llm(\n            model=model,\n            messages=messages,\n            temperature=0.1,\n            max_tokens=50\n        )\n\n        # Extract response content\n        if hasattr(response, 'content'):\n            content = response.content\n        elif isinstance(response, dict) and 'content' in response:\n            content = response['content']\n        else:\n            content = str(response)\n\n        preview = content[:50] + \"...\" if len(content) > 50 else content\n        logger.success(f\"API connection successful. Response preview: {preview}\")\n        return True\n\n    except ImportError as e:\n        logger.error(f\"Failed to import api_client: {e}\")\n        return False\n    except ConnectionError as e:\n        logger.error(f\"Connection error: {e}\")\n        return False\n    except TimeoutError as e:\n        logger.error(f\"Timeout error: {e}\")\n        return False\n    except Exception as e:\n        logger.exception(f\"Unexpected error during API test: {e}\")\n        return False\n\n\ndef test_create_history() -> Tuple[str, HistoryManager]:\n    \"\"\"Create a new conversation thread with test messages.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Creating new conversation thread...\")\n\n    try:\n        # Ensure tests directory exists\n        db_path = Path(\"tests/test_history.db\")\n        db_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Initialize HistoryManager with test database\n        manager = HistoryManager(db_path=str(db_path))\n\n        # Create new thread\n        thread = manager.create_thread(\n            user_id=\"test_user\",\n            project_path=\"test_project\",\n            project_name=\"Test Project\"\n        )\n\n        thread_id = thread.id\n        logger.info(f\"Thread created with ID: {thread_id}\")\n\n        # Assert thread_id is not None\n        assert thread_id is not None, \"Thread ID should not be None\"\n        logger.debug(f\"Assertion passed: thread_id = {thread_id}\")\n\n        # Add test messages\n        test_messages = [\n            {\"role\": \"user\", \"content\": \"Hello, can you help me with my project?\"},\n            {\"role\": \"assistant\", \"content\": \"Of course! I'd be happy to help. What's your project about?\"},\n            {\"role\": \"user\", \"content\": \"It's a web application using FastAPI and React.\"},\n            {\"role\": \"assistant\", \"content\": \"Great choice! FastAPI is excellent for backend APIs.\"},\n            {\"role\": \"user\", \"content\": \"I need help with authentication setup.\"},\n        ]\n\n        for i, msg_data in enumerate(test_messages, 1):\n            message = manager.add_message(\n                thread_id=thread_id,\n                role=msg_data[\"role\"],\n                content=msg_data[\"content\"]\n            )\n            logger.info(f\"Added message {i}/{len(test_messages)}: {message.id}\")\n\n        # Verify message count\n        messages = manager.get_session_history(thread_id)\n        assert len(messages) == len(test_messages), \\\n            f\"Expected {len(test_messages)} messages, got {len(messages)}\"\n        logger.debug(f\"Assertion passed: {len(messages)} messages added\")\n\n        # Get thread statistics\n        stats = manager.get_thread_statistics(thread_id)\n        logger.info(f\"Thread statistics: {stats.get('total_messages', 0)} messages, \"\n                   f\"{stats.get('total_tokens', 0)} tokens\")\n\n        logger.success(\"Thread creation test completed successfully\")\n        return thread_id, manager\n\n    except FileNotFoundError as e:\n        logger.error(f\"Database file not found: {e}\")\n        raise\n    except ValueError as e:\n        logger.error(f\"Invalid value: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in thread creation: {e}\")\n        raise\n\n\nasync def test_history_compression(manager: HistoryManager, thread_id: str) -> Dict[str, Any]:\n    \"\"\"Test dynamic history compression.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing dynamic history compression...\")\n\n    try:\n        # Get initial messages\n        messages = manager.get_session_history(thread_id)\n        initial_count = len(messages)\n\n        # Calculate initial tokens (simplified)\n        token_counter = TokenCounter()\n        initial_tokens = sum(token_counter.count(msg.content) for msg in messages)\n\n        logger.info(f\"Initial state: {initial_count} messages, ~{int(initial_tokens)} tokens\")\n\n        # Add more messages to trigger compression\n        logger.info(\"Adding messages to trigger compression threshold...\")\n        long_content = \"This is a long message \" * 20  # ~200 tokens\n\n        for i in range(1, 11):\n            role = \"user\" if i % 2 == 1 else \"assistant\"\n            manager.add_message(\n                thread_id=thread_id,\n                role=role,\n                content=f\"{long_content} - Message {i}\"\n            )\n            logger.debug(f\"Added long message {i}/10\")\n\n        # Get updated messages\n        messages = manager.get_session_history(thread_id)\n        before_count = len(messages)\n        before_tokens = sum(token_counter.count(msg.content) for msg in messages)\n\n        logger.info(f\"Before compression: {before_count} messages, ~{int(before_tokens)} tokens\")\n\n        # Apply compression\n        compression_result = await compress_history_if_needed(\n            history=messages,\n            threshold=2000\n        )\n\n        after_count = len(compression_result.get(\"compressed_messages\", []))\n        after_tokens = compression_result.get(\"total_tokens_after\", 0)\n        pruned_count = compression_result.get(\"pruned_count\", 0)\n\n        logger.info(f\"After compression: {after_count} messages, ~{int(after_tokens)} tokens\")\n        logger.info(f\"Pruned {pruned_count} messages\")\n\n        # Verify compression occurred\n        compression_happened = (after_tokens < before_tokens) or (after_count < before_count)\n        assert compression_happened, \"Compression should reduce tokens or message count\"\n\n        if compression_happened:\n            logger.success(\"Compression test passed successfully\")\n        else:\n            logger.warning(\"No compression happened - might be below threshold\")\n\n        return {\n            \"before_count\": before_count,\n            \"before_tokens\": before_tokens,\n            \"after_count\": after_count,\n            \"after_tokens\": after_tokens,\n            \"pruned_count\": pruned_count,\n            \"compression_happened\": compression_happened\n        }\n\n    except AssertionError as e:\n        logger.error(f\"Assertion failed: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in compression test: {e}\")\n        raise\n\n\ndef test_prune_context() -> Dict[str, Any]:\n    \"\"\"Test context pruning for irrelevant tool results.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Testing context pruning for irrelevant tool results...\")\n\n    try:\n        # Create test messages with tool results\n        messages = [\n            Message(id=\"1\", thread_id=\"test\", role=\"user\", content=\"Show me the auth service code\", tokens=0),\n            Message(id=\"2\", thread_id=\"test\", role=\"tool\", content=\"File: app/services/auth.py\\nCode: def authenticate(): ...\", tokens=0),\n            Message(id=\"3\", thread_id=\"test\", role=\"tool\", content=\"File: app/utils/logger.py\\nCode: def log(): ...\", tokens=0),\n            Message(id=\"4\", thread_id=\"test\", role=\"tool\", content=\"File: app/models/user.py\\nCode: class User: ...\", tokens=0),\n            Message(id=\"5\", thread_id=\"test\", role=\"assistant\", content=\"Here's the auth service code...\", tokens=0),\n            Message(id=\"6\", thread_id=\"test\", role=\"user\", content=\"Now modify only app/services/auth.py\", tokens=0),\n        ]\n\n        original_count = len(messages)\n        logger.info(f\"Original messages: {original_count}\")\n\n        # User query mentioning specific file\n        user_query = \"Modify the authenticate function in app/services/auth.py\"\n\n        # Apply pruning\n        pruned_messages = prune_irrelevant_context(\n            history=messages,\n            current_query=user_query\n        )\n\n        pruned_count = len(pruned_messages)\n        logger.info(f\"After pruning: {pruned_count} messages\")\n        logger.info(f\"Removed {original_count - pruned_count} irrelevant messages\")\n\n        # Verify irrelevant tool results were removed\n        tool_messages_before = [m for m in messages if m.role == \"tool\"]\n        tool_messages_after = [m for m in pruned_messages if m.role == \"tool\"]\n\n        # Check that only auth.py tool result remains\n        auth_tools = [m for m in tool_messages_after if \"auth.py\" in m.content]\n        other_tools = [m for m in tool_messages_after if \"auth.py\" not in m.content]\n\n        assert len(other_tools) == 0, \"Irrelevant tool results should be removed\"\n        assert len(auth_tools) > 0, \"Relevant tool results should remain\"\n\n        logger.success(\"Context pruning test passed successfully\")\n\n        return {\n            \"original_count\": original_count,\n            \"pruned_count\": pruned_count,\n            \"removed_count\": original_count - pruned_count,\n            \"files_mentioned\": [\"app/services/auth.py\"]\n        }\n\n    except AssertionError as e:\n        logger.error(f\"Assertion failed: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in pruning test: {e}\")\n        raise\n\n\nasync def cleanup_test_database() -> bool:\n    \"\"\"Clean up test database file.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\"Cleaning up test database...\")\n\n    try:\n        db_path = Path(\"tests/test_history.db\")\n        if db_path.exists():\n            gc.collect()  # Force garbage collection to close DB handles\n            await asyncio.sleep(0.5)  # Give Windows time to release file lock\n            db_path.unlink()\n            logger.success(f\"Removed test database: {db_path}\")\n            return True\n        else:\n            logger.info(\"Test database does not exist, nothing to clean up\")\n            return True\n    except Exception as e:\n        logger.error(f\"Failed to clean up test database: {e}\")\n        return False\n\n\nasync def main() -> int:\n    \"\"\"Main test runner.\"\"\"\n    print(\"\\n\" + \"=\" * 40)\n    print(\"=== HISTORY MANAGEMENT TEST SUITE ===\")\n    print(\"=\" * 40 + \"\\n\")\n\n    # Setup logging\n    setup_logging()\n    logger = logging.getLogger(__name__)\n\n    # Test results tracker\n    test_results = {}\n\n    try:\n        # Test 1: Imports\n        logger.info(\"Starting test: Imports\")\n        test_results[\"imports\"] = test_imports()\n\n        if not test_results[\"imports\"]:\n            logger.error(\"Import test failed. Skipping further tests.\")\n            return 1\n\n        # Test 2: API Connection\n        logger.info(\"Starting test: API Connection\")\n        test_results[\"api_connection\"] = await test_api_connection()\n\n        if not test_results[\"api_connection\"]:\n            logger.warning(\"API connection test failed. Some tests may be limited.\")\n\n        # Test 3: Create History\n        logger.info(\"Starting test: Create History\")\n        try:\n            thread_id, manager = test_create_history()\n            test_results[\"create_history\"] = True\n        except Exception:\n            test_results[\"create_history\"] = False\n            thread_id, manager = None, None\n\n        # Test 4: History Compression (if history created successfully)\n        if test_results.get(\"create_history\") and thread_id and manager:\n            logger.info(\"Starting test: History Compression\")\n            try:\n                compression_stats = await test_history_compression(manager, thread_id)\n                test_results[\"history_compression\"] = compression_stats.get(\"compression_happened\", False)\n            except Exception:\n                test_results[\"history_compression\"] = False\n        else:\n            logger.warning(\"Skipping compression test - history creation failed\")\n            test_results[\"history_compression\"] = False\n\n        # Test 5: Prune Context\n        logger.info(\"Starting test: Prune Context\")\n        try:\n            prune_stats = test_prune_context()\n            test_results[\"prune_context\"] = prune_stats.get(\"removed_count\", 0) > 0\n        except Exception:\n            test_results[\"prune_context\"] = False\n\n        # Cleanup\n        logger.info(\"Starting cleanup\")\n        test_results[\"cleanup\"] = await cleanup_test_database()\n\n    except KeyboardInterrupt:\n        logger.error(\"Test suite interrupted by user\")\n        return 1\n    except Exception as e:\n        logger.exception(f\"Unexpected error in main test runner: {e}\")\n        return 1\n\n    # Print summary\n    print(\"\\n\" + \"=\" * 40)\n    print(\"=== TEST RESULTS ===\")\n    print(\"=\" * 40)\n\n    passed = sum(1 for result in test_results.values() if result is True)\n    total = sum(1 for result in test_results.values() if isinstance(result, bool))\n\n    print(f\"\\nSummary: {passed}/{total} tests passed\")\n\n    for test_name, result in test_results.items():\n        status = \"✓ PASS\" if result else \"✗ FAIL\"\n        color = '\\033[92m' if result else '\\033[91m'\n        reset = '\\033[0m'\n        print(f\"  {color}{status}{reset}: {test_name}\")\n\n    print(\"\\n\" + \"=\" * 40)\n\n    # Return exit code\n    if passed == total:\n        logger.success(\"All tests passed!\")\n        return 0\n    else:\n        logger.error(f\"{total - passed} test(s) failed\")\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit_code = asyncio.run(main())\n    sys.exit(exit_code)",
      "explanation": "Этот код вносит несколько важных исправлений в тестовый скрипт для менеджера истории:\n\n1. **Добавлен",
      "error": null,
      "model_used": "deepseek-chat",
      "tokens_used": 0
    }
  }
}