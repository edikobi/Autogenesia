# ğŸ¤– AI Code Agent - Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¢ĞµÑÑ‚

**Ğ”Ğ°Ñ‚Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ:** 20.12.2025 06:12:16
**ĞŸÑ€Ğ¾ĞµĞºÑ‚:** `C:\Users\Admin\AI_Assistant_Pro`
**Ğ’Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ:** 212.73 ÑĞµĞº.

---

## ğŸ“ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ

> Ğ£ Ğ¼ĞµĞ½Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ General Chat, Ğ° Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ¸ Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸ ĞºĞ¾Ğ´ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ  File "c:\Users\Admin\AI_Assistant_Pro\scripts\test_general_chat.py", line 128, in main    result: GeneralChatResult = await orchestrator.orchestrate_general(                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  File "c:\Users\Admin\AI_Assistant_Pro\app\agents\orchestrator.py", line 850, in orchestrate_general    prompts = format_orchestrator_prompt_general(              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^TypeError: format_orchestrator_prompt_general() got an unexpected keyword argument 'user_query'

---

## ğŸ¯ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

- **Orchestrator:** GPT-5.1 Codex Max (RouterAI)
- **Code Generator:** deepseek-chat

---

## ğŸ” ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ°

ROOT CAUSE: The General Chat orchestrator calls `format_orchestrator_prompt_general` with `user_query`, `user_files`, and `remaining_web_searches`, expecting a dict with `"system"` and `"user"` prompts, but the function currently accepts only `is_legal_mode` and returns a single string. This signature mismatch causes `TypeError: unexpected keyword argument 'user_query'`.

---

## ğŸ“‹ Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Code Generator

SCOPE: A (single function in one file)

**Task:** Update the General Chat prompt formatter to accept the parameters used by `orchestrate_general` and return both system and user prompts as a dict.

**File:** `app/llm/prompt_templates.py`

**Target Location:**
- Function: `format_orchestrator_prompt_general`
- Lines: 2111-2192 (per current file)
- Identifying pattern: `def format_orchestrator_prompt_general(is_legal_mode: bool = False) -> str:`

**Current Code (for reference):**
```
def format_orchestrator_prompt_general(is_legal_mode: bool = False) -> str:
    """
    Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° General Chat.
    Args:
        is_legal_mode: Ğ•ÑĞ»Ğ¸ True, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºÑƒ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚ÑŒ.
    """
    prompt_parts = []
    ...
    return "\n".join(prompt_parts)
```

**Required Changes:**
1. Change the signature to accept `user_query`, optional `user_files`, `is_legal_mode`, and `remaining_web_searches` with default `MAX_WEB_SEARCH_CALLS`.
2. Return a `Dict[str, str]` with keys `"system"` and `"user"` instead of a single string.
3. Preserve the existing system prompt content, but include a line showing remaining web search calls (`remaining_web_searches` out of `MAX_WEB_SEARCH_CALLS`).
4. Build a user prompt that includes the user query and, if provided, a compact list of attached files (filename and content). Default to an empty list if `user_files` is None.
5. Update typing imports to include `List` alongside existing `Dict` if not already imported.

**New/Modified Code:**
```
from typing import Dict, List  # ensure List is included with existing typing imports

def format_orchestrator_prompt_general(
    user_query: str,
    user_files: List[Dict[str, str]] = None,
    is_legal_mode: bool = False,
    remaining_web_searches: int = MAX_WEB_SEARCH_CALLS,
) -> Dict[str, str]:
    """
    Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° General Chat.
    Args:
        user_query: Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
        user_files: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ²Ğ¸Ğ´Ğ° {"filename": ..., "content": ...}
        is_legal_mode: Ğ•ÑĞ»Ğ¸ True, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºÑƒ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚ÑŒ.
        remaining_web_searches: Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ web_search ĞµÑ‰Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ
    """
    user_files = user_files or []

    prompt_parts = []
    # --- ROLE DEFINITION ---
    if is_legal_mode:
        prompt_parts.append("Ğ¢Ñ‹ â€” Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ğ½Ñ‚ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²Ñ‹ÑÑˆĞµĞ¹ ĞºĞ²Ğ°Ğ»Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.")
        prompt_parts.append("Ğ¢Ğ²Ğ¾Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ Ğ¤, Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¾ Ğ¸ ÑÑƒĞ´ĞµĞ±Ğ½Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°.")
        prompt_parts.append("Ğ¢Ğ²Ğ¾Ñ Ñ†ĞµĞ»ÑŒ: Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³Ñ€Ğ°Ğ¼Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ñ‹.")
    else:
        prompt_parts.append("Ğ¢Ñ‹ â€” Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI-Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚.")
        prompt_parts.append("Ğ¢Ğ²Ğ¾Ñ Ñ†ĞµĞ»ÑŒ: Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹.")
        prompt_parts.append("Ğ¢Ñ‹ ÑƒĞ¼ĞµĞµÑˆÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸.")

    prompt_parts.append("")
    prompt_parts.append(f"ĞÑÑ‚Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² web_search: {remaining_web_searches} Ğ¸Ğ· {MAX_WEB_SEARCH_CALLS}.")
    prompt_parts.append("")

    # --- AVAILABLE TOOLS & PHILOSOPHY ---
    prompt_parts.append("Ğ”ĞĞ¡Ğ¢Ğ£ĞŸĞĞ«Ğ• Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞœĞ•ĞĞ¢Ğ«")
    prompt_parts.append("- general_web_search(query, time_limit, max_results): ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ (Google/DDG).")
    prompt_parts.append("  Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ 'time_limit'='w' (Ğ½ĞµĞ´ĞµĞ»Ñ) Ğ¸Ğ»Ğ¸ 'm' (Ğ¼ĞµÑÑÑ†) Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ²ĞµĞ¶Ğ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ².")
    prompt_parts.append("")

    # =========================================================================
    # CRITICAL FIX: Ğ¯Ğ²Ğ½Ğ¾Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
    # =========================================================================
    prompt_parts.append("ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ™ WORKFLOW (Ğ’ĞĞ–ĞĞ!)")
    prompt_parts.append("Ğ¢Ğ²Ğ¾Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²:")
    prompt_parts.append("")
    prompt_parts.append("Ğ­Ğ¢ĞĞŸ 1: ĞŸĞĞ˜Ğ¡Ğš Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ˜ (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾)")
    prompt_parts.append("Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.")
    prompt_parts.append("")
    prompt_parts.append("Ğ­Ğ¢ĞĞŸ 2: Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ™ ĞĞ¢Ğ’Ğ•Ğ¢ ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞ¢Ğ•Ğ›Ğ® (ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ!)")
    prompt_parts.append("ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚Ñ‹ Ğ”ĞĞ›Ğ–Ğ•Ğ:")
    prompt_parts.append("â€¢ ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ")
    prompt_parts.append("â€¢ Ğ¡Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ Ğ£Ğ¡Ğ¡ĞšĞĞœ ÑĞ·Ñ‹ĞºĞµ")
    prompt_parts.append("â€¢ ĞŸÑ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸")
    prompt_parts.append("")
    prompt_parts.append("âš ï¸ ĞĞ• ĞĞ¡Ğ¢ĞĞĞĞ’Ğ›Ğ˜Ğ’ĞĞ™Ğ¡Ğ¯ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°.")
    prompt_parts.append("âš ï¸ Ğ’Ğ¡Ğ•Ğ“Ğ”Ğ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞ¹ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.")
    prompt_parts.append("")

    prompt_parts.append("Ğ¤Ğ˜Ğ›ĞĞ¡ĞĞ¤Ğ˜Ğ¯ Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞĞ˜Ğ¯ ĞŸĞĞ˜Ğ¡ĞšĞ (TOOL USAGE STRATEGY)")
    prompt_parts.append("Ğ¢Ñ‹ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑˆÑŒ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¼Ğ¸Ñ€ Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºĞ°Ğ¶Ğ´ÑƒÑ ÑĞµĞºÑƒĞ½Ğ´Ñƒ.")
    prompt_parts.append("1. ĞŸĞ Ğ˜ĞĞ¦Ğ˜ĞŸ ĞĞ•ĞĞ‘Ğ¥ĞĞ”Ğ˜ĞœĞĞ¡Ğ¢Ğ˜: ĞĞµ Ğ¸Ñ‰Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±Ñ‰ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ¼ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 'ÑÑ‚Ğ¾Ğ»Ğ¸Ñ†Ğ° Ğ¤Ñ€Ğ°Ğ½Ñ†Ğ¸Ğ¸'). Ğ˜Ñ‰Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµÑ‚ÑŒ (ĞºÑƒÑ€ÑÑ‹ Ğ²Ğ°Ğ»ÑÑ‚, Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹) Ğ¸Ğ»Ğ¸ Ñ‡ĞµĞ³Ğ¾ Ñ‚Ñ‹ Ğ½Ğµ Ğ·Ğ½Ğ°ĞµÑˆÑŒ (ÑĞ²ĞµĞ¶Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ° ÑƒĞ·ĞºĞ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸).")
    prompt_parts.append("2. ĞŸĞ Ğ˜ĞĞ¦Ğ˜ĞŸ Ğ¢ĞĞ§ĞĞĞ¡Ğ¢Ğ˜ Ğ—ĞĞŸĞ ĞĞ¡Ğ: Ğ¢Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ² Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°.")
    prompt_parts.append("   - ĞŸĞ»Ğ¾Ñ…Ğ¾: 'Ğ¡ĞºĞ°Ğ¶Ğ¸ Ğ¼Ğ½Ğµ, ĞºĞ°ĞºĞ¸Ğµ Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ Ğ˜ĞŸ Ğ² 2025 Ğ³Ğ¾Ğ´Ñƒ?'")
    prompt_parts.append("   - Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾: 'Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ˜ĞŸ 2025 Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ Ğ¤'")
    if is_legal_mode:
        prompt_parts.append("3. Ğ®Ğ Ğ˜Ğ”Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ¡ĞŸĞ•Ğ¦Ğ˜Ğ¤Ğ˜ĞšĞ: ĞŸÑ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ½Ğ¾Ğ¼ĞµÑ€ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´ĞµĞºÑĞ°, ĞµÑĞ»Ğ¸ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾. Ğ•ÑĞ»Ğ¸ Ğ¸Ñ‰ĞµÑˆÑŒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºÑƒ â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ 'ÑÑƒĞ´ĞµĞ±Ğ½Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°' Ğ¸Ğ»Ğ¸ 'Ğ¿Ğ»ĞµĞ½ÑƒĞ¼ Ğ’Ğ¡ Ğ Ğ¤'.")
    else:
        prompt_parts.append("3. Ğ˜Ğ¢Ğ•Ğ ĞĞ¢Ğ˜Ğ’ĞĞĞ¡Ğ¢Ğ¬: Ğ•ÑĞ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğµ Ğ´Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°, Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹ Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹.")
    prompt_parts.append("")

    # --- OUTPUT FORMATTING ---
    prompt_parts.append("Ğ¤ĞĞ ĞœĞĞ¢ ĞĞ¢Ğ’Ğ•Ğ¢Ğ")
    prompt_parts.append("Ğ¢Ñ‹ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ Ğ£Ğ¡Ğ¡ĞšĞĞœ ÑĞ·Ñ‹ĞºĞµ (ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ½Ğµ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ğ» Ğ¸Ğ½Ğ¾Ğµ).")
    prompt_parts.append("Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Markdown Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹:")
    prompt_parts.append("- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ (

---

## ğŸ’» Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´

**Ğ¤Ğ°Ğ¹Ğ»:** `app/llm/prompt_templates.py`

```python
"""
Prompt templates for AI Code Agent roles.

Each role has:
- SYSTEM prompt (defines behavior and output format)
- USER prompt template (with placeholders for variables)

NEW: Adaptive blocks for different model cognitive types:
- GPT-5.1 Codex Max (executor): Standard prompts, no modifications
- Claude Sonnet 4.5 / Opus 4.5 (deep_thinker): Additional guidance for concrete instructions
- DeepSeek V3.2 Reasoning (reasoner): Leverages reasoning capabilities

Prompts are in English for better model performance.
Uses prompt_parts.append pattern for clean multi-line prompts.

CENTRALIZED PROMPT STORAGE:
- Router prompts: stored in app/agents/router.py (co-located with routing logic)
- All other prompts: stored here
"""

from typing import Dict, List
from config.settings import Config
# ============================================================================
# CONSTANTS (shared across prompts)
# ============================================================================

MAX_WEB_SEARCH_CALLS = 3  # Maximum web_search calls per session


# ============================================================================
# MODEL COGNITIVE TYPES
# ============================================================================

# Exact model IDs from config/settings.py for reference:
# - Claude Opus 4.5:    "anthropic/claude-opus-4.5"
# - Claude Sonnet 4.5:  "anthropic/claude-sonnet-4.5"
# - GPT-5.1 Codex Max:  "openai/gpt-5.1-codex-max"
# - Gemini 3.0 Pro:     "google/gemini-3-pro-preview"
# - Gemini 2.0 Flash:   "google/gemini-2.0-flash-001"
# - DeepSeek Reasoner:  "deepseek-reasoner"
# - DeepSeek Chat:      "deepseek-chat"

# Mapping of EXACT model IDs to their cognitive types
# Used as fallback after fuzzy matching
MODEL_COGNITIVE_TYPES: Dict[str, str] = {
    # Deep Thinker - ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸
    # ĞÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…
    Config.MODEL_OPUS_4_5: "deep_thinker",      # "anthropic/claude-opus-4.5"
    Config.MODEL_SONNET_4_5: "deep_thinker",    # "anthropic/claude-sonnet-4.5"
    
    # Executor - Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡
    # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹
    Config.MODEL_GPT_5_1_Codex_MAX: "executor", # "openai/gpt-5.1-codex-max"
    Config.MODEL_GEMINI_3_PRO: "executor", # "google/gemini-3-pro-preview"
    
    # Reasoner - Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹
    # ĞœĞ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ½ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° "Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ"
    Config.MODEL_DEEPSEEK_REASONER: "reasoner", # "deepseek-reasoner"
}

def get_model_cognitive_type(model_id: str) -> str:
    """
    Determine the cognitive type of a model.
    
    Uses FUZZY MATCHING to handle variations in model IDs from different
    providers (RouterAI, OpenRouter, direct API). This is critical because
    the same model can have different IDs:
    - "anthropic/claude-sonnet-4.5" (OpenRouter style)
    - "claude-sonnet-4.5" (short form)
    - "Claude Sonnet 4.5 (RouterAI)" (display name - should not happen but safe)
    
    Args:
        model_id: Model identifier (e.g., "anthropic/claude-opus-4.5")
        
    Returns:
        Cognitive type: "deep_thinker", "executor", "reasoner", or "general"
    """
    if not model_id:
        return "general"
    
    # Normalize for comparison
    model_lower = model_id.lower()
    
    # === CLAUDE FAMILY â†’ deep_thinker ===
    # Matches: claude-opus-4.5, claude-sonnet-4.5, anthropic/claude-3.5-sonnet, etc.
    if "claude" in model_lower:
        # Opus and Sonnet variants are deep thinkers
        if any(variant in model_lower for variant in ["opus", "sonnet"]):
            return "deep_thinker"
    
    # === GEMINI FAMILY ===
    if "gemini" in model_lower:
        # Gemini Pro models (3.0, 2.5, etc.) are deep thinkers
        if "pro" in model_lower or "ultra" in model_lower:
            return "deep_thinker"
        # Gemini Flash models are executors (fast, less reasoning)
        if "flash" in model_lower:
            return "executor"
    
    # === GPT FAMILY â†’ executor ===
    # Matches: gpt-5.1-codex-max, openai/gpt-5.1-codex-max, etc.
    if "gpt" in model_lower:
        # GPT-5.x and Codex models are executors
        if "5" in model_lower or "codex" in model_lower:
            return "executor"
    
    # === DEEPSEEK FAMILY ===
    if "deepseek" in model_lower:
        # DeepSeek Reasoner (R1) is a reasoner
        if "reason" in model_lower or "r1" in model_lower:
            return "reasoner"
        # DeepSeek Chat is an executor
        if "chat" in model_lower:
            return "executor"
    
    # === FALLBACK: Exact match from dictionary ===
    if model_id in MODEL_COGNITIVE_TYPES:
        return MODEL_COGNITIVE_TYPES[model_id]
    
    # === DEFAULT ===
    return "general"

# ============================================================================
# ADAPTIVE BLOCKS FOR ORCHESTRATOR
# ============================================================================


def _build_adaptive_block_ask_deep_thinker() -> str:
    """Build adaptive block for deep_thinker models (Claude Opus/Sonnet) in ASK mode"""
    prompt_parts: List[str] = []
    
    prompt_parts.append("")
    prompt_parts.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    prompt_parts.append("ğŸ¤ ORCHESTRATOR-WORKER COLLABORATION PROTOCOL")
    prompt_parts.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    prompt_parts.append("")
    prompt_parts.append("<delegation_context>")
    prompt_parts.append("You are the LEAD AGENT in a multi-agent system.")
    prompt_parts.append("Your output will be consumed by a WORKER AGENT (Code Generator) that:")
    prompt_parts.append("â€¢ Operates with an isolated context window")
    prompt_parts.append("â€¢ Has no access to your analysis, tool results, or conversation history")
    prompt_parts.append("â€¢ Receives ONLY the 'Instruction for Code Generator' section")
    prompt_parts.append("")
    prompt_parts.append("Your delegation must include:")
    prompt_parts.append("")
    prompt_parts.append("1. OBJECTIVE: What should the worker achieve?")
    prompt_parts.append("   Example: 'Add input validation to the login function'")
    prompt_parts.append("")
    prompt_parts.append("2. OUTPUT FORMAT: What deliverable should the worker produce?")
    prompt_parts.append("   Example: 'Modified function with try-except block catching ValueError'")
    prompt_parts.append("")
    prompt_parts.append("3. TOOL GUIDANCE: What code patterns/imports should the worker use?")
    prompt_parts.append("   Example: 'Import: from typing import Optional; Pattern: return None on error'")
    prompt_parts.append("")
    prompt_parts.append("4. TASK BOUNDARIES: What should the worker NOT modify?")
    prompt_parts.append("   Example: 'Do not change the function signature or return type'")
    prompt_parts.append("")
    prompt_parts.append("</delegation_context>")
    prompt_parts.append("")
    prompt_parts.append("<division_of_labor>")
    prompt_parts.append("")
    prompt_parts.append("YOUR ROLE (Orchestrator):")
    prompt_parts.append("â€¢ Analyze the problem and identify root cause")
    prompt_parts.append("â€¢ Use tools to gather necessary code context")
    prompt_parts.append("â€¢ Decide WHAT needs to change and WHY")
    prompt_parts.append("")
    prompt_parts.append("WORKER'S ROLE (Code Generator):")
    prompt_parts.append("â€¢ Receive your instruction with complete context")
    prompt_parts.append("â€¢ Write/modify code based on your specification")
    prompt_parts.append("â€¢ Execute the HOW based on your WHAT/WHY")
    prompt_parts.append("")
    prompt_parts.append("HANDOFF QUALITY CHECK:")
    prompt_parts.append("Before submitting, verify your instruction contains:")
    prompt_parts.append("âœ“ Sufficient context (worker can understand the problem)")
    prompt_parts.append("âœ“ Precise location (file path + method/class + insertion point)")
    prompt_parts.append("âœ“ Actual code snippets (not descriptions like 'add validation')")
    prompt_parts.append("âœ“ All necessary imports explicitly listed")
    prompt_parts.append("")
    prompt_parts.append("</division_of_labor>")
    
# =========================================================================
# INSTRUCTION COMPLETENESS (following Anthropic delegation framework)
# =========================================================================
    prompt_parts.append("<instruction_completeness>")
    prompt_parts.append("")
    prompt_parts.append("After using tools to analyze the problem, compose a complete instruction")
    prompt_parts.append("for the Code Generator that follows this delegation framework:")
    prompt_parts.append("")
    prompt_parts.append("1. OBJECTIVE (What should be achieved):")
    prompt_parts.append("   State the goal in one clear sentence.")
    prompt_parts.append("   Template: 'Modify {component_name} to {desired_behavior}'")
    prompt_parts.append("")
    prompt_parts.append("2. OUTPUT FORMAT (What the worker should produce):")
    prompt_parts.append("   Specify the deliverable with actual code blocks.")
    prompt_parts.append("   ")
    prompt_parts.append("   Structure:")
    prompt_parts.append("   FILE: {full_file_path}")
    prompt_parts.append("   LOCATION: {where_to_apply_change}")
    prompt_parts.append("   ACTION: INSERT | REPLACE | DELETE")
    prompt_parts.append("   ")
    prompt_parts.append("   CODE:")
    prompt_parts.append("
```

---

## ğŸ“– ĞŸĞ¾ÑÑĞ½ĞµĞ½Ğ¸Ñ Ğº ĞºĞ¾Ğ´Ñƒ

")
    prompt_parts.append("   {complete_runnable_code}")
    prompt_parts.append("   ```")
    prompt_parts.append("")
    prompt_parts.append("   Include:")
    prompt_parts.append("   â€¢ All necessary imports at the top")
    prompt_parts.append("   â€¢ Complete function/method signatures with type hints")
    prompt_parts.append("   â€¢ Exact variable names and parameter lists")
    prompt_parts.append("")
    prompt_parts.append("3. TOOL GUIDANCE (How to implement):")
    prompt_parts.append("   Provide implementation context the worker needs:")
    prompt_parts.append("   â€¢ Which design patterns to follow (if project has conventions)")
    prompt_parts.append("   â€¢ What error handling strategy to use")
    prompt_parts.append("   â€¢ Any project-specific utilities to leverage")
    prompt_parts.append("")
    prompt_parts.append("4. TASK BOUNDARIES (What NOT to change):")
    prompt_parts.append("   Explicitly state constraints:")
    prompt_parts.append("   â€¢ Which parts of the code should remain untouched")
    prompt_parts.append("   â€¢ Which APIs/interfaces must stay compatible")
    prompt_parts.append("   â€¢ What scope limits apply (single file vs. multi-file)")
    prompt_parts.append("")
    prompt_parts.append("5. CONTEXT BRIEFING (Why this matters):")
    prompt_parts.append("   Explain the reasoning so the worker understands:")
    prompt_parts.append("   â€¢ ROOT CAUSE: One sentence explaining the fundamental issue")
    prompt_parts.append("   â€¢ EXPECTED BEHAVIOR: What should happen after the change")
    prompt_parts.append("   â€¢ DEPENDENCIES: Other components that might be affected")
    prompt_parts.append("")
    prompt_parts.append("</instruction_completeness>")
    prompt_parts.append("")
    prompt_parts.append("<quality_checklist>")
    prompt_parts.append("")
    prompt_parts.append("Before submitting your instruction, verify:")
    prompt_parts.append("âœ“ Code blocks contain implementations (not descriptions like 'add validation')")
    prompt_parts.append("âœ“ Location markers use patterns from the actual file you read")
    prompt_parts.append("âœ“ All imports are explicitly listed with full module paths")
    prompt_parts.append("âœ“ The worker could execute this without asking follow-up questions")
    prompt_parts.append("âœ“ You copied relevant existing code patterns from tool results")
    prompt_parts.append("")
    prompt_parts.append("</quality_checklist>")
        
    # =========================================================================
    # HOLISTIC FIXING (positive framing)
    # =========================================================================
    prompt_parts.append("<holistic_fixing>")
    prompt_parts.append("")
    prompt_parts.append("When you identify a bug, scan the entire file for similar patterns.")
    prompt_parts.append("Batch all related fixes into a single instruction block.")
    prompt_parts.append("Focus on critical bugs (crashes, logic errors) and skip style changes.")
    prompt_parts.append("")
    prompt_parts.append("</holistic_fixing>")
    prompt_parts.append("")
    
    # =========================================================================
    # VERIFICATION STEP
    # =========================================================================
    prompt_parts.append("<self_verification>")
    prompt_parts.append("")
    prompt_parts.append("Before submitting your instruction, verify:")
    prompt_parts.append("âœ“ Code blocks contain actual implementations (not pseudocode)")
    prompt_parts.append("âœ“ All imports are listed explicitly")
    prompt_parts.append("âœ“ File paths are complete and accurate")
    prompt_parts.append("âœ“ Location markers are precise enough to find the spot")
    prompt_parts.append("")
    prompt_parts.append("</self_verification>")
    
    return "\n".join(prompt_parts)


def _build_adaptive_block_ask_reasoner() -> str:
    """Build adaptive block for reasoner models (DeepSeek V3.2) in ASK mode"""
    prompt_parts: List[str] = []
    
    prompt_parts.append("")
    prompt_parts.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    prompt_parts.append("ğŸ§  REASONING-FIRST ORCHESTRATION PROTOCOL")
    prompt_parts.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    prompt_parts.append("")
    
    # =========================================================================
    # LEVERAGE YOUR REASONING STRENGTHS
    # =========================================================================
    prompt_parts.append("<reasoning_strengths>")
    prompt_parts.append("")
    prompt_parts.append("Your reasoning model excels at:")
    prompt_parts.append("â€¢ Multi-step logical inference")
    prompt_parts.append("â€¢ Pattern identification across large codebases")
    prompt_parts.append("â€¢ Tracing consequence chains through dependencies")
    prompt_parts.append("â€¢ Comprehensive code analysis")
    prompt_parts.append("")
    prompt_parts.append("Apply these strengths to orchestration:")
    prompt_parts.append("")
    prompt_parts.append("DEPENDENCY REASONING PATTERN:")
    prompt_parts.append("When analyzing a change, reason through:")
    prompt_parts.append("1. IF we modify component X in module M,")
    prompt_parts.append("2. THEN which components import from M? (upstream impact)")
    prompt_parts.append("3. AND which components does M import? (downstream dependencies)")
    prompt_parts.append("4. THEREFORE, what is the ripple effect scope?")
    prompt_parts.append("")
    prompt_parts.append("Use this chain to predict:")
    prompt_parts.append("â€¢ Breaking changes (API modifications)")
    prompt_parts.append("â€¢ Hidden circular dependency risks")
    prompt_parts.append("â€¢ Integration points that need updates")
    prompt_parts.append("")
    prompt_parts.append("</reasoning_strengths>")
    prompt_parts.append("")
    
    # =========================================================================
    # COMPENSATE FOR SPARSE ATTENTION (FILE DETECTION)
    # =========================================================================
    prompt_parts.append("<file_detection_strategy>")
    prompt_parts.append("")
    prompt_parts.append("IMPORTANT: Your sparse attention mechanism optimizes for efficiency,")
    prompt_parts.append("but may filter out relevant files if they don't match initial patterns.")
    prompt_parts.append("")
    prompt_parts.append("MANDATORY FILE DISCOVERY PROTOCOL:")
    prompt_parts.append("")
    prompt_parts.append("1. EXPLICIT SEARCH BEFORE REASONING:")
    prompt_parts.append("   Before reasoning about the problem, actively SEARCH for files.")
    prompt_parts.append("   ")
    prompt_parts.append("   Use tools to force file visibility:")
    prompt_parts.append("   â€¢ search_code({keyword_from_user_query}) â†’ Find ALL mentions")
    prompt_parts.append("   â€¢ read_file({config_path}) â†’ Check configuration for related modules")
    prompt_parts.append("   â€¢ search_code({function_name}) â†’ Locate definitions and usages")

---

## ğŸ› ï¸ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²

1. âœ… **read_code_chunk**
   - ĞÑ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: `file_path=app/llm/prompt_templates.py, chunk_name=format_orchestrator_prompt_general`

2. âœ… **read_code_chunk**
   - ĞÑ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: `file_path=app/agents/orchestrator.py, chunk_name=orchestrate_general`

3. âœ… **search_code**
   - ĞÑ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: `query=format_orchestrator_prompt_general(, search_type=function`

4. âœ… **read_code_chunk**
   - ĞÑ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: `file_path=app/llm/prompt_templates.py, chunk_name=format_orchestrator_prompt_ask`

5. âœ… **read_code_chunk**
   - ĞÑ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: `file_path=app/llm/prompt_templates.py, chunk_name=format_orchestrator_prompt`

---

*ĞÑ‚Ñ‡ĞµÑ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸: 2025-12-20T06:12:16.379309*