{
  "metadata": {
    "timestamp": "2025-12-20T06:12:16.378306",
    "project_dir": "C:\\Users\\Admin\\AI_Assistant_Pro",
    "user_query": "Ğ£ Ğ¼ĞµĞ½Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ General Chat, Ğ° Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ¸ Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸ ĞºĞ¾Ğ´ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ  File \"c:\\Users\\Admin\\AI_Assistant_Pro\\scripts\\test_general_chat.py\", line 128, in main    result: GeneralChatResult = await orchestrator.orchestrate_general(                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  File \"c:\\Users\\Admin\\AI_Assistant_Pro\\app\\agents\\orchestrator.py\", line 850, in orchestrate_general    prompts = format_orchestrator_prompt_general(              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^TypeError: format_orchestrator_prompt_general() got an unexpected keyword argument 'user_query'",
    "duration_seconds": 212.73429012298584,
    "models_used": {
      "orchestrator": "GPT-5.1 Codex Max (RouterAI)",
      "code_generator": "deepseek-chat"
    }
  },
  "orchestrator": {
    "analysis": "ROOT CAUSE: The General Chat orchestrator calls `format_orchestrator_prompt_general` with `user_query`, `user_files`, and `remaining_web_searches`, expecting a dict with `\"system\"` and `\"user\"` prompts, but the function currently accepts only `is_legal_mode` and returns a single string. This signature mismatch causes `TypeError: unexpected keyword argument 'user_query'`.",
    "instruction": "SCOPE: A (single function in one file)\n\n**Task:** Update the General Chat prompt formatter to accept the parameters used by `orchestrate_general` and return both system and user prompts as a dict.\n\n**File:** `app/llm/prompt_templates.py`\n\n**Target Location:**\n- Function: `format_orchestrator_prompt_general`\n- Lines: 2111-2192 (per current file)\n- Identifying pattern: `def format_orchestrator_prompt_general(is_legal_mode: bool = False) -> str:`\n\n**Current Code (for reference):**\n```\ndef format_orchestrator_prompt_general(is_legal_mode: bool = False) -> str:\n    \"\"\"\n    Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° General Chat.\n    Args:\n        is_legal_mode: Ğ•ÑĞ»Ğ¸ True, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºÑƒ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚ÑŒ.\n    \"\"\"\n    prompt_parts = []\n    ...\n    return \"\\n\".join(prompt_parts)\n```\n\n**Required Changes:**\n1. Change the signature to accept `user_query`, optional `user_files`, `is_legal_mode`, and `remaining_web_searches` with default `MAX_WEB_SEARCH_CALLS`.\n2. Return a `Dict[str, str]` with keys `\"system\"` and `\"user\"` instead of a single string.\n3. Preserve the existing system prompt content, but include a line showing remaining web search calls (`remaining_web_searches` out of `MAX_WEB_SEARCH_CALLS`).\n4. Build a user prompt that includes the user query and, if provided, a compact list of attached files (filename and content). Default to an empty list if `user_files` is None.\n5. Update typing imports to include `List` alongside existing `Dict` if not already imported.\n\n**New/Modified Code:**\n```\nfrom typing import Dict, List  # ensure List is included with existing typing imports\n\ndef format_orchestrator_prompt_general(\n    user_query: str,\n    user_files: List[Dict[str, str]] = None,\n    is_legal_mode: bool = False,\n    remaining_web_searches: int = MAX_WEB_SEARCH_CALLS,\n) -> Dict[str, str]:\n    \"\"\"\n    Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° General Chat.\n    Args:\n        user_query: Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ\n        user_files: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ²Ğ¸Ğ´Ğ° {\"filename\": ..., \"content\": ...}\n        is_legal_mode: Ğ•ÑĞ»Ğ¸ True, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºÑƒ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚ÑŒ.\n        remaining_web_searches: Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ web_search ĞµÑ‰Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ\n    \"\"\"\n    user_files = user_files or []\n\n    prompt_parts = []\n    # --- ROLE DEFINITION ---\n    if is_legal_mode:\n        prompt_parts.append(\"Ğ¢Ñ‹ â€” Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ğ½Ñ‚ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²Ñ‹ÑÑˆĞµĞ¹ ĞºĞ²Ğ°Ğ»Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.\")\n        prompt_parts.append(\"Ğ¢Ğ²Ğ¾Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ Ğ¤, Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¾ Ğ¸ ÑÑƒĞ´ĞµĞ±Ğ½Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°.\")\n        prompt_parts.append(\"Ğ¢Ğ²Ğ¾Ñ Ñ†ĞµĞ»ÑŒ: Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³Ñ€Ğ°Ğ¼Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ñ‹.\")\n    else:\n        prompt_parts.append(\"Ğ¢Ñ‹ â€” Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI-Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚.\")\n        prompt_parts.append(\"Ğ¢Ğ²Ğ¾Ñ Ñ†ĞµĞ»ÑŒ: Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹.\")\n        prompt_parts.append(\"Ğ¢Ñ‹ ÑƒĞ¼ĞµĞµÑˆÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸.\")\n\n    prompt_parts.append(\"\")\n    prompt_parts.append(f\"ĞÑÑ‚Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² web_search: {remaining_web_searches} Ğ¸Ğ· {MAX_WEB_SEARCH_CALLS}.\")\n    prompt_parts.append(\"\")\n\n    # --- AVAILABLE TOOLS & PHILOSOPHY ---\n    prompt_parts.append(\"Ğ”ĞĞ¡Ğ¢Ğ£ĞŸĞĞ«Ğ• Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞœĞ•ĞĞ¢Ğ«\")\n    prompt_parts.append(\"- general_web_search(query, time_limit, max_results): ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ (Google/DDG).\")\n    prompt_parts.append(\"  Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ 'time_limit'='w' (Ğ½ĞµĞ´ĞµĞ»Ñ) Ğ¸Ğ»Ğ¸ 'm' (Ğ¼ĞµÑÑÑ†) Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ²ĞµĞ¶Ğ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ².\")\n    prompt_parts.append(\"\")\n\n    # =========================================================================\n    # CRITICAL FIX: Ğ¯Ğ²Ğ½Ğ¾Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°\n    # =========================================================================\n    prompt_parts.append(\"ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ™ WORKFLOW (Ğ’ĞĞ–ĞĞ!)\")\n    prompt_parts.append(\"Ğ¢Ğ²Ğ¾Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Ğ­Ğ¢ĞĞŸ 1: ĞŸĞĞ˜Ğ¡Ğš Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ˜ (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾)\")\n    prompt_parts.append(\"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Ğ­Ğ¢ĞĞŸ 2: Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ™ ĞĞ¢Ğ’Ğ•Ğ¢ ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞ¢Ğ•Ğ›Ğ® (ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ!)\")\n    prompt_parts.append(\"ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚Ñ‹ Ğ”ĞĞ›Ğ–Ğ•Ğ:\")\n    prompt_parts.append(\"â€¢ ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ\")\n    prompt_parts.append(\"â€¢ Ğ¡Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ Ğ£Ğ¡Ğ¡ĞšĞĞœ ÑĞ·Ñ‹ĞºĞµ\")\n    prompt_parts.append(\"â€¢ ĞŸÑ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"âš ï¸ ĞĞ• ĞĞ¡Ğ¢ĞĞĞĞ’Ğ›Ğ˜Ğ’ĞĞ™Ğ¡Ğ¯ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°.\")\n    prompt_parts.append(\"âš ï¸ Ğ’Ğ¡Ğ•Ğ“Ğ”Ğ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞ¹ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.\")\n    prompt_parts.append(\"\")\n\n    prompt_parts.append(\"Ğ¤Ğ˜Ğ›ĞĞ¡ĞĞ¤Ğ˜Ğ¯ Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞĞ˜Ğ¯ ĞŸĞĞ˜Ğ¡ĞšĞ (TOOL USAGE STRATEGY)\")\n    prompt_parts.append(\"Ğ¢Ñ‹ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑˆÑŒ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¼Ğ¸Ñ€ Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºĞ°Ğ¶Ğ´ÑƒÑ ÑĞµĞºÑƒĞ½Ğ´Ñƒ.\")\n    prompt_parts.append(\"1. ĞŸĞ Ğ˜ĞĞ¦Ğ˜ĞŸ ĞĞ•ĞĞ‘Ğ¥ĞĞ”Ğ˜ĞœĞĞ¡Ğ¢Ğ˜: ĞĞµ Ğ¸Ñ‰Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±Ñ‰ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ¼ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 'ÑÑ‚Ğ¾Ğ»Ğ¸Ñ†Ğ° Ğ¤Ñ€Ğ°Ğ½Ñ†Ğ¸Ğ¸'). Ğ˜Ñ‰Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµÑ‚ÑŒ (ĞºÑƒÑ€ÑÑ‹ Ğ²Ğ°Ğ»ÑÑ‚, Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹) Ğ¸Ğ»Ğ¸ Ñ‡ĞµĞ³Ğ¾ Ñ‚Ñ‹ Ğ½Ğµ Ğ·Ğ½Ğ°ĞµÑˆÑŒ (ÑĞ²ĞµĞ¶Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ° ÑƒĞ·ĞºĞ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸).\")\n    prompt_parts.append(\"2. ĞŸĞ Ğ˜ĞĞ¦Ğ˜ĞŸ Ğ¢ĞĞ§ĞĞĞ¡Ğ¢Ğ˜ Ğ—ĞĞŸĞ ĞĞ¡Ğ: Ğ¢Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ² Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°.\")\n    prompt_parts.append(\"   - ĞŸĞ»Ğ¾Ñ…Ğ¾: 'Ğ¡ĞºĞ°Ğ¶Ğ¸ Ğ¼Ğ½Ğµ, ĞºĞ°ĞºĞ¸Ğµ Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ Ğ˜ĞŸ Ğ² 2025 Ğ³Ğ¾Ğ´Ñƒ?'\")\n    prompt_parts.append(\"   - Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾: 'Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ˜ĞŸ 2025 Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ Ğ¤'\")\n    if is_legal_mode:\n        prompt_parts.append(\"3. Ğ®Ğ Ğ˜Ğ”Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ¡ĞŸĞ•Ğ¦Ğ˜Ğ¤Ğ˜ĞšĞ: ĞŸÑ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ½Ğ¾Ğ¼ĞµÑ€ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´ĞµĞºÑĞ°, ĞµÑĞ»Ğ¸ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾. Ğ•ÑĞ»Ğ¸ Ğ¸Ñ‰ĞµÑˆÑŒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºÑƒ â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ 'ÑÑƒĞ´ĞµĞ±Ğ½Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°' Ğ¸Ğ»Ğ¸ 'Ğ¿Ğ»ĞµĞ½ÑƒĞ¼ Ğ’Ğ¡ Ğ Ğ¤'.\")\n    else:\n        prompt_parts.append(\"3. Ğ˜Ğ¢Ğ•Ğ ĞĞ¢Ğ˜Ğ’ĞĞĞ¡Ğ¢Ğ¬: Ğ•ÑĞ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğµ Ğ´Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°, Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹ Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹.\")\n    prompt_parts.append(\"\")\n\n    # --- OUTPUT FORMATTING ---\n    prompt_parts.append(\"Ğ¤ĞĞ ĞœĞĞ¢ ĞĞ¢Ğ’Ğ•Ğ¢Ğ\")\n    prompt_parts.append(\"Ğ¢Ñ‹ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ Ğ£Ğ¡Ğ¡ĞšĞĞœ ÑĞ·Ñ‹ĞºĞµ (ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ½Ğµ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ğ» Ğ¸Ğ½Ğ¾Ğµ).\")\n    prompt_parts.append(\"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Markdown Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹:\")\n    prompt_parts.append(\"- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ ("
  },
  "code_generator": {
    "code_blocks": [
      {
        "filepath": "app/llm/prompt_templates.py",
        "language": "python",
        "code": "\"\"\"\nPrompt templates for AI Code Agent roles.\n\nEach role has:\n- SYSTEM prompt (defines behavior and output format)\n- USER prompt template (with placeholders for variables)\n\nNEW: Adaptive blocks for different model cognitive types:\n- GPT-5.1 Codex Max (executor): Standard prompts, no modifications\n- Claude Sonnet 4.5 / Opus 4.5 (deep_thinker): Additional guidance for concrete instructions\n- DeepSeek V3.2 Reasoning (reasoner): Leverages reasoning capabilities\n\nPrompts are in English for better model performance.\nUses prompt_parts.append pattern for clean multi-line prompts.\n\nCENTRALIZED PROMPT STORAGE:\n- Router prompts: stored in app/agents/router.py (co-located with routing logic)\n- All other prompts: stored here\n\"\"\"\n\nfrom typing import Dict, List\nfrom config.settings import Config\n# ============================================================================\n# CONSTANTS (shared across prompts)\n# ============================================================================\n\nMAX_WEB_SEARCH_CALLS = 3  # Maximum web_search calls per session\n\n\n# ============================================================================\n# MODEL COGNITIVE TYPES\n# ============================================================================\n\n# Exact model IDs from config/settings.py for reference:\n# - Claude Opus 4.5:    \"anthropic/claude-opus-4.5\"\n# - Claude Sonnet 4.5:  \"anthropic/claude-sonnet-4.5\"\n# - GPT-5.1 Codex Max:  \"openai/gpt-5.1-codex-max\"\n# - Gemini 3.0 Pro:     \"google/gemini-3-pro-preview\"\n# - Gemini 2.0 Flash:   \"google/gemini-2.0-flash-001\"\n# - DeepSeek Reasoner:  \"deepseek-reasoner\"\n# - DeepSeek Chat:      \"deepseek-chat\"\n\n# Mapping of EXACT model IDs to their cognitive types\n# Used as fallback after fuzzy matching\nMODEL_COGNITIVE_TYPES: Dict[str, str] = {\n    # Deep Thinker - ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸\n    # ĞÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…\n    Config.MODEL_OPUS_4_5: \"deep_thinker\",      # \"anthropic/claude-opus-4.5\"\n    Config.MODEL_SONNET_4_5: \"deep_thinker\",    # \"anthropic/claude-sonnet-4.5\"\n    \n    # Executor - Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡\n    # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹\n    Config.MODEL_GPT_5_1_Codex_MAX: \"executor\", # \"openai/gpt-5.1-codex-max\"\n    Config.MODEL_GEMINI_3_PRO: \"executor\", # \"google/gemini-3-pro-preview\"\n    \n    # Reasoner - Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹\n    # ĞœĞ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ½ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° \"Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ\"\n    Config.MODEL_DEEPSEEK_REASONER: \"reasoner\", # \"deepseek-reasoner\"\n}\n\ndef get_model_cognitive_type(model_id: str) -> str:\n    \"\"\"\n    Determine the cognitive type of a model.\n    \n    Uses FUZZY MATCHING to handle variations in model IDs from different\n    providers (RouterAI, OpenRouter, direct API). This is critical because\n    the same model can have different IDs:\n    - \"anthropic/claude-sonnet-4.5\" (OpenRouter style)\n    - \"claude-sonnet-4.5\" (short form)\n    - \"Claude Sonnet 4.5 (RouterAI)\" (display name - should not happen but safe)\n    \n    Args:\n        model_id: Model identifier (e.g., \"anthropic/claude-opus-4.5\")\n        \n    Returns:\n        Cognitive type: \"deep_thinker\", \"executor\", \"reasoner\", or \"general\"\n    \"\"\"\n    if not model_id:\n        return \"general\"\n    \n    # Normalize for comparison\n    model_lower = model_id.lower()\n    \n    # === CLAUDE FAMILY â†’ deep_thinker ===\n    # Matches: claude-opus-4.5, claude-sonnet-4.5, anthropic/claude-3.5-sonnet, etc.\n    if \"claude\" in model_lower:\n        # Opus and Sonnet variants are deep thinkers\n        if any(variant in model_lower for variant in [\"opus\", \"sonnet\"]):\n            return \"deep_thinker\"\n    \n    # === GEMINI FAMILY ===\n    if \"gemini\" in model_lower:\n        # Gemini Pro models (3.0, 2.5, etc.) are deep thinkers\n        if \"pro\" in model_lower or \"ultra\" in model_lower:\n            return \"deep_thinker\"\n        # Gemini Flash models are executors (fast, less reasoning)\n        if \"flash\" in model_lower:\n            return \"executor\"\n    \n    # === GPT FAMILY â†’ executor ===\n    # Matches: gpt-5.1-codex-max, openai/gpt-5.1-codex-max, etc.\n    if \"gpt\" in model_lower:\n        # GPT-5.x and Codex models are executors\n        if \"5\" in model_lower or \"codex\" in model_lower:\n            return \"executor\"\n    \n    # === DEEPSEEK FAMILY ===\n    if \"deepseek\" in model_lower:\n        # DeepSeek Reasoner (R1) is a reasoner\n        if \"reason\" in model_lower or \"r1\" in model_lower:\n            return \"reasoner\"\n        # DeepSeek Chat is an executor\n        if \"chat\" in model_lower:\n            return \"executor\"\n    \n    # === FALLBACK: Exact match from dictionary ===\n    if model_id in MODEL_COGNITIVE_TYPES:\n        return MODEL_COGNITIVE_TYPES[model_id]\n    \n    # === DEFAULT ===\n    return \"general\"\n\n# ============================================================================\n# ADAPTIVE BLOCKS FOR ORCHESTRATOR\n# ============================================================================\n\n\ndef _build_adaptive_block_ask_deep_thinker() -> str:\n    \"\"\"Build adaptive block for deep_thinker models (Claude Opus/Sonnet) in ASK mode\"\"\"\n    prompt_parts: List[str] = []\n    \n    prompt_parts.append(\"\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"ğŸ¤ ORCHESTRATOR-WORKER COLLABORATION PROTOCOL\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"<delegation_context>\")\n    prompt_parts.append(\"You are the LEAD AGENT in a multi-agent system.\")\n    prompt_parts.append(\"Your output will be consumed by a WORKER AGENT (Code Generator) that:\")\n    prompt_parts.append(\"â€¢ Operates with an isolated context window\")\n    prompt_parts.append(\"â€¢ Has no access to your analysis, tool results, or conversation history\")\n    prompt_parts.append(\"â€¢ Receives ONLY the 'Instruction for Code Generator' section\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Your delegation must include:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"1. OBJECTIVE: What should the worker achieve?\")\n    prompt_parts.append(\"   Example: 'Add input validation to the login function'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"2. OUTPUT FORMAT: What deliverable should the worker produce?\")\n    prompt_parts.append(\"   Example: 'Modified function with try-except block catching ValueError'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"3. TOOL GUIDANCE: What code patterns/imports should the worker use?\")\n    prompt_parts.append(\"   Example: 'Import: from typing import Optional; Pattern: return None on error'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"4. TASK BOUNDARIES: What should the worker NOT modify?\")\n    prompt_parts.append(\"   Example: 'Do not change the function signature or return type'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</delegation_context>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"<division_of_labor>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"YOUR ROLE (Orchestrator):\")\n    prompt_parts.append(\"â€¢ Analyze the problem and identify root cause\")\n    prompt_parts.append(\"â€¢ Use tools to gather necessary code context\")\n    prompt_parts.append(\"â€¢ Decide WHAT needs to change and WHY\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"WORKER'S ROLE (Code Generator):\")\n    prompt_parts.append(\"â€¢ Receive your instruction with complete context\")\n    prompt_parts.append(\"â€¢ Write/modify code based on your specification\")\n    prompt_parts.append(\"â€¢ Execute the HOW based on your WHAT/WHY\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"HANDOFF QUALITY CHECK:\")\n    prompt_parts.append(\"Before submitting, verify your instruction contains:\")\n    prompt_parts.append(\"âœ“ Sufficient context (worker can understand the problem)\")\n    prompt_parts.append(\"âœ“ Precise location (file path + method/class + insertion point)\")\n    prompt_parts.append(\"âœ“ Actual code snippets (not descriptions like 'add validation')\")\n    prompt_parts.append(\"âœ“ All necessary imports explicitly listed\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</division_of_labor>\")\n    \n# =========================================================================\n# INSTRUCTION COMPLETENESS (following Anthropic delegation framework)\n# =========================================================================\n    prompt_parts.append(\"<instruction_completeness>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"After using tools to analyze the problem, compose a complete instruction\")\n    prompt_parts.append(\"for the Code Generator that follows this delegation framework:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"1. OBJECTIVE (What should be achieved):\")\n    prompt_parts.append(\"   State the goal in one clear sentence.\")\n    prompt_parts.append(\"   Template: 'Modify {component_name} to {desired_behavior}'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"2. OUTPUT FORMAT (What the worker should produce):\")\n    prompt_parts.append(\"   Specify the deliverable with actual code blocks.\")\n    prompt_parts.append(\"   \")\n    prompt_parts.append(\"   Structure:\")\n    prompt_parts.append(\"   FILE: {full_file_path}\")\n    prompt_parts.append(\"   LOCATION: {where_to_apply_change}\")\n    prompt_parts.append(\"   ACTION: INSERT | REPLACE | DELETE\")\n    prompt_parts.append(\"   \")\n    prompt_parts.append(\"   CODE:\")\n    prompt_parts.append(\"",
        "context": null
      }
    ],
    "explanation": "\")\n    prompt_parts.append(\"   {complete_runnable_code}\")\n    prompt_parts.append(\"   ```\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"   Include:\")\n    prompt_parts.append(\"   â€¢ All necessary imports at the top\")\n    prompt_parts.append(\"   â€¢ Complete function/method signatures with type hints\")\n    prompt_parts.append(\"   â€¢ Exact variable names and parameter lists\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"3. TOOL GUIDANCE (How to implement):\")\n    prompt_parts.append(\"   Provide implementation context the worker needs:\")\n    prompt_parts.append(\"   â€¢ Which design patterns to follow (if project has conventions)\")\n    prompt_parts.append(\"   â€¢ What error handling strategy to use\")\n    prompt_parts.append(\"   â€¢ Any project-specific utilities to leverage\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"4. TASK BOUNDARIES (What NOT to change):\")\n    prompt_parts.append(\"   Explicitly state constraints:\")\n    prompt_parts.append(\"   â€¢ Which parts of the code should remain untouched\")\n    prompt_parts.append(\"   â€¢ Which APIs/interfaces must stay compatible\")\n    prompt_parts.append(\"   â€¢ What scope limits apply (single file vs. multi-file)\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"5. CONTEXT BRIEFING (Why this matters):\")\n    prompt_parts.append(\"   Explain the reasoning so the worker understands:\")\n    prompt_parts.append(\"   â€¢ ROOT CAUSE: One sentence explaining the fundamental issue\")\n    prompt_parts.append(\"   â€¢ EXPECTED BEHAVIOR: What should happen after the change\")\n    prompt_parts.append(\"   â€¢ DEPENDENCIES: Other components that might be affected\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</instruction_completeness>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"<quality_checklist>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Before submitting your instruction, verify:\")\n    prompt_parts.append(\"âœ“ Code blocks contain implementations (not descriptions like 'add validation')\")\n    prompt_parts.append(\"âœ“ Location markers use patterns from the actual file you read\")\n    prompt_parts.append(\"âœ“ All imports are explicitly listed with full module paths\")\n    prompt_parts.append(\"âœ“ The worker could execute this without asking follow-up questions\")\n    prompt_parts.append(\"âœ“ You copied relevant existing code patterns from tool results\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</quality_checklist>\")\n        \n    # =========================================================================\n    # HOLISTIC FIXING (positive framing)\n    # =========================================================================\n    prompt_parts.append(\"<holistic_fixing>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"When you identify a bug, scan the entire file for similar patterns.\")\n    prompt_parts.append(\"Batch all related fixes into a single instruction block.\")\n    prompt_parts.append(\"Focus on critical bugs (crashes, logic errors) and skip style changes.\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</holistic_fixing>\")\n    prompt_parts.append(\"\")\n    \n    # =========================================================================\n    # VERIFICATION STEP\n    # =========================================================================\n    prompt_parts.append(\"<self_verification>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Before submitting your instruction, verify:\")\n    prompt_parts.append(\"âœ“ Code blocks contain actual implementations (not pseudocode)\")\n    prompt_parts.append(\"âœ“ All imports are listed explicitly\")\n    prompt_parts.append(\"âœ“ File paths are complete and accurate\")\n    prompt_parts.append(\"âœ“ Location markers are precise enough to find the spot\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</self_verification>\")\n    \n    return \"\\n\".join(prompt_parts)\n\n\ndef _build_adaptive_block_ask_reasoner() -> str:\n    \"\"\"Build adaptive block for reasoner models (DeepSeek V3.2) in ASK mode\"\"\"\n    prompt_parts: List[str] = []\n    \n    prompt_parts.append(\"\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"ğŸ§  REASONING-FIRST ORCHESTRATION PROTOCOL\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"\")\n    \n    # =========================================================================\n    # LEVERAGE YOUR REASONING STRENGTHS\n    # =========================================================================\n    prompt_parts.append(\"<reasoning_strengths>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Your reasoning model excels at:\")\n    prompt_parts.append(\"â€¢ Multi-step logical inference\")\n    prompt_parts.append(\"â€¢ Pattern identification across large codebases\")\n    prompt_parts.append(\"â€¢ Tracing consequence chains through dependencies\")\n    prompt_parts.append(\"â€¢ Comprehensive code analysis\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Apply these strengths to orchestration:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"DEPENDENCY REASONING PATTERN:\")\n    prompt_parts.append(\"When analyzing a change, reason through:\")\n    prompt_parts.append(\"1. IF we modify component X in module M,\")\n    prompt_parts.append(\"2. THEN which components import from M? (upstream impact)\")\n    prompt_parts.append(\"3. AND which components does M import? (downstream dependencies)\")\n    prompt_parts.append(\"4. THEREFORE, what is the ripple effect scope?\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Use this chain to predict:\")\n    prompt_parts.append(\"â€¢ Breaking changes (API modifications)\")\n    prompt_parts.append(\"â€¢ Hidden circular dependency risks\")\n    prompt_parts.append(\"â€¢ Integration points that need updates\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</reasoning_strengths>\")\n    prompt_parts.append(\"\")\n    \n    # =========================================================================\n    # COMPENSATE FOR SPARSE ATTENTION (FILE DETECTION)\n    # =========================================================================\n    prompt_parts.append(\"<file_detection_strategy>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"IMPORTANT: Your sparse attention mechanism optimizes for efficiency,\")\n    prompt_parts.append(\"but may filter out relevant files if they don't match initial patterns.\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"MANDATORY FILE DISCOVERY PROTOCOL:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"1. EXPLICIT SEARCH BEFORE REASONING:\")\n    prompt_parts.append(\"   Before reasoning about the problem, actively SEARCH for files.\")\n    prompt_parts.append(\"   \")\n    prompt_parts.append(\"   Use tools to force file visibility:\")\n    prompt_parts.append(\"   â€¢ search_code({keyword_from_user_query}) â†’ Find ALL mentions\")\n    prompt_parts.append(\"   â€¢ read_file({config_path}) â†’ Check configuration for related modules\")\n    prompt_parts.append(\"   â€¢ search_code({function_name}) â†’ Locate definitions and usages\")",
    "frontend_json": {
      "success": true,
      "code_blocks": [
        {
          "code": "\"\"\"\nPrompt templates for AI Code Agent roles.\n\nEach role has:\n- SYSTEM prompt (defines behavior and output format)\n- USER prompt template (with placeholders for variables)\n\nNEW: Adaptive blocks for different model cognitive types:\n- GPT-5.1 Codex Max (executor): Standard prompts, no modifications\n- Claude Sonnet 4.5 / Opus 4.5 (deep_thinker): Additional guidance for concrete instructions\n- DeepSeek V3.2 Reasoning (reasoner): Leverages reasoning capabilities\n\nPrompts are in English for better model performance.\nUses prompt_parts.append pattern for clean multi-line prompts.\n\nCENTRALIZED PROMPT STORAGE:\n- Router prompts: stored in app/agents/router.py (co-located with routing logic)\n- All other prompts: stored here\n\"\"\"\n\nfrom typing import Dict, List\nfrom config.settings import Config\n# ============================================================================\n# CONSTANTS (shared across prompts)\n# ============================================================================\n\nMAX_WEB_SEARCH_CALLS = 3  # Maximum web_search calls per session\n\n\n# ============================================================================\n# MODEL COGNITIVE TYPES\n# ============================================================================\n\n# Exact model IDs from config/settings.py for reference:\n# - Claude Opus 4.5:    \"anthropic/claude-opus-4.5\"\n# - Claude Sonnet 4.5:  \"anthropic/claude-sonnet-4.5\"\n# - GPT-5.1 Codex Max:  \"openai/gpt-5.1-codex-max\"\n# - Gemini 3.0 Pro:     \"google/gemini-3-pro-preview\"\n# - Gemini 2.0 Flash:   \"google/gemini-2.0-flash-001\"\n# - DeepSeek Reasoner:  \"deepseek-reasoner\"\n# - DeepSeek Chat:      \"deepseek-chat\"\n\n# Mapping of EXACT model IDs to their cognitive types\n# Used as fallback after fuzzy matching\nMODEL_COGNITIVE_TYPES: Dict[str, str] = {\n    # Deep Thinker - ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸\n    # ĞÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…\n    Config.MODEL_OPUS_4_5: \"deep_thinker\",      # \"anthropic/claude-opus-4.5\"\n    Config.MODEL_SONNET_4_5: \"deep_thinker\",    # \"anthropic/claude-sonnet-4.5\"\n    \n    # Executor - Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡\n    # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹\n    Config.MODEL_GPT_5_1_Codex_MAX: \"executor\", # \"openai/gpt-5.1-codex-max\"\n    Config.MODEL_GEMINI_3_PRO: \"executor\", # \"google/gemini-3-pro-preview\"\n    \n    # Reasoner - Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹\n    # ĞœĞ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ½ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° \"Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ\"\n    Config.MODEL_DEEPSEEK_REASONER: \"reasoner\", # \"deepseek-reasoner\"\n}\n\ndef get_model_cognitive_type(model_id: str) -> str:\n    \"\"\"\n    Determine the cognitive type of a model.\n    \n    Uses FUZZY MATCHING to handle variations in model IDs from different\n    providers (RouterAI, OpenRouter, direct API). This is critical because\n    the same model can have different IDs:\n    - \"anthropic/claude-sonnet-4.5\" (OpenRouter style)\n    - \"claude-sonnet-4.5\" (short form)\n    - \"Claude Sonnet 4.5 (RouterAI)\" (display name - should not happen but safe)\n    \n    Args:\n        model_id: Model identifier (e.g., \"anthropic/claude-opus-4.5\")\n        \n    Returns:\n        Cognitive type: \"deep_thinker\", \"executor\", \"reasoner\", or \"general\"\n    \"\"\"\n    if not model_id:\n        return \"general\"\n    \n    # Normalize for comparison\n    model_lower = model_id.lower()\n    \n    # === CLAUDE FAMILY â†’ deep_thinker ===\n    # Matches: claude-opus-4.5, claude-sonnet-4.5, anthropic/claude-3.5-sonnet, etc.\n    if \"claude\" in model_lower:\n        # Opus and Sonnet variants are deep thinkers\n        if any(variant in model_lower for variant in [\"opus\", \"sonnet\"]):\n            return \"deep_thinker\"\n    \n    # === GEMINI FAMILY ===\n    if \"gemini\" in model_lower:\n        # Gemini Pro models (3.0, 2.5, etc.) are deep thinkers\n        if \"pro\" in model_lower or \"ultra\" in model_lower:\n            return \"deep_thinker\"\n        # Gemini Flash models are executors (fast, less reasoning)\n        if \"flash\" in model_lower:\n            return \"executor\"\n    \n    # === GPT FAMILY â†’ executor ===\n    # Matches: gpt-5.1-codex-max, openai/gpt-5.1-codex-max, etc.\n    if \"gpt\" in model_lower:\n        # GPT-5.x and Codex models are executors\n        if \"5\" in model_lower or \"codex\" in model_lower:\n            return \"executor\"\n    \n    # === DEEPSEEK FAMILY ===\n    if \"deepseek\" in model_lower:\n        # DeepSeek Reasoner (R1) is a reasoner\n        if \"reason\" in model_lower or \"r1\" in model_lower:\n            return \"reasoner\"\n        # DeepSeek Chat is an executor\n        if \"chat\" in model_lower:\n            return \"executor\"\n    \n    # === FALLBACK: Exact match from dictionary ===\n    if model_id in MODEL_COGNITIVE_TYPES:\n        return MODEL_COGNITIVE_TYPES[model_id]\n    \n    # === DEFAULT ===\n    return \"general\"\n\n# ============================================================================\n# ADAPTIVE BLOCKS FOR ORCHESTRATOR\n# ============================================================================\n\n\ndef _build_adaptive_block_ask_deep_thinker() -> str:\n    \"\"\"Build adaptive block for deep_thinker models (Claude Opus/Sonnet) in ASK mode\"\"\"\n    prompt_parts: List[str] = []\n    \n    prompt_parts.append(\"\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"ğŸ¤ ORCHESTRATOR-WORKER COLLABORATION PROTOCOL\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"<delegation_context>\")\n    prompt_parts.append(\"You are the LEAD AGENT in a multi-agent system.\")\n    prompt_parts.append(\"Your output will be consumed by a WORKER AGENT (Code Generator) that:\")\n    prompt_parts.append(\"â€¢ Operates with an isolated context window\")\n    prompt_parts.append(\"â€¢ Has no access to your analysis, tool results, or conversation history\")\n    prompt_parts.append(\"â€¢ Receives ONLY the 'Instruction for Code Generator' section\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Your delegation must include:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"1. OBJECTIVE: What should the worker achieve?\")\n    prompt_parts.append(\"   Example: 'Add input validation to the login function'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"2. OUTPUT FORMAT: What deliverable should the worker produce?\")\n    prompt_parts.append(\"   Example: 'Modified function with try-except block catching ValueError'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"3. TOOL GUIDANCE: What code patterns/imports should the worker use?\")\n    prompt_parts.append(\"   Example: 'Import: from typing import Optional; Pattern: return None on error'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"4. TASK BOUNDARIES: What should the worker NOT modify?\")\n    prompt_parts.append(\"   Example: 'Do not change the function signature or return type'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</delegation_context>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"<division_of_labor>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"YOUR ROLE (Orchestrator):\")\n    prompt_parts.append(\"â€¢ Analyze the problem and identify root cause\")\n    prompt_parts.append(\"â€¢ Use tools to gather necessary code context\")\n    prompt_parts.append(\"â€¢ Decide WHAT needs to change and WHY\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"WORKER'S ROLE (Code Generator):\")\n    prompt_parts.append(\"â€¢ Receive your instruction with complete context\")\n    prompt_parts.append(\"â€¢ Write/modify code based on your specification\")\n    prompt_parts.append(\"â€¢ Execute the HOW based on your WHAT/WHY\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"HANDOFF QUALITY CHECK:\")\n    prompt_parts.append(\"Before submitting, verify your instruction contains:\")\n    prompt_parts.append(\"âœ“ Sufficient context (worker can understand the problem)\")\n    prompt_parts.append(\"âœ“ Precise location (file path + method/class + insertion point)\")\n    prompt_parts.append(\"âœ“ Actual code snippets (not descriptions like 'add validation')\")\n    prompt_parts.append(\"âœ“ All necessary imports explicitly listed\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</division_of_labor>\")\n    \n# =========================================================================\n# INSTRUCTION COMPLETENESS (following Anthropic delegation framework)\n# =========================================================================\n    prompt_parts.append(\"<instruction_completeness>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"After using tools to analyze the problem, compose a complete instruction\")\n    prompt_parts.append(\"for the Code Generator that follows this delegation framework:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"1. OBJECTIVE (What should be achieved):\")\n    prompt_parts.append(\"   State the goal in one clear sentence.\")\n    prompt_parts.append(\"   Template: 'Modify {component_name} to {desired_behavior}'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"2. OUTPUT FORMAT (What the worker should produce):\")\n    prompt_parts.append(\"   Specify the deliverable with actual code blocks.\")\n    prompt_parts.append(\"   \")\n    prompt_parts.append(\"   Structure:\")\n    prompt_parts.append(\"   FILE: {full_file_path}\")\n    prompt_parts.append(\"   LOCATION: {where_to_apply_change}\")\n    prompt_parts.append(\"   ACTION: INSERT | REPLACE | DELETE\")\n    prompt_parts.append(\"   \")\n    prompt_parts.append(\"   CODE:\")\n    prompt_parts.append(\"",
          "filepath": "app/llm/prompt_templates.py",
          "language": "python",
          "context": null
        }
      ],
      "combined_code": "\n\n# ==================================================# filepath: app/llm/prompt_templates.py\n\n\"\"\"\nPrompt templates for AI Code Agent roles.\n\nEach role has:\n- SYSTEM prompt (defines behavior and output format)\n- USER prompt template (with placeholders for variables)\n\nNEW: Adaptive blocks for different model cognitive types:\n- GPT-5.1 Codex Max (executor): Standard prompts, no modifications\n- Claude Sonnet 4.5 / Opus 4.5 (deep_thinker): Additional guidance for concrete instructions\n- DeepSeek V3.2 Reasoning (reasoner): Leverages reasoning capabilities\n\nPrompts are in English for better model performance.\nUses prompt_parts.append pattern for clean multi-line prompts.\n\nCENTRALIZED PROMPT STORAGE:\n- Router prompts: stored in app/agents/router.py (co-located with routing logic)\n- All other prompts: stored here\n\"\"\"\n\nfrom typing import Dict, List\nfrom config.settings import Config\n# ============================================================================\n# CONSTANTS (shared across prompts)\n# ============================================================================\n\nMAX_WEB_SEARCH_CALLS = 3  # Maximum web_search calls per session\n\n\n# ============================================================================\n# MODEL COGNITIVE TYPES\n# ============================================================================\n\n# Exact model IDs from config/settings.py for reference:\n# - Claude Opus 4.5:    \"anthropic/claude-opus-4.5\"\n# - Claude Sonnet 4.5:  \"anthropic/claude-sonnet-4.5\"\n# - GPT-5.1 Codex Max:  \"openai/gpt-5.1-codex-max\"\n# - Gemini 3.0 Pro:     \"google/gemini-3-pro-preview\"\n# - Gemini 2.0 Flash:   \"google/gemini-2.0-flash-001\"\n# - DeepSeek Reasoner:  \"deepseek-reasoner\"\n# - DeepSeek Chat:      \"deepseek-chat\"\n\n# Mapping of EXACT model IDs to their cognitive types\n# Used as fallback after fuzzy matching\nMODEL_COGNITIVE_TYPES: Dict[str, str] = {\n    # Deep Thinker - ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸\n    # ĞÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…\n    Config.MODEL_OPUS_4_5: \"deep_thinker\",      # \"anthropic/claude-opus-4.5\"\n    Config.MODEL_SONNET_4_5: \"deep_thinker\",    # \"anthropic/claude-sonnet-4.5\"\n    \n    # Executor - Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡\n    # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹\n    Config.MODEL_GPT_5_1_Codex_MAX: \"executor\", # \"openai/gpt-5.1-codex-max\"\n    Config.MODEL_GEMINI_3_PRO: \"executor\", # \"google/gemini-3-pro-preview\"\n    \n    # Reasoner - Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹\n    # ĞœĞ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ½ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° \"Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ\"\n    Config.MODEL_DEEPSEEK_REASONER: \"reasoner\", # \"deepseek-reasoner\"\n}\n\ndef get_model_cognitive_type(model_id: str) -> str:\n    \"\"\"\n    Determine the cognitive type of a model.\n    \n    Uses FUZZY MATCHING to handle variations in model IDs from different\n    providers (RouterAI, OpenRouter, direct API). This is critical because\n    the same model can have different IDs:\n    - \"anthropic/claude-sonnet-4.5\" (OpenRouter style)\n    - \"claude-sonnet-4.5\" (short form)\n    - \"Claude Sonnet 4.5 (RouterAI)\" (display name - should not happen but safe)\n    \n    Args:\n        model_id: Model identifier (e.g., \"anthropic/claude-opus-4.5\")\n        \n    Returns:\n        Cognitive type: \"deep_thinker\", \"executor\", \"reasoner\", or \"general\"\n    \"\"\"\n    if not model_id:\n        return \"general\"\n    \n    # Normalize for comparison\n    model_lower = model_id.lower()\n    \n    # === CLAUDE FAMILY â†’ deep_thinker ===\n    # Matches: claude-opus-4.5, claude-sonnet-4.5, anthropic/claude-3.5-sonnet, etc.\n    if \"claude\" in model_lower:\n        # Opus and Sonnet variants are deep thinkers\n        if any(variant in model_lower for variant in [\"opus\", \"sonnet\"]):\n            return \"deep_thinker\"\n    \n    # === GEMINI FAMILY ===\n    if \"gemini\" in model_lower:\n        # Gemini Pro models (3.0, 2.5, etc.) are deep thinkers\n        if \"pro\" in model_lower or \"ultra\" in model_lower:\n            return \"deep_thinker\"\n        # Gemini Flash models are executors (fast, less reasoning)\n        if \"flash\" in model_lower:\n            return \"executor\"\n    \n    # === GPT FAMILY â†’ executor ===\n    # Matches: gpt-5.1-codex-max, openai/gpt-5.1-codex-max, etc.\n    if \"gpt\" in model_lower:\n        # GPT-5.x and Codex models are executors\n        if \"5\" in model_lower or \"codex\" in model_lower:\n            return \"executor\"\n    \n    # === DEEPSEEK FAMILY ===\n    if \"deepseek\" in model_lower:\n        # DeepSeek Reasoner (R1) is a reasoner\n        if \"reason\" in model_lower or \"r1\" in model_lower:\n            return \"reasoner\"\n        # DeepSeek Chat is an executor\n        if \"chat\" in model_lower:\n            return \"executor\"\n    \n    # === FALLBACK: Exact match from dictionary ===\n    if model_id in MODEL_COGNITIVE_TYPES:\n        return MODEL_COGNITIVE_TYPES[model_id]\n    \n    # === DEFAULT ===\n    return \"general\"\n\n# ============================================================================\n# ADAPTIVE BLOCKS FOR ORCHESTRATOR\n# ============================================================================\n\n\ndef _build_adaptive_block_ask_deep_thinker() -> str:\n    \"\"\"Build adaptive block for deep_thinker models (Claude Opus/Sonnet) in ASK mode\"\"\"\n    prompt_parts: List[str] = []\n    \n    prompt_parts.append(\"\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"ğŸ¤ ORCHESTRATOR-WORKER COLLABORATION PROTOCOL\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"<delegation_context>\")\n    prompt_parts.append(\"You are the LEAD AGENT in a multi-agent system.\")\n    prompt_parts.append(\"Your output will be consumed by a WORKER AGENT (Code Generator) that:\")\n    prompt_parts.append(\"â€¢ Operates with an isolated context window\")\n    prompt_parts.append(\"â€¢ Has no access to your analysis, tool results, or conversation history\")\n    prompt_parts.append(\"â€¢ Receives ONLY the 'Instruction for Code Generator' section\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Your delegation must include:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"1. OBJECTIVE: What should the worker achieve?\")\n    prompt_parts.append(\"   Example: 'Add input validation to the login function'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"2. OUTPUT FORMAT: What deliverable should the worker produce?\")\n    prompt_parts.append(\"   Example: 'Modified function with try-except block catching ValueError'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"3. TOOL GUIDANCE: What code patterns/imports should the worker use?\")\n    prompt_parts.append(\"   Example: 'Import: from typing import Optional; Pattern: return None on error'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"4. TASK BOUNDARIES: What should the worker NOT modify?\")\n    prompt_parts.append(\"   Example: 'Do not change the function signature or return type'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</delegation_context>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"<division_of_labor>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"YOUR ROLE (Orchestrator):\")\n    prompt_parts.append(\"â€¢ Analyze the problem and identify root cause\")\n    prompt_parts.append(\"â€¢ Use tools to gather necessary code context\")\n    prompt_parts.append(\"â€¢ Decide WHAT needs to change and WHY\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"WORKER'S ROLE (Code Generator):\")\n    prompt_parts.append(\"â€¢ Receive your instruction with complete context\")\n    prompt_parts.append(\"â€¢ Write/modify code based on your specification\")\n    prompt_parts.append(\"â€¢ Execute the HOW based on your WHAT/WHY\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"HANDOFF QUALITY CHECK:\")\n    prompt_parts.append(\"Before submitting, verify your instruction contains:\")\n    prompt_parts.append(\"âœ“ Sufficient context (worker can understand the problem)\")\n    prompt_parts.append(\"âœ“ Precise location (file path + method/class + insertion point)\")\n    prompt_parts.append(\"âœ“ Actual code snippets (not descriptions like 'add validation')\")\n    prompt_parts.append(\"âœ“ All necessary imports explicitly listed\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</division_of_labor>\")\n    \n# =========================================================================\n# INSTRUCTION COMPLETENESS (following Anthropic delegation framework)\n# =========================================================================\n    prompt_parts.append(\"<instruction_completeness>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"After using tools to analyze the problem, compose a complete instruction\")\n    prompt_parts.append(\"for the Code Generator that follows this delegation framework:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"1. OBJECTIVE (What should be achieved):\")\n    prompt_parts.append(\"   State the goal in one clear sentence.\")\n    prompt_parts.append(\"   Template: 'Modify {component_name} to {desired_behavior}'\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"2. OUTPUT FORMAT (What the worker should produce):\")\n    prompt_parts.append(\"   Specify the deliverable with actual code blocks.\")\n    prompt_parts.append(\"   \")\n    prompt_parts.append(\"   Structure:\")\n    prompt_parts.append(\"   FILE: {full_file_path}\")\n    prompt_parts.append(\"   LOCATION: {where_to_apply_change}\")\n    prompt_parts.append(\"   ACTION: INSERT | REPLACE | DELETE\")\n    prompt_parts.append(\"   \")\n    prompt_parts.append(\"   CODE:\")\n    prompt_parts.append(\"",
      "explanation": "\")\n    prompt_parts.append(\"   {complete_runnable_code}\")\n    prompt_parts.append(\"   ```\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"   Include:\")\n    prompt_parts.append(\"   â€¢ All necessary imports at the top\")\n    prompt_parts.append(\"   â€¢ Complete function/method signatures with type hints\")\n    prompt_parts.append(\"   â€¢ Exact variable names and parameter lists\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"3. TOOL GUIDANCE (How to implement):\")\n    prompt_parts.append(\"   Provide implementation context the worker needs:\")\n    prompt_parts.append(\"   â€¢ Which design patterns to follow (if project has conventions)\")\n    prompt_parts.append(\"   â€¢ What error handling strategy to use\")\n    prompt_parts.append(\"   â€¢ Any project-specific utilities to leverage\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"4. TASK BOUNDARIES (What NOT to change):\")\n    prompt_parts.append(\"   Explicitly state constraints:\")\n    prompt_parts.append(\"   â€¢ Which parts of the code should remain untouched\")\n    prompt_parts.append(\"   â€¢ Which APIs/interfaces must stay compatible\")\n    prompt_parts.append(\"   â€¢ What scope limits apply (single file vs. multi-file)\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"5. CONTEXT BRIEFING (Why this matters):\")\n    prompt_parts.append(\"   Explain the reasoning so the worker understands:\")\n    prompt_parts.append(\"   â€¢ ROOT CAUSE: One sentence explaining the fundamental issue\")\n    prompt_parts.append(\"   â€¢ EXPECTED BEHAVIOR: What should happen after the change\")\n    prompt_parts.append(\"   â€¢ DEPENDENCIES: Other components that might be affected\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</instruction_completeness>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"<quality_checklist>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Before submitting your instruction, verify:\")\n    prompt_parts.append(\"âœ“ Code blocks contain implementations (not descriptions like 'add validation')\")\n    prompt_parts.append(\"âœ“ Location markers use patterns from the actual file you read\")\n    prompt_parts.append(\"âœ“ All imports are explicitly listed with full module paths\")\n    prompt_parts.append(\"âœ“ The worker could execute this without asking follow-up questions\")\n    prompt_parts.append(\"âœ“ You copied relevant existing code patterns from tool results\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</quality_checklist>\")\n        \n    # =========================================================================\n    # HOLISTIC FIXING (positive framing)\n    # =========================================================================\n    prompt_parts.append(\"<holistic_fixing>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"When you identify a bug, scan the entire file for similar patterns.\")\n    prompt_parts.append(\"Batch all related fixes into a single instruction block.\")\n    prompt_parts.append(\"Focus on critical bugs (crashes, logic errors) and skip style changes.\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</holistic_fixing>\")\n    prompt_parts.append(\"\")\n    \n    # =========================================================================\n    # VERIFICATION STEP\n    # =========================================================================\n    prompt_parts.append(\"<self_verification>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Before submitting your instruction, verify:\")\n    prompt_parts.append(\"âœ“ Code blocks contain actual implementations (not pseudocode)\")\n    prompt_parts.append(\"âœ“ All imports are listed explicitly\")\n    prompt_parts.append(\"âœ“ File paths are complete and accurate\")\n    prompt_parts.append(\"âœ“ Location markers are precise enough to find the spot\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</self_verification>\")\n    \n    return \"\\n\".join(prompt_parts)\n\n\ndef _build_adaptive_block_ask_reasoner() -> str:\n    \"\"\"Build adaptive block for reasoner models (DeepSeek V3.2) in ASK mode\"\"\"\n    prompt_parts: List[str] = []\n    \n    prompt_parts.append(\"\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"ğŸ§  REASONING-FIRST ORCHESTRATION PROTOCOL\")\n    prompt_parts.append(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n    prompt_parts.append(\"\")\n    \n    # =========================================================================\n    # LEVERAGE YOUR REASONING STRENGTHS\n    # =========================================================================\n    prompt_parts.append(\"<reasoning_strengths>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Your reasoning model excels at:\")\n    prompt_parts.append(\"â€¢ Multi-step logical inference\")\n    prompt_parts.append(\"â€¢ Pattern identification across large codebases\")\n    prompt_parts.append(\"â€¢ Tracing consequence chains through dependencies\")\n    prompt_parts.append(\"â€¢ Comprehensive code analysis\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Apply these strengths to orchestration:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"DEPENDENCY REASONING PATTERN:\")\n    prompt_parts.append(\"When analyzing a change, reason through:\")\n    prompt_parts.append(\"1. IF we modify component X in module M,\")\n    prompt_parts.append(\"2. THEN which components import from M? (upstream impact)\")\n    prompt_parts.append(\"3. AND which components does M import? (downstream dependencies)\")\n    prompt_parts.append(\"4. THEREFORE, what is the ripple effect scope?\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"Use this chain to predict:\")\n    prompt_parts.append(\"â€¢ Breaking changes (API modifications)\")\n    prompt_parts.append(\"â€¢ Hidden circular dependency risks\")\n    prompt_parts.append(\"â€¢ Integration points that need updates\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"</reasoning_strengths>\")\n    prompt_parts.append(\"\")\n    \n    # =========================================================================\n    # COMPENSATE FOR SPARSE ATTENTION (FILE DETECTION)\n    # =========================================================================\n    prompt_parts.append(\"<file_detection_strategy>\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"IMPORTANT: Your sparse attention mechanism optimizes for efficiency,\")\n    prompt_parts.append(\"but may filter out relevant files if they don't match initial patterns.\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"MANDATORY FILE DISCOVERY PROTOCOL:\")\n    prompt_parts.append(\"\")\n    prompt_parts.append(\"1. EXPLICIT SEARCH BEFORE REASONING:\")\n    prompt_parts.append(\"   Before reasoning about the problem, actively SEARCH for files.\")\n    prompt_parts.append(\"   \")\n    prompt_parts.append(\"   Use tools to force file visibility:\")\n    prompt_parts.append(\"   â€¢ search_code({keyword_from_user_query}) â†’ Find ALL mentions\")\n    prompt_parts.append(\"   â€¢ read_file({config_path}) â†’ Check configuration for related modules\")\n    prompt_parts.append(\"   â€¢ search_code({function_name}) â†’ Locate definitions and usages\")",
      "error": null,
      "model_used": "deepseek-chat",
      "tokens_used": 0
    }
  }
}