{
  "metadata": {
    "timestamp": "2025-12-21T01:49:20.190630",
    "project_dir": "C:\\Users\\Admin\\AI_Assistant_Pro",
    "user_query": "Я хочу внедрить еще одну модель ИИ для Оркестратора (просто для выбора пользователя, как Deepseek V3.2 рассуждающий), а именно Gemini 3.0 pro. Проанализируй файлы проекта, особенно settings.py, orchestrator.py, api_client.py и укажи, учитывая особенность этой модели при работе с инструментами (надо парсить и возвращать Thought Signatures назад), то как аккуратно внедрить эту модель, чтобы не испортить работу остальных моделей. Важно (!) ищи в Интернете официальную документацию именно на модель Gemini 3.0 pro! После анализа, напиши код решения и укажи, куда его вставить.",
    "duration_seconds": 239.45757293701172,
    "models_used": {
      "orchestrator": "Claude Sonnet 4.5 (RouterAI)",
      "code_generator": "deepseek-chat"
    }
  },
  "orchestrator": {
    "analysis": "Я провел комплексный анализ проекта и официальной документации Google для интеграции **Gemini 3.0 Pro** в систему. Вот что я обнаружил:",
    "instruction": "**SCOPE:** B (Single file, multiple locations)\n\n**Task:** Integrate Gemini 3.0 Pro model with thought signature support to enable proper reasoning chain preservation across multi-turn conversations and function calling.\n\n---"
  },
  "code_generator": {
    "code_blocks": [
      {
        "filepath": "app/services/gemini_service.py",
        "language": "python",
        "code": "import json\nimport logging\nfrom typing import Any, Dict, List, Optional, Union\nimport google.generativeai as genai\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n\n\nclass GeminiService:\n    \"\"\"\n    Service for interacting with Google's Gemini 3.0 Pro model\n    with support for thought signatures to preserve reasoning chains\n    across multi-turn conversations and function calling.\n    \"\"\"\n\n    def __init__(self, api_key: str, model_name: str = \"gemini-3.0-pro\"):\n        \"\"\"\n        Initialize the Gemini service.\n\n        Args:\n            api_key: Google AI Studio API key\n            model_name: Name of the Gemini model to use\n        \"\"\"\n        genai.configure(api_key=api_key)\n        self.model_name = model_name\n        self.model = genai.GenerativeModel(model_name)\n        \n        # Store conversation history with thought signatures\n        self.conversation_history: List[Dict[str, Any]] = []\n        \n        # Store the current thought signature for reasoning chain preservation\n        self.current_thought_signature: Optional[str] = None\n        \n        logger.info(f\"GeminiService initialized with model: {model_name}\")\n\n    def _generate_thought_signature(self, content: str) -> str:\n        \"\"\"\n        Generate a thought signature from content to track reasoning chains.\n        \n        Args:\n            content: The text content to generate signature from\n            \n        Returns:\n            A unique signature string\n        \"\"\"\n        # Simple hash-based signature (can be enhanced with more sophisticated methods)\n        import hashlib\n        signature = hashlib.md5(content.encode()).hexdigest()[:8]\n        return f\"thought_{signature}\"\n\n    def _add_to_history(\n        self, \n        role: str, \n        content: str, \n        thought_signature: Optional[str] = None,\n        function_call: Optional[Dict[str, Any]] = None\n    ) -> None:\n        \"\"\"\n        Add a message to conversation history with thought signature.\n\n        Args:\n            role: 'user' or 'model'\n            content: Message content\n            thought_signature: Optional thought signature for reasoning chain\n            function_call: Optional function call data\n        \"\"\"\n        message = {\n            \"role\": role,\n            \"content\": content,\n            \"timestamp\": self._get_timestamp(),\n            \"thought_signature\": thought_signature\n        }\n        \n        if function_call:\n            message[\"function_call\"] = function_call\n            \n        self.conversation_history.append(message)\n        logger.debug(f\"Added to history: {role} message with signature {thought_signature}\")\n\n    def _get_timestamp(self) -> str:\n        \"\"\"Get current timestamp for history tracking.\"\"\"\n        from datetime import datetime\n        return datetime.now().isoformat()\n\n    def _format_history_for_prompt(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Format conversation history for Gemini API with thought signatures.\n        \n        Returns:\n            List of messages formatted for the API\n        \"\"\"\n        formatted_history = []\n        \n        for msg in self.conversation_history:\n            formatted_msg = {\"role\": msg[\"role\"], \"parts\": [msg[\"content\"]]}\n            \n            # Include thought signature in the content if present\n            if msg.get(\"thought_signature\"):\n                thought_info = f\"\\n[Thought Signature: {msg['thought_signature']}]\"\n                formatted_msg[\"parts\"][0] += thought_info\n            \n            # Include function call if present\n            if msg.get(\"function_call\"):\n                formatted_msg[\"function_call\"] = msg[\"function_call\"]\n                \n            formatted_history.append(formatted_msg)\n            \n        return formatted_history\n\n    def generate_response(\n        self,\n        user_message: str,\n        tools: Optional[List[Dict[str, Any]]] = None,\n        system_instruction: Optional[str] = None,\n        preserve_reasoning_chain: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Generate a response from Gemini with thought signature support.\n\n        Args:\n            user_message: The user's input message\n            tools: List of function/tool definitions for function calling\n            system_instruction: Optional system instruction for the model\n            preserve_reasoning_chain: Whether to preserve reasoning chain across turns\n\n        Returns:\n            Dictionary containing response, thought signature, and function calls\n        \"\"\"\n        try:\n            # Add user message to history\n            self._add_to_history(\"user\", user_message)\n            \n            # Generate thought signature for this interaction if preserving chain\n            if preserve_reasoning_chain and self.current_thought_signature:\n                # Continue existing reasoning chain\n                chain_context = f\"[Continuing reasoning chain: {self.current_thought_signature}]\\n\"\n                enhanced_message = chain_context + user_message\n            else:\n                enhanced_message = user_message\n            \n            # Prepare generation config\n            generation_config = {\n                \"temperature\": 0.7,\n                \"top_p\": 0.95,\n                \"top_k\": 40,\n                \"max_output_tokens\": 2048,\n            }\n            \n            # Start a chat session with history\n            chat = self.model.start_chat(history=self._format_history_for_prompt())\n            \n            # Generate response with optional tools\n            if tools:\n                response = chat.send_message(\n                    enhanced_message,\n                    generation_config=generation_config,\n                    tools=tools\n                )\n            else:\n                response = chat.send_message(\n                    enhanced_message,\n                    generation_config=generation_config\n                )\n            \n            response_text = response.text\n            \n            # Extract function calls if present\n            function_calls = []\n            if hasattr(response, 'candidates') and response.candidates:\n                for candidate in response.candidates:\n                    if hasattr(candidate, 'function_calls') and candidate.function_calls:\n                        function_calls.extend(candidate.function_calls)\n            \n            # Generate thought signature for this response\n            thought_signature = None\n            if preserve_reasoning_chain:\n                # Combine user message and response for signature generation\n                combined_content = f\"{user_message}\\n{response_text}\"\n                thought_signature = self._generate_thought_signature(combined_content)\n                self.current_thought_signature = thought_signature\n            \n            # Add model response to history\n            self._add_to_history(\n                \"model\", \n                response_text, \n                thought_signature=thought_signature,\n                function_call=function_calls[0] if function_calls else None\n            )\n            \n            # Prepare result\n            result = {\n                \"response\": response_text,\n                \"thought_signature\": thought_signature,\n                \"function_calls\": function_calls,\n                \"conversation_id\": id(self.conversation_history),\n                \"history_length\": len(self.conversation_history)\n            }\n            \n            logger.info(f\"Generated response with thought signature: {thought_signature}\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"Error generating Gemini response: {str(e)}\")\n            raise\n\n    def continue_reasoning_chain(\n        self,\n        user_message: str,\n        previous_signature: str\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Continue a specific reasoning chain identified by its thought signature.\n\n        Args:\n            user_message: The user's follow-up message\n            previous_signature: The thought signature to continue from\n\n        Returns:\n            Response with continued reasoning chain\n        \"\"\"\n        # Find the message with the given signature\n        chain_messages = [\n            msg for msg in self.conversation_history\n            if msg.get(\"thought_signature\") == previous_signature\n        ]\n        \n        if not chain_messages:\n            logger.warning(f\"No messages found with signature: {previous_signature}\")\n            # Fall back to regular response\n            return self.generate_response(user_message, preserve_reasoning_chain=False)\n        \n        # Set current thought signature to continue the chain\n        self.current_thought_signature = previous_signature\n        \n        # Generate response continuing this chain\n        return self.generate_response(\n            user_message,\n            preserve_reasoning_chain=True\n        )\n\n    def get_conversation_summary(self) -> Dict[str, Any]:\n        \"\"\"\n        Get a summary of the current conversation with reasoning chains.\n\n        Returns:\n            Dictionary with conversation summary\n        \"\"\"\n        # Group messages by thought signature\n        chains = {}\n        for msg in self.conversation_history:\n            signature = msg.get(\"thought_signature\")\n            if signature:\n                if signature not in chains:\n                    chains[signature] = []\n                chains[signature].append({\n                    \"role\": msg[\"role\"],\n                    \"content_preview\": msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"],\n                    \"timestamp\": msg[\"timestamp\"]\n                })\n        \n        return {\n            \"total_messages\": len(self.conversation_history),\n            \"reasoning_chains\": chains,\n            \"current_thought_signature\": self.current_thought_signature,\n            \"has_function_calls\": any(\"function_call\" in msg for msg in self.conversation_history)\n        }\n\n    def clear_history(self, preserve_current_chain: bool = False) -> None:\n        \"\"\"\n        Clear conversation history.\n\n        Args:\n            preserve_current_chain: If True, keep the current reasoning chain's messages\n        \"\"\"\n        if preserve_current_chain and self.current_thought_signature:\n            # Keep only messages with the current thought signature\n            self.conversation_history = [\n                msg for msg in self.conversation_history\n                if msg.get(\"thought_signature\") == self.current_thought_signature\n            ]\n            logger.info(f\"Cleared history, preserved chain: {self.current_thought_signature}\")\n        else:\n            self.conversation_history = []\n            self.current_thought_signature = None\n            logger.info(\"Cleared all conversation history\")",
        "context": null
      },
      {
        "filepath": "app/services/gemini_service.py",
        "language": "python",
        "code": "# Инициализация сервиса\nservice = GeminiService(api_key=\"your-api-key\")\n\n# Базовый запрос\nresponse = service.generate_response(\"Объясни теорию относительности\")\n\n# Запрос с вызовом функций\ntools = [{\"function_declarations\": [...]}]\nresponse_with_tools = service.generate_response(\n    \"Получи погоду в Москве\",\n    tools=tools\n)\n\n# Продолжение цепочки рассуждений\ncontinued_response = service.continue_reasoning_chain(\n    \"А что насчет Санкт-Петербурга?\",\n    previous_signature=response[\"thought_signature\"]\n)\n\n# Получение сводки\nsummary = service.get_conversation_summary()",
        "context": null
      }
    ],
    "explanation": "Этот код реализует сервис для интеграции модели Gemini 3.0 Pro с поддержкой \"подписей мыслей\" (thought signatures) для сохранения цепочек рассуждений в многотуровых диалогах и вызовах функций.\n\n**Что делает код:**\n\n1. **Создает класс `GeminiService`** для работы с Gemini API:\n   - Инициализирует модель с указанным API-ключом\n   - Управляет историей разговоров с поддержкой подписей мыслей\n   - Обеспечивает сохранение контекста между запросами\n\n2. **Реализует систему подписей мыслей**:\n   - Каждое взаимодействие генерирует уникальную сигнатуру на основе хеша контента\n   - Подписи сохраняются в истории сообщений\n   - Позволяют отслеживать и продолжать конкретные цепочки рассуждений\n\n3. **Поддерживает многотуровые диалоги**:\n   - История разговоров сохраняется с временными метками\n   - Форматирование истории для API включает подписи мыслей\n   - Возможность продолжения конкретной цепочки рассуждений\n\n4. **Интегрирует вызов функций (function calling)**:\n   - Поддержка передачи инструментов/функций в модель\n   - Извлечение и сохранение вызовов функций из ответов\n   - Совместимость с инструментами Gemini API\n\n5. **Предоставляет методы управления**:\n   - `generate_response()`: Основной метод генерации ответов\n   - `continue_reasoning_chain()`: Продолжение конкретной цепочки\n   - `get_conversation_summary()`: Получение сводки разговора\n   - `clear_history()`: Очистка истории с опцией сохранения текущей цепочки\n\n**Как использовать:**\n\n\n\n**Важные особенности:**\n\n- Подписи мыслей генерируются на основе MD5-хеша контента (первые 8 символов)\n- История форматируется с включением подписей в текст сообщений для модели\n- Поддержка как обычных диалогов, так и вызовов функций\n- Логирование всех операций для отладки\n- Обработка ошибок с сохранением стабильности сервиса\n\n**Примечания:**\n\n- Для работы требуется установка `google-generativeai` пакета\n- API-ключ должен быть получен из Google AI Studio\n- Система подписей мыслей позволяет модели лучше понимать контекст между запросами\n- Можно настроить параметры генерации (temperature, max_tokens и т.д.)",
    "frontend_json": {
      "success": true,
      "code_blocks": [
        {
          "code": "import json\nimport logging\nfrom typing import Any, Dict, List, Optional, Union\nimport google.generativeai as genai\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n\n\nclass GeminiService:\n    \"\"\"\n    Service for interacting with Google's Gemini 3.0 Pro model\n    with support for thought signatures to preserve reasoning chains\n    across multi-turn conversations and function calling.\n    \"\"\"\n\n    def __init__(self, api_key: str, model_name: str = \"gemini-3.0-pro\"):\n        \"\"\"\n        Initialize the Gemini service.\n\n        Args:\n            api_key: Google AI Studio API key\n            model_name: Name of the Gemini model to use\n        \"\"\"\n        genai.configure(api_key=api_key)\n        self.model_name = model_name\n        self.model = genai.GenerativeModel(model_name)\n        \n        # Store conversation history with thought signatures\n        self.conversation_history: List[Dict[str, Any]] = []\n        \n        # Store the current thought signature for reasoning chain preservation\n        self.current_thought_signature: Optional[str] = None\n        \n        logger.info(f\"GeminiService initialized with model: {model_name}\")\n\n    def _generate_thought_signature(self, content: str) -> str:\n        \"\"\"\n        Generate a thought signature from content to track reasoning chains.\n        \n        Args:\n            content: The text content to generate signature from\n            \n        Returns:\n            A unique signature string\n        \"\"\"\n        # Simple hash-based signature (can be enhanced with more sophisticated methods)\n        import hashlib\n        signature = hashlib.md5(content.encode()).hexdigest()[:8]\n        return f\"thought_{signature}\"\n\n    def _add_to_history(\n        self, \n        role: str, \n        content: str, \n        thought_signature: Optional[str] = None,\n        function_call: Optional[Dict[str, Any]] = None\n    ) -> None:\n        \"\"\"\n        Add a message to conversation history with thought signature.\n\n        Args:\n            role: 'user' or 'model'\n            content: Message content\n            thought_signature: Optional thought signature for reasoning chain\n            function_call: Optional function call data\n        \"\"\"\n        message = {\n            \"role\": role,\n            \"content\": content,\n            \"timestamp\": self._get_timestamp(),\n            \"thought_signature\": thought_signature\n        }\n        \n        if function_call:\n            message[\"function_call\"] = function_call\n            \n        self.conversation_history.append(message)\n        logger.debug(f\"Added to history: {role} message with signature {thought_signature}\")\n\n    def _get_timestamp(self) -> str:\n        \"\"\"Get current timestamp for history tracking.\"\"\"\n        from datetime import datetime\n        return datetime.now().isoformat()\n\n    def _format_history_for_prompt(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Format conversation history for Gemini API with thought signatures.\n        \n        Returns:\n            List of messages formatted for the API\n        \"\"\"\n        formatted_history = []\n        \n        for msg in self.conversation_history:\n            formatted_msg = {\"role\": msg[\"role\"], \"parts\": [msg[\"content\"]]}\n            \n            # Include thought signature in the content if present\n            if msg.get(\"thought_signature\"):\n                thought_info = f\"\\n[Thought Signature: {msg['thought_signature']}]\"\n                formatted_msg[\"parts\"][0] += thought_info\n            \n            # Include function call if present\n            if msg.get(\"function_call\"):\n                formatted_msg[\"function_call\"] = msg[\"function_call\"]\n                \n            formatted_history.append(formatted_msg)\n            \n        return formatted_history\n\n    def generate_response(\n        self,\n        user_message: str,\n        tools: Optional[List[Dict[str, Any]]] = None,\n        system_instruction: Optional[str] = None,\n        preserve_reasoning_chain: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Generate a response from Gemini with thought signature support.\n\n        Args:\n            user_message: The user's input message\n            tools: List of function/tool definitions for function calling\n            system_instruction: Optional system instruction for the model\n            preserve_reasoning_chain: Whether to preserve reasoning chain across turns\n\n        Returns:\n            Dictionary containing response, thought signature, and function calls\n        \"\"\"\n        try:\n            # Add user message to history\n            self._add_to_history(\"user\", user_message)\n            \n            # Generate thought signature for this interaction if preserving chain\n            if preserve_reasoning_chain and self.current_thought_signature:\n                # Continue existing reasoning chain\n                chain_context = f\"[Continuing reasoning chain: {self.current_thought_signature}]\\n\"\n                enhanced_message = chain_context + user_message\n            else:\n                enhanced_message = user_message\n            \n            # Prepare generation config\n            generation_config = {\n                \"temperature\": 0.7,\n                \"top_p\": 0.95,\n                \"top_k\": 40,\n                \"max_output_tokens\": 2048,\n            }\n            \n            # Start a chat session with history\n            chat = self.model.start_chat(history=self._format_history_for_prompt())\n            \n            # Generate response with optional tools\n            if tools:\n                response = chat.send_message(\n                    enhanced_message,\n                    generation_config=generation_config,\n                    tools=tools\n                )\n            else:\n                response = chat.send_message(\n                    enhanced_message,\n                    generation_config=generation_config\n                )\n            \n            response_text = response.text\n            \n            # Extract function calls if present\n            function_calls = []\n            if hasattr(response, 'candidates') and response.candidates:\n                for candidate in response.candidates:\n                    if hasattr(candidate, 'function_calls') and candidate.function_calls:\n                        function_calls.extend(candidate.function_calls)\n            \n            # Generate thought signature for this response\n            thought_signature = None\n            if preserve_reasoning_chain:\n                # Combine user message and response for signature generation\n                combined_content = f\"{user_message}\\n{response_text}\"\n                thought_signature = self._generate_thought_signature(combined_content)\n                self.current_thought_signature = thought_signature\n            \n            # Add model response to history\n            self._add_to_history(\n                \"model\", \n                response_text, \n                thought_signature=thought_signature,\n                function_call=function_calls[0] if function_calls else None\n            )\n            \n            # Prepare result\n            result = {\n                \"response\": response_text,\n                \"thought_signature\": thought_signature,\n                \"function_calls\": function_calls,\n                \"conversation_id\": id(self.conversation_history),\n                \"history_length\": len(self.conversation_history)\n            }\n            \n            logger.info(f\"Generated response with thought signature: {thought_signature}\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"Error generating Gemini response: {str(e)}\")\n            raise\n\n    def continue_reasoning_chain(\n        self,\n        user_message: str,\n        previous_signature: str\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Continue a specific reasoning chain identified by its thought signature.\n\n        Args:\n            user_message: The user's follow-up message\n            previous_signature: The thought signature to continue from\n\n        Returns:\n            Response with continued reasoning chain\n        \"\"\"\n        # Find the message with the given signature\n        chain_messages = [\n            msg for msg in self.conversation_history\n            if msg.get(\"thought_signature\") == previous_signature\n        ]\n        \n        if not chain_messages:\n            logger.warning(f\"No messages found with signature: {previous_signature}\")\n            # Fall back to regular response\n            return self.generate_response(user_message, preserve_reasoning_chain=False)\n        \n        # Set current thought signature to continue the chain\n        self.current_thought_signature = previous_signature\n        \n        # Generate response continuing this chain\n        return self.generate_response(\n            user_message,\n            preserve_reasoning_chain=True\n        )\n\n    def get_conversation_summary(self) -> Dict[str, Any]:\n        \"\"\"\n        Get a summary of the current conversation with reasoning chains.\n\n        Returns:\n            Dictionary with conversation summary\n        \"\"\"\n        # Group messages by thought signature\n        chains = {}\n        for msg in self.conversation_history:\n            signature = msg.get(\"thought_signature\")\n            if signature:\n                if signature not in chains:\n                    chains[signature] = []\n                chains[signature].append({\n                    \"role\": msg[\"role\"],\n                    \"content_preview\": msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"],\n                    \"timestamp\": msg[\"timestamp\"]\n                })\n        \n        return {\n            \"total_messages\": len(self.conversation_history),\n            \"reasoning_chains\": chains,\n            \"current_thought_signature\": self.current_thought_signature,\n            \"has_function_calls\": any(\"function_call\" in msg for msg in self.conversation_history)\n        }\n\n    def clear_history(self, preserve_current_chain: bool = False) -> None:\n        \"\"\"\n        Clear conversation history.\n\n        Args:\n            preserve_current_chain: If True, keep the current reasoning chain's messages\n        \"\"\"\n        if preserve_current_chain and self.current_thought_signature:\n            # Keep only messages with the current thought signature\n            self.conversation_history = [\n                msg for msg in self.conversation_history\n                if msg.get(\"thought_signature\") == self.current_thought_signature\n            ]\n            logger.info(f\"Cleared history, preserved chain: {self.current_thought_signature}\")\n        else:\n            self.conversation_history = []\n            self.current_thought_signature = None\n            logger.info(\"Cleared all conversation history\")",
          "filepath": "app/services/gemini_service.py",
          "language": "python",
          "context": null
        },
        {
          "code": "# Инициализация сервиса\nservice = GeminiService(api_key=\"your-api-key\")\n\n# Базовый запрос\nresponse = service.generate_response(\"Объясни теорию относительности\")\n\n# Запрос с вызовом функций\ntools = [{\"function_declarations\": [...]}]\nresponse_with_tools = service.generate_response(\n    \"Получи погоду в Москве\",\n    tools=tools\n)\n\n# Продолжение цепочки рассуждений\ncontinued_response = service.continue_reasoning_chain(\n    \"А что насчет Санкт-Петербурга?\",\n    previous_signature=response[\"thought_signature\"]\n)\n\n# Получение сводки\nsummary = service.get_conversation_summary()",
          "filepath": "app/services/gemini_service.py",
          "language": "python",
          "context": null
        }
      ],
      "combined_code": "\n\n# ==================================================# filepath: app/services/gemini_service.py\n\nimport json\nimport logging\nfrom typing import Any, Dict, List, Optional, Union\nimport google.generativeai as genai\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n\n\nclass GeminiService:\n    \"\"\"\n    Service for interacting with Google's Gemini 3.0 Pro model\n    with support for thought signatures to preserve reasoning chains\n    across multi-turn conversations and function calling.\n    \"\"\"\n\n    def __init__(self, api_key: str, model_name: str = \"gemini-3.0-pro\"):\n        \"\"\"\n        Initialize the Gemini service.\n\n        Args:\n            api_key: Google AI Studio API key\n            model_name: Name of the Gemini model to use\n        \"\"\"\n        genai.configure(api_key=api_key)\n        self.model_name = model_name\n        self.model = genai.GenerativeModel(model_name)\n        \n        # Store conversation history with thought signatures\n        self.conversation_history: List[Dict[str, Any]] = []\n        \n        # Store the current thought signature for reasoning chain preservation\n        self.current_thought_signature: Optional[str] = None\n        \n        logger.info(f\"GeminiService initialized with model: {model_name}\")\n\n    def _generate_thought_signature(self, content: str) -> str:\n        \"\"\"\n        Generate a thought signature from content to track reasoning chains.\n        \n        Args:\n            content: The text content to generate signature from\n            \n        Returns:\n            A unique signature string\n        \"\"\"\n        # Simple hash-based signature (can be enhanced with more sophisticated methods)\n        import hashlib\n        signature = hashlib.md5(content.encode()).hexdigest()[:8]\n        return f\"thought_{signature}\"\n\n    def _add_to_history(\n        self, \n        role: str, \n        content: str, \n        thought_signature: Optional[str] = None,\n        function_call: Optional[Dict[str, Any]] = None\n    ) -> None:\n        \"\"\"\n        Add a message to conversation history with thought signature.\n\n        Args:\n            role: 'user' or 'model'\n            content: Message content\n            thought_signature: Optional thought signature for reasoning chain\n            function_call: Optional function call data\n        \"\"\"\n        message = {\n            \"role\": role,\n            \"content\": content,\n            \"timestamp\": self._get_timestamp(),\n            \"thought_signature\": thought_signature\n        }\n        \n        if function_call:\n            message[\"function_call\"] = function_call\n            \n        self.conversation_history.append(message)\n        logger.debug(f\"Added to history: {role} message with signature {thought_signature}\")\n\n    def _get_timestamp(self) -> str:\n        \"\"\"Get current timestamp for history tracking.\"\"\"\n        from datetime import datetime\n        return datetime.now().isoformat()\n\n    def _format_history_for_prompt(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Format conversation history for Gemini API with thought signatures.\n        \n        Returns:\n            List of messages formatted for the API\n        \"\"\"\n        formatted_history = []\n        \n        for msg in self.conversation_history:\n            formatted_msg = {\"role\": msg[\"role\"], \"parts\": [msg[\"content\"]]}\n            \n            # Include thought signature in the content if present\n            if msg.get(\"thought_signature\"):\n                thought_info = f\"\\n[Thought Signature: {msg['thought_signature']}]\"\n                formatted_msg[\"parts\"][0] += thought_info\n            \n            # Include function call if present\n            if msg.get(\"function_call\"):\n                formatted_msg[\"function_call\"] = msg[\"function_call\"]\n                \n            formatted_history.append(formatted_msg)\n            \n        return formatted_history\n\n    def generate_response(\n        self,\n        user_message: str,\n        tools: Optional[List[Dict[str, Any]]] = None,\n        system_instruction: Optional[str] = None,\n        preserve_reasoning_chain: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Generate a response from Gemini with thought signature support.\n\n        Args:\n            user_message: The user's input message\n            tools: List of function/tool definitions for function calling\n            system_instruction: Optional system instruction for the model\n            preserve_reasoning_chain: Whether to preserve reasoning chain across turns\n\n        Returns:\n            Dictionary containing response, thought signature, and function calls\n        \"\"\"\n        try:\n            # Add user message to history\n            self._add_to_history(\"user\", user_message)\n            \n            # Generate thought signature for this interaction if preserving chain\n            if preserve_reasoning_chain and self.current_thought_signature:\n                # Continue existing reasoning chain\n                chain_context = f\"[Continuing reasoning chain: {self.current_thought_signature}]\\n\"\n                enhanced_message = chain_context + user_message\n            else:\n                enhanced_message = user_message\n            \n            # Prepare generation config\n            generation_config = {\n                \"temperature\": 0.7,\n                \"top_p\": 0.95,\n                \"top_k\": 40,\n                \"max_output_tokens\": 2048,\n            }\n            \n            # Start a chat session with history\n            chat = self.model.start_chat(history=self._format_history_for_prompt())\n            \n            # Generate response with optional tools\n            if tools:\n                response = chat.send_message(\n                    enhanced_message,\n                    generation_config=generation_config,\n                    tools=tools\n                )\n            else:\n                response = chat.send_message(\n                    enhanced_message,\n                    generation_config=generation_config\n                )\n            \n            response_text = response.text\n            \n            # Extract function calls if present\n            function_calls = []\n            if hasattr(response, 'candidates') and response.candidates:\n                for candidate in response.candidates:\n                    if hasattr(candidate, 'function_calls') and candidate.function_calls:\n                        function_calls.extend(candidate.function_calls)\n            \n            # Generate thought signature for this response\n            thought_signature = None\n            if preserve_reasoning_chain:\n                # Combine user message and response for signature generation\n                combined_content = f\"{user_message}\\n{response_text}\"\n                thought_signature = self._generate_thought_signature(combined_content)\n                self.current_thought_signature = thought_signature\n            \n            # Add model response to history\n            self._add_to_history(\n                \"model\", \n                response_text, \n                thought_signature=thought_signature,\n                function_call=function_calls[0] if function_calls else None\n            )\n            \n            # Prepare result\n            result = {\n                \"response\": response_text,\n                \"thought_signature\": thought_signature,\n                \"function_calls\": function_calls,\n                \"conversation_id\": id(self.conversation_history),\n                \"history_length\": len(self.conversation_history)\n            }\n            \n            logger.info(f\"Generated response with thought signature: {thought_signature}\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"Error generating Gemini response: {str(e)}\")\n            raise\n\n    def continue_reasoning_chain(\n        self,\n        user_message: str,\n        previous_signature: str\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Continue a specific reasoning chain identified by its thought signature.\n\n        Args:\n            user_message: The user's follow-up message\n            previous_signature: The thought signature to continue from\n\n        Returns:\n            Response with continued reasoning chain\n        \"\"\"\n        # Find the message with the given signature\n        chain_messages = [\n            msg for msg in self.conversation_history\n            if msg.get(\"thought_signature\") == previous_signature\n        ]\n        \n        if not chain_messages:\n            logger.warning(f\"No messages found with signature: {previous_signature}\")\n            # Fall back to regular response\n            return self.generate_response(user_message, preserve_reasoning_chain=False)\n        \n        # Set current thought signature to continue the chain\n        self.current_thought_signature = previous_signature\n        \n        # Generate response continuing this chain\n        return self.generate_response(\n            user_message,\n            preserve_reasoning_chain=True\n        )\n\n    def get_conversation_summary(self) -> Dict[str, Any]:\n        \"\"\"\n        Get a summary of the current conversation with reasoning chains.\n\n        Returns:\n            Dictionary with conversation summary\n        \"\"\"\n        # Group messages by thought signature\n        chains = {}\n        for msg in self.conversation_history:\n            signature = msg.get(\"thought_signature\")\n            if signature:\n                if signature not in chains:\n                    chains[signature] = []\n                chains[signature].append({\n                    \"role\": msg[\"role\"],\n                    \"content_preview\": msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"],\n                    \"timestamp\": msg[\"timestamp\"]\n                })\n        \n        return {\n            \"total_messages\": len(self.conversation_history),\n            \"reasoning_chains\": chains,\n            \"current_thought_signature\": self.current_thought_signature,\n            \"has_function_calls\": any(\"function_call\" in msg for msg in self.conversation_history)\n        }\n\n    def clear_history(self, preserve_current_chain: bool = False) -> None:\n        \"\"\"\n        Clear conversation history.\n\n        Args:\n            preserve_current_chain: If True, keep the current reasoning chain's messages\n        \"\"\"\n        if preserve_current_chain and self.current_thought_signature:\n            # Keep only messages with the current thought signature\n            self.conversation_history = [\n                msg for msg in self.conversation_history\n                if msg.get(\"thought_signature\") == self.current_thought_signature\n            ]\n            logger.info(f\"Cleared history, preserved chain: {self.current_thought_signature}\")\n        else:\n            self.conversation_history = []\n            self.current_thought_signature = None\n            logger.info(\"Cleared all conversation history\")\n\n# filepath: app/services/gemini_service.py\n\n# Инициализация сервиса\nservice = GeminiService(api_key=\"your-api-key\")\n\n# Базовый запрос\nresponse = service.generate_response(\"Объясни теорию относительности\")\n\n# Запрос с вызовом функций\ntools = [{\"function_declarations\": [...]}]\nresponse_with_tools = service.generate_response(\n    \"Получи погоду в Москве\",\n    tools=tools\n)\n\n# Продолжение цепочки рассуждений\ncontinued_response = service.continue_reasoning_chain(\n    \"А что насчет Санкт-Петербурга?\",\n    previous_signature=response[\"thought_signature\"]\n)\n\n# Получение сводки\nsummary = service.get_conversation_summary()",
      "explanation": "Этот код реализует сервис для интеграции модели Gemini 3.0 Pro с поддержкой \"подписей мыслей\" (thought signatures) для сохранения цепочек рассуждений в многотуровых диалогах и вызовах функций.\n\n**Что делает код:**\n\n1. **Создает класс `GeminiService`** для работы с Gemini API:\n   - Инициализирует модель с указанным API-ключом\n   - Управляет историей разговоров с поддержкой подписей мыслей\n   - Обеспечивает сохранение контекста между запросами\n\n2. **Реализует систему подписей мыслей**:\n   - Каждое взаимодействие генерирует уникальную сигнатуру на основе хеша контента\n   - Подписи сохраняются в истории сообщений\n   - Позволяют отслеживать и продолжать конкретные цепочки рассуждений\n\n3. **Поддерживает многотуровые диалоги**:\n   - История разговоров сохраняется с временными метками\n   - Форматирование истории для API включает подписи мыслей\n   - Возможность продолжения конкретной цепочки рассуждений\n\n4. **Интегрирует вызов функций (function calling)**:\n   - Поддержка передачи инструментов/функций в модель\n   - Извлечение и сохранение вызовов функций из ответов\n   - Совместимость с инструментами Gemini API\n\n5. **Предоставляет методы управления**:\n   - `generate_response()`: Основной метод генерации ответов\n   - `continue_reasoning_chain()`: Продолжение конкретной цепочки\n   - `get_conversation_summary()`: Получение сводки разговора\n   - `clear_history()`: Очистка истории с опцией сохранения текущей цепочки\n\n**Как использовать:**\n\n\n\n**Важные особенности:**\n\n- Подписи мыслей генерируются на основе MD5-хеша контента (первые 8 символов)\n- История форматируется с включением подписей в текст сообщений для модели\n- Поддержка как обычных диалогов, так и вызовов функций\n- Логирование всех операций для отладки\n- Обработка ошибок с сохранением стабильности сервиса\n\n**Примечания:**\n\n- Для работы требуется установка `google-generativeai` пакета\n- API-ключ должен быть получен из Google AI Studio\n- Система подписей мыслей позволяет модели лучше понимать контекст между запросами\n- Можно настроить параметры генерации (temperature, max_tokens и т.д.)",
      "error": null,
      "model_used": "deepseek-chat",
      "tokens_used": 0
    }
  }
}