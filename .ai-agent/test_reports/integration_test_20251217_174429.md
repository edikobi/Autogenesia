# ü§ñ AI Code Agent - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –¢–µ—Å—Ç

**–î–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** 17.12.2025 17:44:29
**–ü—Ä–æ–µ–∫—Ç:** `C:\Users\Admin\AI_Assistant_Pro`
**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** 198.29 —Å–µ–∫.

---

## üìù –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

> –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ–∞–π–ª general_web_search.py –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏, –≤ —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –µ—Å—Ç—å, –Ω–∞–ø–∏—à–∏ –∫–æ–¥ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–π, –≥–¥–µ –ø—Ä–æ–±–ª–µ–º–∞

---

## üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

- **Orchestrator:** Claude Opus 4.5 (RouterAI)
- **Code Generator:** deepseek-chat

---

## üîç –ê–Ω–∞–ª–∏–∑ –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–≤ —Ñ–∞–π–ª `app/tools/general_web_search.py`, —è –æ–±–Ω–∞—Ä—É–∂–∏–ª **–∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å—é** –≤ —Ñ—É–Ω–∫—Ü–∏–∏ `_run_async()` (—Å—Ç—Ä–æ–∫–∏ 18-32).

---

## üìã –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è Code Generator

**Task:** –ò—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É —Å –±–ª–æ–∫–∏—Ä—É—é—â–µ–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å—é –≤ `general_web_search.py` ‚Äî —Å–¥–µ–ª–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∏ –¥–æ–±–∞–≤–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—ë—Ä—Ç–∫—É –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.

**File:** `app/tools/general_web_search.py`

**Location:** –§—É–Ω–∫—Ü–∏–∏ `_run_async()` (—Å—Ç—Ä–æ–∫–∏ 18-32) –∏ `general_web_search_tool()` (—Å—Ç—Ä–æ–∫–∏ 58-86)

**Changes:**

1. **–£–¥–∞–ª–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `_run_async()`** (—Å—Ç—Ä–æ–∫–∏ 18-32) ‚Äî –æ–Ω–∞ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–∞.

2. **–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å `general_web_search_tool()` –≤ `general_web_search_tool_sync()`** –∏ –æ—Å—Ç–∞–≤–∏—Ç—å –µ—ë –∫–∞–∫ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—ë—Ä—Ç–∫—É –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏. –ò–∑–º–µ–Ω–∏—Ç—å –µ—ë —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é:
   ```python
   def general_web_search_tool_sync(query: str, max_results: int = 10, time_limit: str = "w", region: str = "ru-ru") -> str:
       """
       –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è general_web_search_tool.
       –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ async –≤–µ—Ä—Å–∏—é, –µ—Å–ª–∏ –≤—ã–∑—ã–≤–∞–µ—Ç–µ –∏–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
       """
       return asyncio.run(general_web_search_tool(query, max_results, time_limit, region))
   ```

3. **–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é `general_web_search_tool()`** (–æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è):
   ```python
   async def general_web_search_tool(query: str, max_results: int = 10, time_limit: str = "w", region: str = "ru-ru") -> str:
       """
       –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –¥–ª—è –æ–±—â–∏—Ö, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
       
       Args:
           query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
           max_results: –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–¥–æ 10).
           time_limit: –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏ ('d' - –¥–µ–Ω—å, 'w' - –Ω–µ–¥–µ–ª—è, 'm' - –º–µ—Å—è—Ü, 'y' - –≥–æ–¥, None - –≤—Å–µ –≤—Ä–µ–º—è).
           region: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'ru-ru' –¥–ª—è –†–§).
       """
       if not query:
           return format_error("Query is required")

       max_results = min(max_results, 10)
       
       try:
           result = await async_general_web_search(query, max_results, time_limit, region)
               
           if not result.success:
               return format_error(result.error or "Search failed")
               
           if not result.pages:
               return format_no_results(query)
               
           return format_results_xml(result)
           
       except Exception as e:
           logger.error(f"General web search error: {e}")
           return format_error(f"Search failed: {e}")
   ```

4. **–ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `truncate_content()`** (—Å—Ç—Ä–æ–∫–∞ 262-268) ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π `counter` –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á—ë—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤:
   ```python
   def truncate_content(content: str, max_tokens: int, counter: TokenCounter) -> str:
       """–û–±—Ä–µ–∑–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤."""
       # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ‚Äî –µ—Å–ª–∏ —É–∂–µ –≤–ª–µ–∑–∞–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
       if counter.count(content) <= max_tokens:
           return content
       
       # –ë–∏–Ω–∞—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Ç–æ—á–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏
       low, high = 0, len(content)
       result = content
       
       while low < high:
           mid = (low + high + 1) // 2
           truncated = content[:mid]
           if counter.count(truncated) <= max_tokens:
               result = truncated
               low = mid
           else:
               high = mid - 1
       
       return result
   ```

5. **–£–¥–∞–ª–∏—Ç—å –∏–º–ø–æ—Ä—Ç `concurrent.futures`** (—Å—Ç—Ä–æ–∫–∞ 6), —Ç–∞–∫ –∫–∞–∫ –æ–Ω –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.

**–ü–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Ñ–∞–π–ª–µ –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π:**
- –ò–º–ø–æ—Ä—Ç—ã (–±–µ–∑ `concurrent.futures`)
- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã (`MAX_TOTAL_TOKENS`, `REQUEST_TIMEOUT`, `MAX_CONTENT_PER_PAGE`)
- Dataclasses (`GeneralWebPage`, `GeneralWebSearchResult`)
- `async def general_web_search_tool()` ‚Äî –æ—Å–Ω–æ–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
- `def general_web_search_tool_sync()` ‚Äî —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞
- `async def async_general_web_search()` ‚Äî –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ª–æ–≥–∏–∫–∞
- –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

**Why:** –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è `_run_async()` –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –∏–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ –Ω–∞—Ä—É—à–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å Python –∏ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ deadlock'–∞–º. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ ‚Äî —Å–¥–µ–ª–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—ë—Ä—Ç–∫—É –¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.

---

## üíª –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥

**–§–∞–π–ª:** `app/tools/general_web_search.py`

```python
from __future__ import annotations
import asyncio
import logging
import re
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from urllib.parse import urlparse, unquote
import httpx
from app.utils.token_counter import TokenCounter
from config.settings import cfg

logger = logging.getLogger(__name__)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–º–∏—Ç –∏–∑ settings –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
MAX_TOTAL_TOKENS = 25000  
REQUEST_TIMEOUT = 15.0
MAX_CONTENT_PER_PAGE = 4000 # –ß—É—Ç—å –º–µ–Ω—å—à–µ –¥–ª—è —Å—Ç–∞—Ç–µ–π, —á—Ç–æ–±—ã –≤–º–µ—Å—Ç–∏—Ç—å –±–æ–ª—å—à–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

@dataclass
class GeneralWebPage:
    url: str
    title: str
    snippet: str
    content: str = ""
    tokens: int = 0
    relevance_score: float = 0.0
    error: Optional[str] = None
    published_date: Optional[str] = None # –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π

@dataclass
class GeneralWebSearchResult:
    success: bool
    query: str
    pages: List[GeneralWebPage] = field(default_factory=list)
    total_tokens: int = 0
    error: Optional[str] = None

async def general_web_search_tool(query: str, max_results: int = 10, time_limit: str = "w", region: str = "ru-ru") -> str:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –¥–ª—è –æ–±—â–∏—Ö, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        max_results: –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–¥–æ 10).
        time_limit: –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏ ('d' - –¥–µ–Ω—å, 'w' - –Ω–µ–¥–µ–ª—è, 'm' - –º–µ—Å—è—Ü, 'y' - –≥–æ–¥, None - –≤—Å–µ –≤—Ä–µ–º—è).
        region: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'ru-ru' –¥–ª—è –†–§).
    """
    if not query:
        return format_error("Query is required")

    max_results = min(max_results, 10)
    
    try:
        result = await async_general_web_search(query, max_results, time_limit, region)
            
        if not result.success:
            return format_error(result.error or "Search failed")
            
        if not result.pages:
            return format_no_results(query)
            
        return format_results_xml(result)
        
    except Exception as e:
        logger.error(f"General web search error: {e}")
        return format_error(f"Search failed: {e}")

def general_web_search_tool_sync(query: str, max_results: int = 10, time_limit: str = "w", region: str = "ru-ru") -> str:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è general_web_search_tool.
    –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ async –≤–µ—Ä—Å–∏—é, –µ—Å–ª–∏ –≤—ã–∑—ã–≤–∞–µ—Ç–µ –∏–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    return asyncio.run(general_web_search_tool(query, max_results, time_limit, region))

async def async_general_web_search(query: str, max_results: int, time_limit: str, region: str) -> GeneralWebSearchResult:
    # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ DDG —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ–≥–∏–æ–Ω–∞
    search_results = await duckduckgo_search(query, max_results * 2, time_limit, region)
    
    if not search_results:
        return GeneralWebSearchResult(success=False, query=query, error="No search results found")

    # 2. –°–∫–∞—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    pages = await fetch_pages_parallel(search_results, max_results)

    # 3. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (Pseudo-Semantic Jaccard)
    pages = calculate_relevance_scores(pages, query)
    
    # 4. –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
    pages.sort(key=lambda p: p.relevance_score, reverse=True)

    # 5. –û—Ç–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ, –ø–æ–∫–∞ –≤–ª–µ–∑–∞–µ–º –≤ –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
    selected_pages = select_within_token_limit(pages, MAX_TOTAL_TOKENS)
    
    total_tokens = sum(p.tokens for p in selected_pages)
    
    return GeneralWebSearchResult(
        success=True,
        query=query,
        pages=selected_pages,
        total_tokens=total_tokens
    )

async def duckduckgo_search(query: str, num_results: int, time_limit: str, region: str) -> List[Dict[str, str]]:
    """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ HTML –≤–µ—Ä—Å–∏—é DDG —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
    search_url = "https://html.duckduckgo.com/html/"
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã DDG
    # df: w (week), m (month), d (day), y (year)
    params = {
        'q': query,
        'kl': region, # region settings (ru-ru)
    }
    if time_limit and time_limit in ['d', 'w', 'm', 'y']:
        params['df'] = time_limit

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7" # –í–∞–∂–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    }

    try:
        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
            response = await client.post(search_url, data=params, headers=headers)
            
            if response.status_code != 200:
                logger.warning(f"DDG returned status {response.status_code}")
                return []

            return parse_ddg_html(response.text, num_results)
    except Exception as e:
        logger.error(f"DDG search error: {e}")
        return []

def parse_ddg_html(html: str, max_results: int) -> List[Dict[str, str]]:
    results = []
    # –ß—É—Ç—å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π regex –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    result_pattern = re.compile(r'<a[^>]*class="[^"]*result__a[^"]*"[^>]*href="([^"]+)"[^>]*>(.*?)</a>', re.IGNORECASE)
    snippet_pattern = re.compile(r'<a[^>]*class="[^"]*result__snippet[^"]*"[^>]*>(.*?)</a>', re.IGNORECASE)
    
    matches = result_pattern.findall(html)
    snippets = snippet_pattern.findall(html)
    
    for i, (url, title) in enumerate(matches):
        if i >= max_results:
            break
            
        actual_url = extract_actual_url(url)
        if not is_valid_url(actual_url):
            continue
            
        snippet = snippets[i] if i < len(snippets) else ""
        
        # –û—á–∏—Å—Ç–∫–∞ HTML —Ç–µ–≥–æ–≤ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ —Å–Ω–∏–ø–ø–µ—Ç–∞
        title = remove_html_tags(title)
        snippet = remove_html_tags(snippet)
        
        results.append({
            "url": actual_url,
            "title": title.strip(),
            "snippet": snippet.strip()
        })
        
    return results

def remove_html_tags(text: str) -> str:
    return re.sub(r'<[^>]+>', '', text)

def extract_actual_url(ddg_url: str) -> str:
    if "uddg=" in ddg_url:
        match = re.search(r'uddg=([^&]+)', ddg_url)
        if match:
            return unquote(match.group(1))
    return ddg_url

def is_valid_url(url: str) -> bool:
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return False
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º PDF –∏ –±–∏–Ω–∞—Ä–Ω–∏–∫–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —è–≤–Ω–æ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if any(url.lower().endswith(ext) for ext in ['.pdf', '.doc', '.docx', '.xls', '.zip']):
            return False 
        return True
    except:
        return False

async def fetch_pages_parallel(search_results: List[Dict[str, str]], max_results: int) -> List[GeneralWebPage]:
    tasks = [fetch_single_page(r) for r in search_results]
    pages = await asyncio.gather(*tasks)
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –∏ –æ—à–∏–±–æ—á–Ω—ã–µ
    valid_pages = [p for p in pages if p.content and not p.error]
    return valid_pages[:max_results]

async def fetch_single_page(result: Dict[str, str]) -> GeneralWebPage:
    url = result['url']
    try:
        content = await fetch_page_content(url)
        if not content:
            return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error="Empty content")
            
        counter = TokenCounter()
        tokens = counter.count(content)
        
        # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ, –æ–±—Ä–µ–∑–∞–µ–º, –Ω–æ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ
        if tokens > MAX_CONTENT_PER_PAGE:
            content = truncate_content(content, MAX_CONTENT_PER_PAGE, counter)
            tokens = MAX_CONTENT_PER_PAGE
            
        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], content=content, tokens=tokens)
    except Exception as e:
        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error=str(e))

async def fetch_page_content(url: str) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8"
    }
    try:
        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            if response.status_code != 200:
                return ""
            return extract_text_from_html(response.text)
    except:
        return ""

def extract_text_from_html(html: str) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ç–∞—Ç–µ–π"""
    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏, –º–µ—Ç—Ä–∏–∫–∏
    html = re.sub(r'<(script|style|svg|nav|footer|header|aside)[^>]*>.*?</\1>', ' ', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)
    
    # –ó–∞–º–µ–Ω—è–µ–º –±–ª–æ—á–Ω—ã–µ —Ç–µ–≥–∏ –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    html = re.sub(r'</?(p|div|br|h[1-6]|li|tr)[^>]*>', '\n', html, flags=re.IGNORECASE)
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏
    text = re.sub(r'<[^>]+>', '', html)
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML —Å—É—â–Ω–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç–æ –±–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞)
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&quot;', '"').replace('&lt;', '<').replace('&gt;', '>')
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    
    return text.strip()

def truncate_content(content: str, max_tokens: int, counter: TokenCounter) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤."""
    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ‚Äî –µ—Å–ª–∏ —É–∂–µ –≤–ª–µ–∑–∞–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if counter.count(content) <= max_tokens:
        return content
    
    # –ë–∏–Ω–∞—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Ç–æ—á–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏
    low, high = 0, len(content)
    result = content
    
    while low < high:
        mid = (low + high + 1) // 2
        truncated = content[:mid]
        if counter.count(truncated) <= max_tokens:
            result = truncated
            low = mid
        else:
            high = mid - 1
    
    return result

def calculate_relevance_scores(pages: List[GeneralWebPage], query: str) -> List[GeneralWebPage]:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Jaccard Similarity (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤ —Å–ª–æ–≤).
    –≠—Ç–æ –ª—É—á—à–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ–π count, —Ç–∞–∫ –∫–∞–∫ —É—á–∏—Ç—ã–≤–∞–µ—Ç '–ø–æ–∫—Ä—ã—Ç–∏–µ' –∑–∞–ø—Ä–æ—Å–∞ —Ç–µ–∫—Å—Ç–æ–º.
    """
    query_words = set(re.findall(r'\w+', query.lower()))
    if not query_words:
        return pages

    for page in pages:
        score = 0.0
        
        # 1. Title Score (–û—á–µ–Ω—å –≤–∞–∂–Ω–æ)
        title_words = set(re.findall(r'\w+', page.title.lower()))
        if title_words:
            intersection = query_words.intersection(title_words)
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            score += (len(intersection) / len(query_words)) * 3.0 # –í–µ—Å 3.0

        # 2. Snippet Score
        snippet_words = set(re.findall(r'\w+', page.snippet.lower()))
        if snippet_words:
            intersection = query_words.intersection(snippet_words)
            score += (len(intersection) / len(query_words)) * 1.5 # –í–µ—Å 1.5

        # 3. Content Score (–ø–ª–æ—Ç–Ω–æ—Å—Ç—å –≤ –Ω–∞—á–∞–ª–µ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∂–Ω–µ–µ)
        content_preview = page.content[:1000].lower() # –°–º–æ—Ç—Ä–∏–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
        content_words = set(re.findall(r'\w+', content_preview))
        if content_words:
            intersection = query_words.intersection(content_words)
            score += (len(intersection) / len(query_words)) * 1.0

        page.relevance_score = score

    return pages

def select_within_token_limit(pages: List[GeneralWebPage], limit: int) -> List[GeneralWebPage]:
    selected = []
    current_tokens = 0
    for page in pages:
        if current_tokens + page.tokens <= limit:
            selected.append(page)
            current_tokens += page.tokens
        else:
            # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –≤–ª–µ–∑–∞–µ—Ç —Ü–µ–ª–∏–∫–æ–º, –Ω–æ –º–µ—Å—Ç–æ –µ—â–µ –µ—Å—Ç—å (>500 —Ç–æ–∫–µ–Ω–æ–≤), –±–µ—Ä–µ–º –∫—É—Å–æ–∫
            remaining = limit - current_tokens
            if remaining > 500:
                # –¢—É—Ç –Ω—É–∂–µ–Ω truncate, –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ
                # –í —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ –ª—É—á—à–µ –≤—ã–∑–≤–∞—Ç—å truncate_content
                page.content = page.content[:remaining * 4] 
                page.tokens = remaining
                selected.append(page)
            break
    return selected

def format_results_xml(result: GeneralWebSearchResult) -> str:
    parts = []
    parts.append(f"<!-- Web search results for '{result.query}' -->")
    parts.append(f"<websearch query='{result.query}' tokens='{result.total_tokens}'>")
    for i, page in enumerate(result.pages, 1):
        parts.append(f"  <page rank='{i}' relevance='{page.relevance_score:.2f}' url='{page.url}'>")
        parts.append(f"    <title>{page.title}</title>")
        parts.append(f"    <content><![CDATA[{page.content}]]></content>")
        parts.append(f"  </page>")
    parts.append("</websearch>")
    return "\n".join(parts)

def format_error(msg: str) -> str:
    return f"<!-- ERROR -->\n<error message='{msg}'/>"

def format_no_results(query: str) -> str:
    return f"<!-- No results -->\n<websearch query='{query}'><message>No results found</message></websearch>"
```

---

## üìñ –ü–æ—è—Å–Ω–µ–Ω–∏—è –∫ –∫–æ–¥—É

–≠—Ç–æ—Ç –∫–æ–¥ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–ª–æ–∫–∏—Ä—É—é—â–µ–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏ –≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–µ –≤–µ–±-–ø–æ–∏—Å–∫–∞. –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:

1. **–£–¥–∞–ª–µ–Ω–∏–µ `_run_async()`**: –§—É–Ω–∫—Ü–∏—è `_run_async()` (—Å—Ç—Ä–æ–∫–∏ 18-32) –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–µ–Ω–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ —Å–æ–∑–¥–∞–≤–∞–ª–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å event loop –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –∏–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

2. **–°–æ–∑–¥–∞–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏**: –§—É–Ω–∫—Ü–∏—è `general_web_search_tool()` —Ç–µ–ø–µ—Ä—å –æ–±—ä—è–≤–ª–µ–Ω–∞ –∫–∞–∫ `async def` –∏ —è–≤–ª—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–Ω–æ–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π. –û–Ω–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å—é –ª–æ–≥–∏–∫—É –ø–æ–∏—Å–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

3. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—ë—Ä—Ç–∫–∏**: –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è `general_web_search_tool_sync()`, –∫–æ—Ç–æ—Ä–∞—è —Å–ª—É–∂–∏—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—ë—Ä—Ç–∫–æ–π –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `asyncio.run()` –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏.

4. **–£–ª—É—á—à–µ–Ω–∏–µ `truncate_content()`**: –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–µ–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü—ã –æ–±—Ä–µ–∑–∫–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, —á–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º.

5. **–£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞**: –ò–º–ø–æ—Ä—Ç `concurrent.futures` —É–¥–∞–ª—ë–Ω, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è `_run_async()`.

**–ü–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤

---

## üõ†Ô∏è –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

1. ‚úÖ **read_file**
   - –ê—Ä–≥—É–º–µ–Ω—Ç—ã: `file_path=app/tools/general_web_search.py`

---

*–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: 2025-12-17T17:44:29.920526*