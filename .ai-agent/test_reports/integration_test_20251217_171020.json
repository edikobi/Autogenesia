{
  "metadata": {
    "timestamp": "2025-12-17T17:10:20.303181",
    "project_dir": "C:\\Users\\Admin\\AI_Assistant_Pro",
    "user_query": "Проанализируй файл general_web_search.py на предмет неправильной асинхронности, в случае, если проблема есть, напиши код всех методов и функций, где проблема",
    "duration_seconds": 200.10255479812622,
    "models_used": {
      "orchestrator": "Claude Opus 4.5 (RouterAI)",
      "code_generator": "deepseek-chat"
    }
  },
  "orchestrator": {
    "analysis": "",
    "instruction": "**Task:** Исправить проблему с асинхронностью в функции `general_web_search_tool`, которая может вызывать конфликты при работе в уже существующем async-контексте.\n\n**File:** `app/tools/general_web_search.py`\n\n**Location:** Функция `general_web_search_tool` (строки 39-73)\n\n**Changes:**\n\n1. **Добавить вспомогательную функцию `_run_async` после импортов (после строки 13, перед константами на строке 15)**:\n   \n   Создать функцию `_run_async(coro)`, которая безопасно запускает корутину:\n   - Сначала пытается получить текущий event loop через `asyncio.get_running_loop()`\n   - Если loop уже запущен (`RuntimeError` не возникает), использует `concurrent.futures.ThreadPoolExecutor` для запуска корутины в отдельном потоке с новым event loop\n   - Если loop не запущен, использует `asyncio.run(coro)` (Python 3.7+) — это более чистый способ, чем ручное создание/закрытие loop\n\n   Код функции:\n   ```python\n   def _run_async(coro):\n       \"\"\"\n       Безопасно запускает корутину из синхронного контекста.\n       Работает корректно как из обычного кода, так и из уже запущенного event loop.\n       \"\"\"\n       try:\n           loop = asyncio.get_running_loop()\n       except RuntimeError:\n           # Нет запущенного loop — можно использовать asyncio.run()\n           return asyncio.run(coro)\n       \n       # Если loop уже запущен, выполняем в отдельном потоке\n       with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n           future = executor.submit(asyncio.run, coro)\n           return future.result()\n   ```\n\n2. **Заменить содержимое функции `general_web_search_tool` (строки 39-73)**:\n   \n   Заменить блок try с ручным управлением event loop на вызов `_run_async`:\n   \n   ```python\n   def general_web_search_tool(query: str, max_results: int = 10, time_limit: str = \"w\", region: str = \"ru-ru\") -> str:\n       \"\"\"\n       Выполняет поиск в интернете для общих, юридических и новостных запросов.\n       \n       Args:\n           query: Поисковый запрос.\n           max_results: Максимум результатов (до 10).\n           time_limit: Фильтр по времени ('d' - день, 'w' - неделя, 'm' - месяц, 'y' - год, None - все время).\n           region: Регион поиска (по умолчанию 'ru-ru' для РФ).\n       \"\"\"\n       if not query:\n           return format_error(\"Query is required\")\n\n       max_results = min(max_results, 10)\n       \n       try:\n           result = _run_async(async_general_web_search(query, max_results, time_limit, region))\n               \n           if not result.success:\n               return format_error(result.error or \"Search failed\")\n               \n           if not result.pages:\n               return format_no_results(query)\n               \n           return format_results_xml(result)\n           \n       except Exception as e:\n           logger.error(f\"General web search error: {e}\")\n           return format_error(f\"Search failed: {e}\")\n   ```\n\n**Why:** Текущая реализация создаёт новый event loop и устанавливает его глобально через `asyncio.set_event_loop()`, что вызывает конфликты при вызове из уже работающего async-контекста (например, из FastAPI, aiohttp, или async-функций оркестратора). Новая реализация корректно определяет контекст выполнения и использует подходящий метод запуска корутины."
  },
  "code_generator": {
    "code_blocks": [
      {
        "filepath": "app/tools/general_web_search.py",
        "language": "python",
        "code": "from __future__ import annotations\nimport asyncio\nimport concurrent.futures\nimport logging\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dataclasses import dataclass, field\nfrom urllib.parse import urlparse, unquote\nimport httpx\nfrom app.utils.token_counter import TokenCounter\nfrom config.settings import cfg\n\nlogger = logging.getLogger(__name__)\n\ndef _run_async(coro):\n    \"\"\"\n    Безопасно запускает корутину из синхронного контекста.\n    Работает корректно как из обычного кода, так и из уже запущенного event loop.\n    \"\"\"\n    try:\n        loop = asyncio.get_running_loop()\n    except RuntimeError:\n        # Нет запущенного loop — можно использовать asyncio.run()\n        return asyncio.run(coro)\n    \n    # Если loop уже запущен, выполняем в отдельном потоке\n    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n        future = executor.submit(asyncio.run, coro)\n        return future.result()\n\n# Используем лимит из settings или дефолтный\nMAX_TOTAL_TOKENS = 25000  \nREQUEST_TIMEOUT = 15.0\nMAX_CONTENT_PER_PAGE = 4000 # Чуть меньше для статей, чтобы вместить больше источников\n\n@dataclass\nclass GeneralWebPage:\n    url: str\n    title: str\n    snippet: str\n    content: str = \"\"\n    tokens: int = 0\n    relevance_score: float = 0.0\n    error: Optional[str] = None\n    published_date: Optional[str] = None # Полезно для новостей\n\n@dataclass\nclass GeneralWebSearchResult:\n    success: bool\n    query: str\n    pages: List[GeneralWebPage] = field(default_factory=list)\n    total_tokens: int = 0\n    error: Optional[str] = None\n\ndef general_web_search_tool(query: str, max_results: int = 10, time_limit: str = \"w\", region: str = \"ru-ru\") -> str:\n    \"\"\"\n    Выполняет поиск в интернете для общих, юридических и новостных запросов.\n    \n    Args:\n        query: Поисковый запрос.\n        max_results: Максимум результатов (до 10).\n        time_limit: Фильтр по времени ('d' - день, 'w' - неделя, 'm' - месяц, 'y' - год, None - все время).\n        region: Регион поиска (по умолчанию 'ru-ru' для РФ).\n    \"\"\"\n    if not query:\n        return format_error(\"Query is required\")\n\n    max_results = min(max_results, 10)\n    \n    try:\n        result = _run_async(async_general_web_search(query, max_results, time_limit, region))\n            \n        if not result.success:\n            return format_error(result.error or \"Search failed\")\n            \n        if not result.pages:\n            return format_no_results(query)\n            \n        return format_results_xml(result)\n        \n    except Exception as e:\n        logger.error(f\"General web search error: {e}\")\n        return format_error(f\"Search failed: {e}\")\n\nasync def async_general_web_search(query: str, max_results: int, time_limit: str, region: str) -> GeneralWebSearchResult:\n    # 1. Получаем ссылки из DDG с учетом времени и региона\n    search_results = await duckduckgo_search(query, max_results * 2, time_limit, region)\n    \n    if not search_results:\n        return GeneralWebSearchResult(success=False, query=query, error=\"No search results found\")\n\n    # 2. Скачиваем контент страниц параллельно\n    pages = await fetch_pages_parallel(search_results, max_results)\n\n    # 3. Рассчитываем релевантность (Pseudo-Semantic Jaccard)\n    pages = calculate_relevance_scores(pages, query)\n    \n    # 4. Сортируем: сначала самые релевантные\n    pages.sort(key=lambda p: p.relevance_score, reverse=True)\n\n    # 5. Отбираем лучшие, пока влезаем в лимит токенов\n    selected_pages = select_within_token_limit(pages, MAX_TOTAL_TOKENS)\n    \n    total_tokens = sum(p.tokens for p in selected_pages)\n    \n    return GeneralWebSearchResult(\n        success=True,\n        query=query,\n        pages=selected_pages,\n        total_tokens=total_tokens\n    )\n\nasync def duckduckgo_search(query: str, num_results: int, time_limit: str, region: str) -> List[Dict[str, str]]:\n    \"\"\"Поиск через HTML версию DDG с поддержкой фильтров\"\"\"\n    search_url = \"https://html.duckduckgo.com/html/\"\n    \n    # Формируем параметры DDG\n    # df: w (week), m (month), d (day), y (year)\n    params = {\n        'q': query,\n        'kl': region, # region settings (ru-ru)\n    }\n    if time_limit and time_limit in ['d', 'w', 'm', 'y']:\n        params['df'] = time_limit\n\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n        \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\" # Важно для русских результатов\n    }\n\n    try:\n        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:\n            response = await client.post(search_url, data=params, headers=headers)\n            \n            if response.status_code != 200:\n                logger.warning(f\"DDG returned status {response.status_code}\")\n                return []\n\n            return parse_ddg_html(response.text, num_results)\n    except Exception as e:\n        logger.error(f\"DDG search error: {e}\")\n        return []\n\ndef parse_ddg_html(html: str, max_results: int) -> List[Dict[str, str]]:\n    results = []\n    # Чуть более строгий regex для парсинга\n    result_pattern = re.compile(r'<a[^>]*class=\"[^\"]*result__a[^\"]*\"[^>]*href=\"([^\"]+)\"[^>]*>(.*?)</a>', re.IGNORECASE)\n    snippet_pattern = re.compile(r'<a[^>]*class=\"[^\"]*result__snippet[^\"]*\"[^>]*>(.*?)</a>', re.IGNORECASE)\n    \n    matches = result_pattern.findall(html)\n    snippets = snippet_pattern.findall(html)\n    \n    for i, (url, title) in enumerate(matches):\n        if i >= max_results:\n            break\n            \n        actual_url = extract_actual_url(url)\n        if not is_valid_url(actual_url):\n            continue\n            \n        snippet = snippets[i] if i < len(snippets) else \"\"\n        \n        # Очистка HTML тегов из заголовка и сниппета\n        title = remove_html_tags(title)\n        snippet = remove_html_tags(snippet)\n        \n        results.append({\n            \"url\": actual_url,\n            \"title\": title.strip(),\n            \"snippet\": snippet.strip()\n        })\n        \n    return results\n\ndef remove_html_tags(text: str) -> str:\n    return re.sub(r'<[^>]+>', '', text)\n\ndef extract_actual_url(ddg_url: str) -> str:\n    if \"uddg=\" in ddg_url:\n        match = re.search(r'uddg=([^&]+)', ddg_url)\n        if match:\n            return unquote(match.group(1))\n    return ddg_url\n\ndef is_valid_url(url: str) -> bool:\n    try:\n        parsed = urlparse(url)\n        if not parsed.scheme or not parsed.netloc:\n            return False\n        # Игнорируем PDF и бинарники для скорости, если это не явно запрошено\n        if any(url.lower().endswith(ext) for ext in ['.pdf', '.doc', '.docx', '.xls', '.zip']):\n            return False \n        return True\n    except:\n        return False\n\nasync def fetch_pages_parallel(search_results: List[Dict[str, str]], max_results: int) -> List[GeneralWebPage]:\n    tasks = [fetch_single_page(r) for r in search_results]\n    pages = await asyncio.gather(*tasks)\n    # Фильтруем пустые и ошибочные\n    valid_pages = [p for p in pages if p.content and not p.error]\n    return valid_pages[:max_results]\n\nasync def fetch_single_page(result: Dict[str, str]) -> GeneralWebPage:\n    url = result['url']\n    try:\n        content = await fetch_page_content(url)\n        if not content:\n            return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error=\"Empty content\")\n            \n        counter = TokenCounter()\n        tokens = counter.count(content)\n        \n        # Если слишком длинно, обрезаем, но аккуратно\n        if tokens > MAX_CONTENT_PER_PAGE:\n            content = truncate_content(content, MAX_CONTENT_PER_PAGE, counter)\n            tokens = MAX_CONTENT_PER_PAGE\n            \n        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], content=content, tokens=tokens)\n    except Exception as e:\n        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error=str(e))\n\nasync def fetch_page_content(url: str) -> str:\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n        \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8\"\n    }\n    try:\n        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT, follow_redirects=True) as client:\n            response = await client.get(url, headers=headers)\n            if response.status_code != 200:\n                return \"\"\n            return extract_text_from_html(response.text)\n    except:\n        return \"\"\n\ndef extract_text_from_html(html: str) -> str:\n    \"\"\"Улучшенная очистка текста для статей\"\"\"\n    # Удаляем скрипты, стили, метрики\n    html = re.sub(r'<(script|style|svg|nav|footer|header|aside)[^>]*>.*?</\\1>', ' ', html, flags=re.DOTALL | re.IGNORECASE)\n    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)\n    \n    # Заменяем блочные теги на переносы строк\n    html = re.sub(r'</?(p|div|br|h[1-6]|li|tr)[^>]*>', '\\n', html, flags=re.IGNORECASE)\n    \n    # Удаляем все остальные теги\n    text = re.sub(r'<[^>]+>', '', html)\n    \n    # Декодируем HTML сущности (просто базовая очистка)\n    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&quot;', '\"').replace('&lt;', '<').replace('&gt;', '>')\n    \n    # Убираем множественные пробелы и переносы\n    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n    text = re.sub(r' +', ' ', text)\n    \n    return text.strip()\n\ndef truncate_content(content: str, max_tokens: int, counter: TokenCounter) -> str:\n    # Простая обрезка, можно улучшить до обрезки по параграфам\n    # Для скорости просто режем по символам (примерно 4 символа на токен) и проверяем\n    approx_chars = max_tokens * 4\n    if len(content) > approx_chars:\n        content = content[:approx_chars]\n    return content\n\ndef calculate_relevance_scores(pages: List[GeneralWebPage], query: str) -> List[GeneralWebPage]:\n    \"\"\"\n    Рассчитывает релевантность на основе Jaccard Similarity (пересечение множеств слов).\n    Это лучше, чем простой count, так как учитывает 'покрытие' запроса текстом.\n    \"\"\"\n    query_words = set(re.findall(r'\\w+', query.lower()))\n    if not query_words:\n        return pages\n\n    for page in pages:\n        score = 0.0\n        \n        # 1. Title Score (Очень важно)\n        title_words = set(re.findall(r'\\w+', page.title.lower()))\n        if title_words:\n            intersection = query_words.intersection(title_words)\n            # Коэффициент Жаккара для заголовка\n            score += (len(intersection) / len(query_words)) * 3.0 # Вес 3.0\n\n        # 2. Snippet Score\n        snippet_words = set(re.findall(r'\\w+', page.snippet.lower()))\n        if snippet_words:\n            intersection = query_words.intersection(snippet_words)\n            score += (len(intersection) / len(query_words)) * 1.5 # Вес 1.5\n\n        # 3. Content Score (плотность в начале текста важнее)\n        content_preview = page.content[:1000].lower() # Смотрим только начало для скорости и важности\n        content_words = set(re.findall(r'\\w+', content_preview))\n        if content_words:\n            intersection = query_words.intersection(content_words)\n            score += (len(intersection) / len(query_words)) * 1.0\n\n        page.relevance_score = score\n\n    return pages\n\ndef select_within_token_limit(pages: List[GeneralWebPage], limit: int) -> List[GeneralWebPage]:\n    selected = []\n    current_tokens = 0\n    for page in pages:\n        if current_tokens + page.tokens <= limit:\n            selected.append(page)\n            current_tokens += page.tokens\n        else:\n            # Если страница не влезает целиком, но место еще есть (>500 токенов), берем кусок\n            remaining = limit - current_tokens\n            if remaining > 500:\n                # Тут нужен truncate, но для простоты просто берем начало\n                # В реальном коде лучше вызвать truncate_content\n                page.content = page.content[:remaining * 4] \n                page.tokens = remaining\n                selected.append(page)\n            break\n    return selected\n\ndef format_results_xml(result: GeneralWebSearchResult) -> str:\n    parts = []\n    parts.append(f\"<!-- Web search results for '{result.query}' -->\")\n    parts.append(f\"<websearch query='{result.query}' tokens='{result.total_tokens}'>\")\n    for i, page in enumerate(result.pages, 1):\n        parts.append(f\"  <page rank='{i}' relevance='{page.relevance_score:.2f}' url='{page.url}'>\")\n        parts.append(f\"    <title>{page.title}</title>\")\n        parts.append(f\"    <content><![CDATA[{page.content}]]></content>\")\n        parts.append(f\"  </page>\")\n    parts.append(\"</websearch>\")\n    return \"\\n\".join(parts)\n\ndef format_error(msg: str) -> str:\n    return f\"<!-- ERROR -->\\n<error message='{msg}'/>\"\n\ndef format_no_results(query: str) -> str:\n    return f\"<!-- No results -->\\n<websearch query='{query}'><message>No results found</message></websearch>\"",
        "context": null
      }
    ],
    "explanation": "Внесены изменения для решения проблемы с асинхронностью в функции `general_web_search_tool`. Основная проблема старой реализации заключалась в том, что она создавала новый event loop и устанавливала его глобально через `asyncio.set_event_loop()`, что вызывало конфликты при вызове из уже работающего async-контекста (например, из FastAPI, aiohttp или других async-функций).\n\n**Что было сделано:**\n\n1. **Добавлена вспомогательная функция `_run_async(coro)`**:\n   - Функция размещена сразу после импортов, перед константами (как указано в инструкции)\n   - Она определяет, запущен ли уже event loop в текущем контексте\n   - Если loop не запущен (RuntimeError), используется `asyncio.run(coro)` — современный и безопасный способ\n   - Если loop уже запущен, функция использует `ThreadPoolExecutor` для запуска корутины в отдельном потоке с новым event loop\n\n2. **Изменена функция `general_web_search_tool`**:\n   - Удален блок с ручным созданием и управлением event loop (`asyncio.new_event_loop()`, `asyncio.set_event_loop()`, `loop.run_until_complete()`, `loop.close()`)\n   - Вместо этого используется вызов `_run_async(async_general_web_search(...))`\n   - Вся остальная логика функции (обработка ошибок, форматирование результатов) оста",
    "frontend_json": {
      "success": true,
      "code_blocks": [
        {
          "code": "from __future__ import annotations\nimport asyncio\nimport concurrent.futures\nimport logging\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dataclasses import dataclass, field\nfrom urllib.parse import urlparse, unquote\nimport httpx\nfrom app.utils.token_counter import TokenCounter\nfrom config.settings import cfg\n\nlogger = logging.getLogger(__name__)\n\ndef _run_async(coro):\n    \"\"\"\n    Безопасно запускает корутину из синхронного контекста.\n    Работает корректно как из обычного кода, так и из уже запущенного event loop.\n    \"\"\"\n    try:\n        loop = asyncio.get_running_loop()\n    except RuntimeError:\n        # Нет запущенного loop — можно использовать asyncio.run()\n        return asyncio.run(coro)\n    \n    # Если loop уже запущен, выполняем в отдельном потоке\n    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n        future = executor.submit(asyncio.run, coro)\n        return future.result()\n\n# Используем лимит из settings или дефолтный\nMAX_TOTAL_TOKENS = 25000  \nREQUEST_TIMEOUT = 15.0\nMAX_CONTENT_PER_PAGE = 4000 # Чуть меньше для статей, чтобы вместить больше источников\n\n@dataclass\nclass GeneralWebPage:\n    url: str\n    title: str\n    snippet: str\n    content: str = \"\"\n    tokens: int = 0\n    relevance_score: float = 0.0\n    error: Optional[str] = None\n    published_date: Optional[str] = None # Полезно для новостей\n\n@dataclass\nclass GeneralWebSearchResult:\n    success: bool\n    query: str\n    pages: List[GeneralWebPage] = field(default_factory=list)\n    total_tokens: int = 0\n    error: Optional[str] = None\n\ndef general_web_search_tool(query: str, max_results: int = 10, time_limit: str = \"w\", region: str = \"ru-ru\") -> str:\n    \"\"\"\n    Выполняет поиск в интернете для общих, юридических и новостных запросов.\n    \n    Args:\n        query: Поисковый запрос.\n        max_results: Максимум результатов (до 10).\n        time_limit: Фильтр по времени ('d' - день, 'w' - неделя, 'm' - месяц, 'y' - год, None - все время).\n        region: Регион поиска (по умолчанию 'ru-ru' для РФ).\n    \"\"\"\n    if not query:\n        return format_error(\"Query is required\")\n\n    max_results = min(max_results, 10)\n    \n    try:\n        result = _run_async(async_general_web_search(query, max_results, time_limit, region))\n            \n        if not result.success:\n            return format_error(result.error or \"Search failed\")\n            \n        if not result.pages:\n            return format_no_results(query)\n            \n        return format_results_xml(result)\n        \n    except Exception as e:\n        logger.error(f\"General web search error: {e}\")\n        return format_error(f\"Search failed: {e}\")\n\nasync def async_general_web_search(query: str, max_results: int, time_limit: str, region: str) -> GeneralWebSearchResult:\n    # 1. Получаем ссылки из DDG с учетом времени и региона\n    search_results = await duckduckgo_search(query, max_results * 2, time_limit, region)\n    \n    if not search_results:\n        return GeneralWebSearchResult(success=False, query=query, error=\"No search results found\")\n\n    # 2. Скачиваем контент страниц параллельно\n    pages = await fetch_pages_parallel(search_results, max_results)\n\n    # 3. Рассчитываем релевантность (Pseudo-Semantic Jaccard)\n    pages = calculate_relevance_scores(pages, query)\n    \n    # 4. Сортируем: сначала самые релевантные\n    pages.sort(key=lambda p: p.relevance_score, reverse=True)\n\n    # 5. Отбираем лучшие, пока влезаем в лимит токенов\n    selected_pages = select_within_token_limit(pages, MAX_TOTAL_TOKENS)\n    \n    total_tokens = sum(p.tokens for p in selected_pages)\n    \n    return GeneralWebSearchResult(\n        success=True,\n        query=query,\n        pages=selected_pages,\n        total_tokens=total_tokens\n    )\n\nasync def duckduckgo_search(query: str, num_results: int, time_limit: str, region: str) -> List[Dict[str, str]]:\n    \"\"\"Поиск через HTML версию DDG с поддержкой фильтров\"\"\"\n    search_url = \"https://html.duckduckgo.com/html/\"\n    \n    # Формируем параметры DDG\n    # df: w (week), m (month), d (day), y (year)\n    params = {\n        'q': query,\n        'kl': region, # region settings (ru-ru)\n    }\n    if time_limit and time_limit in ['d', 'w', 'm', 'y']:\n        params['df'] = time_limit\n\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n        \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\" # Важно для русских результатов\n    }\n\n    try:\n        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:\n            response = await client.post(search_url, data=params, headers=headers)\n            \n            if response.status_code != 200:\n                logger.warning(f\"DDG returned status {response.status_code}\")\n                return []\n\n            return parse_ddg_html(response.text, num_results)\n    except Exception as e:\n        logger.error(f\"DDG search error: {e}\")\n        return []\n\ndef parse_ddg_html(html: str, max_results: int) -> List[Dict[str, str]]:\n    results = []\n    # Чуть более строгий regex для парсинга\n    result_pattern = re.compile(r'<a[^>]*class=\"[^\"]*result__a[^\"]*\"[^>]*href=\"([^\"]+)\"[^>]*>(.*?)</a>', re.IGNORECASE)\n    snippet_pattern = re.compile(r'<a[^>]*class=\"[^\"]*result__snippet[^\"]*\"[^>]*>(.*?)</a>', re.IGNORECASE)\n    \n    matches = result_pattern.findall(html)\n    snippets = snippet_pattern.findall(html)\n    \n    for i, (url, title) in enumerate(matches):\n        if i >= max_results:\n            break\n            \n        actual_url = extract_actual_url(url)\n        if not is_valid_url(actual_url):\n            continue\n            \n        snippet = snippets[i] if i < len(snippets) else \"\"\n        \n        # Очистка HTML тегов из заголовка и сниппета\n        title = remove_html_tags(title)\n        snippet = remove_html_tags(snippet)\n        \n        results.append({\n            \"url\": actual_url,\n            \"title\": title.strip(),\n            \"snippet\": snippet.strip()\n        })\n        \n    return results\n\ndef remove_html_tags(text: str) -> str:\n    return re.sub(r'<[^>]+>', '', text)\n\ndef extract_actual_url(ddg_url: str) -> str:\n    if \"uddg=\" in ddg_url:\n        match = re.search(r'uddg=([^&]+)', ddg_url)\n        if match:\n            return unquote(match.group(1))\n    return ddg_url\n\ndef is_valid_url(url: str) -> bool:\n    try:\n        parsed = urlparse(url)\n        if not parsed.scheme or not parsed.netloc:\n            return False\n        # Игнорируем PDF и бинарники для скорости, если это не явно запрошено\n        if any(url.lower().endswith(ext) for ext in ['.pdf', '.doc', '.docx', '.xls', '.zip']):\n            return False \n        return True\n    except:\n        return False\n\nasync def fetch_pages_parallel(search_results: List[Dict[str, str]], max_results: int) -> List[GeneralWebPage]:\n    tasks = [fetch_single_page(r) for r in search_results]\n    pages = await asyncio.gather(*tasks)\n    # Фильтруем пустые и ошибочные\n    valid_pages = [p for p in pages if p.content and not p.error]\n    return valid_pages[:max_results]\n\nasync def fetch_single_page(result: Dict[str, str]) -> GeneralWebPage:\n    url = result['url']\n    try:\n        content = await fetch_page_content(url)\n        if not content:\n            return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error=\"Empty content\")\n            \n        counter = TokenCounter()\n        tokens = counter.count(content)\n        \n        # Если слишком длинно, обрезаем, но аккуратно\n        if tokens > MAX_CONTENT_PER_PAGE:\n            content = truncate_content(content, MAX_CONTENT_PER_PAGE, counter)\n            tokens = MAX_CONTENT_PER_PAGE\n            \n        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], content=content, tokens=tokens)\n    except Exception as e:\n        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error=str(e))\n\nasync def fetch_page_content(url: str) -> str:\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n        \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8\"\n    }\n    try:\n        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT, follow_redirects=True) as client:\n            response = await client.get(url, headers=headers)\n            if response.status_code != 200:\n                return \"\"\n            return extract_text_from_html(response.text)\n    except:\n        return \"\"\n\ndef extract_text_from_html(html: str) -> str:\n    \"\"\"Улучшенная очистка текста для статей\"\"\"\n    # Удаляем скрипты, стили, метрики\n    html = re.sub(r'<(script|style|svg|nav|footer|header|aside)[^>]*>.*?</\\1>', ' ', html, flags=re.DOTALL | re.IGNORECASE)\n    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)\n    \n    # Заменяем блочные теги на переносы строк\n    html = re.sub(r'</?(p|div|br|h[1-6]|li|tr)[^>]*>', '\\n', html, flags=re.IGNORECASE)\n    \n    # Удаляем все остальные теги\n    text = re.sub(r'<[^>]+>', '', html)\n    \n    # Декодируем HTML сущности (просто базовая очистка)\n    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&quot;', '\"').replace('&lt;', '<').replace('&gt;', '>')\n    \n    # Убираем множественные пробелы и переносы\n    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n    text = re.sub(r' +', ' ', text)\n    \n    return text.strip()\n\ndef truncate_content(content: str, max_tokens: int, counter: TokenCounter) -> str:\n    # Простая обрезка, можно улучшить до обрезки по параграфам\n    # Для скорости просто режем по символам (примерно 4 символа на токен) и проверяем\n    approx_chars = max_tokens * 4\n    if len(content) > approx_chars:\n        content = content[:approx_chars]\n    return content\n\ndef calculate_relevance_scores(pages: List[GeneralWebPage], query: str) -> List[GeneralWebPage]:\n    \"\"\"\n    Рассчитывает релевантность на основе Jaccard Similarity (пересечение множеств слов).\n    Это лучше, чем простой count, так как учитывает 'покрытие' запроса текстом.\n    \"\"\"\n    query_words = set(re.findall(r'\\w+', query.lower()))\n    if not query_words:\n        return pages\n\n    for page in pages:\n        score = 0.0\n        \n        # 1. Title Score (Очень важно)\n        title_words = set(re.findall(r'\\w+', page.title.lower()))\n        if title_words:\n            intersection = query_words.intersection(title_words)\n            # Коэффициент Жаккара для заголовка\n            score += (len(intersection) / len(query_words)) * 3.0 # Вес 3.0\n\n        # 2. Snippet Score\n        snippet_words = set(re.findall(r'\\w+', page.snippet.lower()))\n        if snippet_words:\n            intersection = query_words.intersection(snippet_words)\n            score += (len(intersection) / len(query_words)) * 1.5 # Вес 1.5\n\n        # 3. Content Score (плотность в начале текста важнее)\n        content_preview = page.content[:1000].lower() # Смотрим только начало для скорости и важности\n        content_words = set(re.findall(r'\\w+', content_preview))\n        if content_words:\n            intersection = query_words.intersection(content_words)\n            score += (len(intersection) / len(query_words)) * 1.0\n\n        page.relevance_score = score\n\n    return pages\n\ndef select_within_token_limit(pages: List[GeneralWebPage], limit: int) -> List[GeneralWebPage]:\n    selected = []\n    current_tokens = 0\n    for page in pages:\n        if current_tokens + page.tokens <= limit:\n            selected.append(page)\n            current_tokens += page.tokens\n        else:\n            # Если страница не влезает целиком, но место еще есть (>500 токенов), берем кусок\n            remaining = limit - current_tokens\n            if remaining > 500:\n                # Тут нужен truncate, но для простоты просто берем начало\n                # В реальном коде лучше вызвать truncate_content\n                page.content = page.content[:remaining * 4] \n                page.tokens = remaining\n                selected.append(page)\n            break\n    return selected\n\ndef format_results_xml(result: GeneralWebSearchResult) -> str:\n    parts = []\n    parts.append(f\"<!-- Web search results for '{result.query}' -->\")\n    parts.append(f\"<websearch query='{result.query}' tokens='{result.total_tokens}'>\")\n    for i, page in enumerate(result.pages, 1):\n        parts.append(f\"  <page rank='{i}' relevance='{page.relevance_score:.2f}' url='{page.url}'>\")\n        parts.append(f\"    <title>{page.title}</title>\")\n        parts.append(f\"    <content><![CDATA[{page.content}]]></content>\")\n        parts.append(f\"  </page>\")\n    parts.append(\"</websearch>\")\n    return \"\\n\".join(parts)\n\ndef format_error(msg: str) -> str:\n    return f\"<!-- ERROR -->\\n<error message='{msg}'/>\"\n\ndef format_no_results(query: str) -> str:\n    return f\"<!-- No results -->\\n<websearch query='{query}'><message>No results found</message></websearch>\"",
          "filepath": "app/tools/general_web_search.py",
          "language": "python",
          "context": null
        }
      ],
      "combined_code": "\n\n# ==================================================# filepath: app/tools/general_web_search.py\n\nfrom __future__ import annotations\nimport asyncio\nimport concurrent.futures\nimport logging\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dataclasses import dataclass, field\nfrom urllib.parse import urlparse, unquote\nimport httpx\nfrom app.utils.token_counter import TokenCounter\nfrom config.settings import cfg\n\nlogger = logging.getLogger(__name__)\n\ndef _run_async(coro):\n    \"\"\"\n    Безопасно запускает корутину из синхронного контекста.\n    Работает корректно как из обычного кода, так и из уже запущенного event loop.\n    \"\"\"\n    try:\n        loop = asyncio.get_running_loop()\n    except RuntimeError:\n        # Нет запущенного loop — можно использовать asyncio.run()\n        return asyncio.run(coro)\n    \n    # Если loop уже запущен, выполняем в отдельном потоке\n    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n        future = executor.submit(asyncio.run, coro)\n        return future.result()\n\n# Используем лимит из settings или дефолтный\nMAX_TOTAL_TOKENS = 25000  \nREQUEST_TIMEOUT = 15.0\nMAX_CONTENT_PER_PAGE = 4000 # Чуть меньше для статей, чтобы вместить больше источников\n\n@dataclass\nclass GeneralWebPage:\n    url: str\n    title: str\n    snippet: str\n    content: str = \"\"\n    tokens: int = 0\n    relevance_score: float = 0.0\n    error: Optional[str] = None\n    published_date: Optional[str] = None # Полезно для новостей\n\n@dataclass\nclass GeneralWebSearchResult:\n    success: bool\n    query: str\n    pages: List[GeneralWebPage] = field(default_factory=list)\n    total_tokens: int = 0\n    error: Optional[str] = None\n\ndef general_web_search_tool(query: str, max_results: int = 10, time_limit: str = \"w\", region: str = \"ru-ru\") -> str:\n    \"\"\"\n    Выполняет поиск в интернете для общих, юридических и новостных запросов.\n    \n    Args:\n        query: Поисковый запрос.\n        max_results: Максимум результатов (до 10).\n        time_limit: Фильтр по времени ('d' - день, 'w' - неделя, 'm' - месяц, 'y' - год, None - все время).\n        region: Регион поиска (по умолчанию 'ru-ru' для РФ).\n    \"\"\"\n    if not query:\n        return format_error(\"Query is required\")\n\n    max_results = min(max_results, 10)\n    \n    try:\n        result = _run_async(async_general_web_search(query, max_results, time_limit, region))\n            \n        if not result.success:\n            return format_error(result.error or \"Search failed\")\n            \n        if not result.pages:\n            return format_no_results(query)\n            \n        return format_results_xml(result)\n        \n    except Exception as e:\n        logger.error(f\"General web search error: {e}\")\n        return format_error(f\"Search failed: {e}\")\n\nasync def async_general_web_search(query: str, max_results: int, time_limit: str, region: str) -> GeneralWebSearchResult:\n    # 1. Получаем ссылки из DDG с учетом времени и региона\n    search_results = await duckduckgo_search(query, max_results * 2, time_limit, region)\n    \n    if not search_results:\n        return GeneralWebSearchResult(success=False, query=query, error=\"No search results found\")\n\n    # 2. Скачиваем контент страниц параллельно\n    pages = await fetch_pages_parallel(search_results, max_results)\n\n    # 3. Рассчитываем релевантность (Pseudo-Semantic Jaccard)\n    pages = calculate_relevance_scores(pages, query)\n    \n    # 4. Сортируем: сначала самые релевантные\n    pages.sort(key=lambda p: p.relevance_score, reverse=True)\n\n    # 5. Отбираем лучшие, пока влезаем в лимит токенов\n    selected_pages = select_within_token_limit(pages, MAX_TOTAL_TOKENS)\n    \n    total_tokens = sum(p.tokens for p in selected_pages)\n    \n    return GeneralWebSearchResult(\n        success=True,\n        query=query,\n        pages=selected_pages,\n        total_tokens=total_tokens\n    )\n\nasync def duckduckgo_search(query: str, num_results: int, time_limit: str, region: str) -> List[Dict[str, str]]:\n    \"\"\"Поиск через HTML версию DDG с поддержкой фильтров\"\"\"\n    search_url = \"https://html.duckduckgo.com/html/\"\n    \n    # Формируем параметры DDG\n    # df: w (week), m (month), d (day), y (year)\n    params = {\n        'q': query,\n        'kl': region, # region settings (ru-ru)\n    }\n    if time_limit and time_limit in ['d', 'w', 'm', 'y']:\n        params['df'] = time_limit\n\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n        \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\" # Важно для русских результатов\n    }\n\n    try:\n        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:\n            response = await client.post(search_url, data=params, headers=headers)\n            \n            if response.status_code != 200:\n                logger.warning(f\"DDG returned status {response.status_code}\")\n                return []\n\n            return parse_ddg_html(response.text, num_results)\n    except Exception as e:\n        logger.error(f\"DDG search error: {e}\")\n        return []\n\ndef parse_ddg_html(html: str, max_results: int) -> List[Dict[str, str]]:\n    results = []\n    # Чуть более строгий regex для парсинга\n    result_pattern = re.compile(r'<a[^>]*class=\"[^\"]*result__a[^\"]*\"[^>]*href=\"([^\"]+)\"[^>]*>(.*?)</a>', re.IGNORECASE)\n    snippet_pattern = re.compile(r'<a[^>]*class=\"[^\"]*result__snippet[^\"]*\"[^>]*>(.*?)</a>', re.IGNORECASE)\n    \n    matches = result_pattern.findall(html)\n    snippets = snippet_pattern.findall(html)\n    \n    for i, (url, title) in enumerate(matches):\n        if i >= max_results:\n            break\n            \n        actual_url = extract_actual_url(url)\n        if not is_valid_url(actual_url):\n            continue\n            \n        snippet = snippets[i] if i < len(snippets) else \"\"\n        \n        # Очистка HTML тегов из заголовка и сниппета\n        title = remove_html_tags(title)\n        snippet = remove_html_tags(snippet)\n        \n        results.append({\n            \"url\": actual_url,\n            \"title\": title.strip(),\n            \"snippet\": snippet.strip()\n        })\n        \n    return results\n\ndef remove_html_tags(text: str) -> str:\n    return re.sub(r'<[^>]+>', '', text)\n\ndef extract_actual_url(ddg_url: str) -> str:\n    if \"uddg=\" in ddg_url:\n        match = re.search(r'uddg=([^&]+)', ddg_url)\n        if match:\n            return unquote(match.group(1))\n    return ddg_url\n\ndef is_valid_url(url: str) -> bool:\n    try:\n        parsed = urlparse(url)\n        if not parsed.scheme or not parsed.netloc:\n            return False\n        # Игнорируем PDF и бинарники для скорости, если это не явно запрошено\n        if any(url.lower().endswith(ext) for ext in ['.pdf', '.doc', '.docx', '.xls', '.zip']):\n            return False \n        return True\n    except:\n        return False\n\nasync def fetch_pages_parallel(search_results: List[Dict[str, str]], max_results: int) -> List[GeneralWebPage]:\n    tasks = [fetch_single_page(r) for r in search_results]\n    pages = await asyncio.gather(*tasks)\n    # Фильтруем пустые и ошибочные\n    valid_pages = [p for p in pages if p.content and not p.error]\n    return valid_pages[:max_results]\n\nasync def fetch_single_page(result: Dict[str, str]) -> GeneralWebPage:\n    url = result['url']\n    try:\n        content = await fetch_page_content(url)\n        if not content:\n            return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error=\"Empty content\")\n            \n        counter = TokenCounter()\n        tokens = counter.count(content)\n        \n        # Если слишком длинно, обрезаем, но аккуратно\n        if tokens > MAX_CONTENT_PER_PAGE:\n            content = truncate_content(content, MAX_CONTENT_PER_PAGE, counter)\n            tokens = MAX_CONTENT_PER_PAGE\n            \n        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], content=content, tokens=tokens)\n    except Exception as e:\n        return GeneralWebPage(url=url, title=result['title'], snippet=result['snippet'], error=str(e))\n\nasync def fetch_page_content(url: str) -> str:\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n        \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8\"\n    }\n    try:\n        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT, follow_redirects=True) as client:\n            response = await client.get(url, headers=headers)\n            if response.status_code != 200:\n                return \"\"\n            return extract_text_from_html(response.text)\n    except:\n        return \"\"\n\ndef extract_text_from_html(html: str) -> str:\n    \"\"\"Улучшенная очистка текста для статей\"\"\"\n    # Удаляем скрипты, стили, метрики\n    html = re.sub(r'<(script|style|svg|nav|footer|header|aside)[^>]*>.*?</\\1>', ' ', html, flags=re.DOTALL | re.IGNORECASE)\n    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)\n    \n    # Заменяем блочные теги на переносы строк\n    html = re.sub(r'</?(p|div|br|h[1-6]|li|tr)[^>]*>', '\\n', html, flags=re.IGNORECASE)\n    \n    # Удаляем все остальные теги\n    text = re.sub(r'<[^>]+>', '', html)\n    \n    # Декодируем HTML сущности (просто базовая очистка)\n    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&quot;', '\"').replace('&lt;', '<').replace('&gt;', '>')\n    \n    # Убираем множественные пробелы и переносы\n    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n    text = re.sub(r' +', ' ', text)\n    \n    return text.strip()\n\ndef truncate_content(content: str, max_tokens: int, counter: TokenCounter) -> str:\n    # Простая обрезка, можно улучшить до обрезки по параграфам\n    # Для скорости просто режем по символам (примерно 4 символа на токен) и проверяем\n    approx_chars = max_tokens * 4\n    if len(content) > approx_chars:\n        content = content[:approx_chars]\n    return content\n\ndef calculate_relevance_scores(pages: List[GeneralWebPage], query: str) -> List[GeneralWebPage]:\n    \"\"\"\n    Рассчитывает релевантность на основе Jaccard Similarity (пересечение множеств слов).\n    Это лучше, чем простой count, так как учитывает 'покрытие' запроса текстом.\n    \"\"\"\n    query_words = set(re.findall(r'\\w+', query.lower()))\n    if not query_words:\n        return pages\n\n    for page in pages:\n        score = 0.0\n        \n        # 1. Title Score (Очень важно)\n        title_words = set(re.findall(r'\\w+', page.title.lower()))\n        if title_words:\n            intersection = query_words.intersection(title_words)\n            # Коэффициент Жаккара для заголовка\n            score += (len(intersection) / len(query_words)) * 3.0 # Вес 3.0\n\n        # 2. Snippet Score\n        snippet_words = set(re.findall(r'\\w+', page.snippet.lower()))\n        if snippet_words:\n            intersection = query_words.intersection(snippet_words)\n            score += (len(intersection) / len(query_words)) * 1.5 # Вес 1.5\n\n        # 3. Content Score (плотность в начале текста важнее)\n        content_preview = page.content[:1000].lower() # Смотрим только начало для скорости и важности\n        content_words = set(re.findall(r'\\w+', content_preview))\n        if content_words:\n            intersection = query_words.intersection(content_words)\n            score += (len(intersection) / len(query_words)) * 1.0\n\n        page.relevance_score = score\n\n    return pages\n\ndef select_within_token_limit(pages: List[GeneralWebPage], limit: int) -> List[GeneralWebPage]:\n    selected = []\n    current_tokens = 0\n    for page in pages:\n        if current_tokens + page.tokens <= limit:\n            selected.append(page)\n            current_tokens += page.tokens\n        else:\n            # Если страница не влезает целиком, но место еще есть (>500 токенов), берем кусок\n            remaining = limit - current_tokens\n            if remaining > 500:\n                # Тут нужен truncate, но для простоты просто берем начало\n                # В реальном коде лучше вызвать truncate_content\n                page.content = page.content[:remaining * 4] \n                page.tokens = remaining\n                selected.append(page)\n            break\n    return selected\n\ndef format_results_xml(result: GeneralWebSearchResult) -> str:\n    parts = []\n    parts.append(f\"<!-- Web search results for '{result.query}' -->\")\n    parts.append(f\"<websearch query='{result.query}' tokens='{result.total_tokens}'>\")\n    for i, page in enumerate(result.pages, 1):\n        parts.append(f\"  <page rank='{i}' relevance='{page.relevance_score:.2f}' url='{page.url}'>\")\n        parts.append(f\"    <title>{page.title}</title>\")\n        parts.append(f\"    <content><![CDATA[{page.content}]]></content>\")\n        parts.append(f\"  </page>\")\n    parts.append(\"</websearch>\")\n    return \"\\n\".join(parts)\n\ndef format_error(msg: str) -> str:\n    return f\"<!-- ERROR -->\\n<error message='{msg}'/>\"\n\ndef format_no_results(query: str) -> str:\n    return f\"<!-- No results -->\\n<websearch query='{query}'><message>No results found</message></websearch>\"",
      "explanation": "Внесены изменения для решения проблемы с асинхронностью в функции `general_web_search_tool`. Основная проблема старой реализации заключалась в том, что она создавала новый event loop и устанавливала его глобально через `asyncio.set_event_loop()`, что вызывало конфликты при вызове из уже работающего async-контекста (например, из FastAPI, aiohttp или других async-функций).\n\n**Что было сделано:**\n\n1. **Добавлена вспомогательная функция `_run_async(coro)`**:\n   - Функция размещена сразу после импортов, перед константами (как указано в инструкции)\n   - Она определяет, запущен ли уже event loop в текущем контексте\n   - Если loop не запущен (RuntimeError), используется `asyncio.run(coro)` — современный и безопасный способ\n   - Если loop уже запущен, функция использует `ThreadPoolExecutor` для запуска корутины в отдельном потоке с новым event loop\n\n2. **Изменена функция `general_web_search_tool`**:\n   - Удален блок с ручным созданием и управлением event loop (`asyncio.new_event_loop()`, `asyncio.set_event_loop()`, `loop.run_until_complete()`, `loop.close()`)\n   - Вместо этого используется вызов `_run_async(async_general_web_search(...))`\n   - Вся остальная логика функции (обработка ошибок, форматирование результатов) оста",
      "error": null,
      "model_used": "deepseek-chat",
      "tokens_used": 0
    }
  }
}