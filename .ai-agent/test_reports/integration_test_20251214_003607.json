{
  "metadata": {
    "timestamp": "2025-12-14T00:36:07.665573",
    "project_dir": "C:\\Users\\Admin\\AI_Assistant_Pro",
    "user_query": "Можешь проанализировать мое создание индексной карты, сейчас она создается для кода только Python, можешь ли предложить как можно чанкировать код и создавать по нему индексную карту по другим языкам программирования. После этого напиши код и поясни, как это внедрить в мой проект",
    "duration_seconds": 170.21097111701965,
    "models_used": {
      "orchestrator": "Claude 4.5 (RouterAI)",
      "code_generator": "deepseek-chat"
    }
  },
  "orchestrator": {
    "analysis": "",
    "instruction": "**Task:** Создать мультиязычную систему чанкирования с унифицированным интерфейсом и интеграцией в существующий `SemanticIndexer`.\n\n---"
  },
  "code_generator": {
    "code_blocks": [
      {
        "filepath": "app/services/chunking/multilingual_chunker.py",
        "language": "python",
        "code": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional\nimport re\nfrom langdetect import detect, DetectorFactory\nfrom langdetect.lang_detect_exception import LangDetectException\n\n# Ensure consistent language detection\nDetectorFactory.seed = 0\n\n\nclass BaseChunker(ABC):\n    \"\"\"Abstract base class for all chunkers.\"\"\"\n    \n    @abstractmethod\n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into chunks.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters for specific chunkers\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Get metadata for a chunk.\n        \n        Args:\n            chunk: Text chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            Dictionary with chunk metadata\n        \"\"\"\n        pass\n\n\nclass LanguageAwareChunker(BaseChunker):\n    \"\"\"Chunker that adapts to different languages.\"\"\"\n    \n    # Language-specific sentence boundary patterns\n    SENTENCE_PATTERNS = {\n        'en': r'(?<=[.!?])\\s+',  # English: period, exclamation, question mark\n        'ru': r'(?<=[.!?])\\s+',  # Russian: same punctuation\n        'de': r'(?<=[.!?])\\s+',  # German\n        'fr': r'(?<=[.!?])\\s+',  # French\n        'es': r'(?<=[.!?])\\s+',  # Spanish\n        'zh': r'(?<=[。！？])\\s*',  # Chinese: full-width punctuation\n        'ja': r'(?<=[。！？])\\s*',  # Japanese\n        'ko': r'(?<=[.!?。！？])\\s+',  # Korean: mixed punctuation\n    }\n    \n    # Default chunk size in characters (approximate)\n    DEFAULT_CHUNK_SIZES = {\n        'en': 1000,\n        'ru': 1000,\n        'de': 1000,\n        'fr': 1000,\n        'es': 1000,\n        'zh': 500,   # Chinese characters are more dense\n        'ja': 500,\n        'ko': 800,\n    }\n    \n    def __init__(self, default_language: str = 'en'):\n        \"\"\"\n        Initialize language-aware chunker.\n        \n        Args:\n            default_language: Default language code if detection fails\n        \"\"\"\n        self.default_language = default_language\n    \n    def detect_language(self, text: str) -> str:\n        \"\"\"\n        Detect language of text.\n        \n        Args:\n            text: Text to analyze\n            \n        Returns:\n            Language code (ISO 639-1)\n        \"\"\"\n        if not text or len(text.strip()) < 10:\n            return self.default_language\n        \n        try:\n            # Take first 500 chars for faster detection\n            sample = text[:500]\n            lang = detect(sample)\n            return lang if lang in self.SENTENCE_PATTERNS else self.default_language\n        except (LangDetectException, Exception):\n            return self.default_language\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into chunks based on language.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: \n                - language: Force specific language code\n                - chunk_size: Override default chunk size\n                - overlap: Number of characters to overlap between chunks\n                \n        Returns:\n            List of text chunks\n        \"\"\"\n        # Get parameters\n        language = kwargs.get('language') or self.detect_language(text)\n        chunk_size = kwargs.get('chunk_size') or self.DEFAULT_CHUNK_SIZES.get(language, 1000)\n        overlap = kwargs.get('overlap', 100)\n        \n        # Get sentence pattern for this language\n        sentence_pattern = self.SENTENCE_PATTERNS.get(language, self.SENTENCE_PATTERNS['en'])\n        \n        # Split into sentences\n        sentences = re.split(sentence_pattern, text)\n        \n        chunks = []\n        current_chunk = \"\"\n        \n        for sentence in sentences:\n            # If adding this sentence would exceed chunk size and we have content\n            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n                chunks.append(current_chunk.strip())\n                \n                # Start new chunk with overlap from previous\n                if overlap > 0 and chunks:\n                    last_chunk = chunks[-1]\n                    overlap_text = last_chunk[-overlap:] if len(last_chunk) > overlap else last_chunk\n                    current_chunk = overlap_text + \" \" + sentence\n                else:\n                    current_chunk = sentence\n            else:\n                if current_chunk:\n                    current_chunk += \" \" + sentence\n                else:\n                    current_chunk = sentence\n        \n        # Add the last chunk if not empty\n        if current_chunk.strip():\n            chunks.append(current_chunk.strip())\n        \n        return chunks\n    \n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Get metadata for a chunk.\n        \n        Args:\n            chunk: Text chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            Dictionary with chunk metadata\n        \"\"\"\n        language = kwargs.get('language') or self.detect_language(chunk)\n        \n        return {\n            'language': language,\n            'length_chars': len(chunk),\n            'length_words': len(chunk.split()),\n            'chunker_type': 'language_aware',\n            'has_sentence_boundary': bool(re.search(r'[.!?。！？]$', chunk.strip())),\n        }\n\n\nclass SemanticChunker(BaseChunker):\n    \"\"\"Chunker that tries to preserve semantic boundaries.\"\"\"\n    \n    def __init__(self, threshold: float = 0.7):\n        \"\"\"\n        Initialize semantic chunker.\n        \n        Args:\n            threshold: Similarity threshold for semantic boundaries\n        \"\"\"\n        self.threshold = threshold\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into semantic chunks.\n        \n        Note: This is a simplified version. In production, you would\n        integrate with a semantic similarity model.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        # For now, use paragraph-based chunking as a proxy for semantic chunks\n        paragraphs = text.split('\\n\\n')\n        \n        chunks = []\n        current_chunk = \"\"\n        max_chunk_size = kwargs.get('chunk_size', 1500)\n        \n        for paragraph in paragraphs:\n            paragraph = paragraph.strip()\n            if not paragraph:\n                continue\n                \n            if len(current_chunk) + len(paragraph) <= max_chunk_size:\n                if current_chunk:\n                    current_chunk += \"\\n\\n\" + paragraph\n                else:\n                    current_chunk = paragraph\n            else:\n                if current_chunk:\n                    chunks.append(current_chunk)\n                current_chunk = paragraph\n        \n        if current_chunk:\n            chunks.append(current_chunk)\n        \n        return chunks\n    \n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Get metadata for a semantic chunk.\n        \n        Args:\n            chunk: Text chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            Dictionary with chunk metadata\n        \"\"\"\n        return {\n            'chunker_type': 'semantic',\n            'length_chars': len(chunk),\n            'paragraph_count': chunk.count('\\n\\n') + 1,\n            'is_coherent': len(chunk.split('\\n\\n')) == 1,  # Single paragraph\n        }\n\n\nclass MultilingualChunker:\n    \"\"\"Unified interface for multilingual chunking.\"\"\"\n    \n    def __init__(self, chunker_type: str = 'language_aware', **chunker_kwargs):\n        \"\"\"\n        Initialize multilingual chunker.\n        \n        Args:\n            chunker_type: Type of chunker ('language_aware', 'semantic', or 'fixed')\n            **chunker_kwargs: Arguments for the specific chunker\n        \"\"\"\n        self.chunker_type = chunker_type\n        \n        if chunker_type == 'language_aware':\n            self.chunker = LanguageAwareChunker(**chunker_kwargs)\n        elif chunker_type == 'semantic':\n            self.chunker = SemanticChunker(**chunker_kwargs)\n        elif chunker_type == 'fixed':\n            self.chunker = FixedSizeChunker(**chunker_kwargs)\n        else:\n            raise ValueError(f\"Unknown chunker type: {chunker_type}\")\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into chunks using the configured chunker.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters for the chunker\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        return self.chunker.chunk(text, **kwargs)\n    \n    def chunk_with_metadata(self, text: str, **kwargs) -> List[Dict[str, Any]]:\n        \"\"\"\n        Split text into chunks and get metadata for each chunk.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            List of dictionaries with 'text' and 'metadata' keys\n        \"\"\"\n        chunks = self.chunk(text, **kwargs)\n        result = []\n        \n        for i, chunk in enumerate(chunks):\n            metadata = self.chunker.get_chunk_metadata(chunk, **kwargs)\n            metadata['chunk_index'] = i\n            metadata['total_chunks'] = len(chunks)\n            \n            result.append({\n                'text': chunk,\n                'metadata': metadata\n            })\n        \n        return result\n    \n    def detect_language(self, text: str) -> str:\n        \"\"\"\n        Detect language of text.\n        \n        Args:\n            text: Text to analyze\n            \n        Returns:\n            Language code\n        \"\"\"\n        if hasattr(self.chunker, 'detect_language'):\n            return self.chunker.detect_language(text)\n        else:\n            # Fallback to LanguageAwareChunker's detection\n            detector = LanguageAwareChunker()\n            return detector.detect_language(text)\n\n\nclass FixedSizeChunker(BaseChunker):\n    \"\"\"Simple fixed-size chunker for backward compatibility.\"\"\"\n    \n    def __init__(self, chunk_size: int = 1000, overlap: int = 100):\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        chunk_size = kwargs.get('chunk_size', self.chunk_size)\n        overlap = kwargs.get('overlap', self.overlap)\n        \n        chunks = []\n        start = 0\n        \n        while start < len(text):\n            end = start + chunk_size\n            \n            if end < len(text):\n                # Try to break at sentence boundary\n                sentence_end = text.rfind('.', start, end)\n                if sentence_end > start + chunk_size * 0.7:  # Only if we're close to end\n                    end = sentence_end + 1\n            \n            chunks.append(text[start:end].strip())\n            \n            # Move start with overlap\n            start = end - overlap if end - overlap > start else end\n        \n        return chunks\n    \n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        return {\n            'chunker_type': 'fixed_size',\n            'length_chars': len(chunk),\n        }\n\n\n# Factory function for easy integration\ndef create_chunker(chunker_type: str = 'language_aware', **kwargs) -> MultilingualChunker:\n    \"\"\"\n    Factory function to create a chunker.\n    \n    Args:\n        chunker_type: Type of chunker\n        **kwargs: Arguments for the chunker\n        \n    Returns:\n        MultilingualChunker instance\n    \"\"\"\n    return MultilingualChunker(chunker_type, **kwargs)",
        "context": "MultilingualChunker class"
      },
      {
        "filepath": "app/services/chunking/multilingual_chunker.py",
        "language": "python",
        "code": "from app.services.chunking.multilingual_chunker import create_chunker",
        "context": null
      },
      {
        "filepath": "app/services/chunking/multilingual_chunker.py",
        "language": "python",
        "code": "chunker = create_chunker('language_aware', default_language='ru')",
        "context": null
      },
      {
        "filepath": "app/services/chunking/multilingual_chunker.py",
        "language": "python",
        "code": "chunks_with_metadata = chunker.chunk_with_metadata(document_text)\n   for chunk_data in chunks_with_metadata:\n       text = chunk_data['text']\n       metadata = chunk_data['metadata']\n       # Индексировать text с metadata",
        "context": null
      }
    ],
    "explanation": "Этот код создает мультиязычную систему чанкирования (разбиения текста на фрагменты) с унифицированным интерфейсом для интеграции в существующий `SemanticIndexer`.\n\n**Что делает код:**\n\n1. **Определяет абстрактный базовый класс `BaseChunker`** - задает общий интерфейс для всех типов чанкеров с методами `chunk()` и `get_chunk_metadata()`.\n\n2. **Реализует несколько специализированных чанкеров:**\n   - `LanguageAwareChunker` - адаптируется к разным языкам, используя языково-специфичные правила для определения границ предложений и оптимальных размеров чанков\n   - `SemanticChunker` - пытается сохранять семантические границы (в упрощенной версии использует разбиение по абзацам)\n   - `FixedSizeChunker` - простой чанкер с фиксированным размером для обратной совместимости\n\n3. **Создает основной класс `MultilingualChunker`** - предоставляет унифицированный интерфейс для работы с любым типом чанкера, включая:\n   - Метод `chunk()` для разбиения текста\n   - Метод `chunk_with_metadata()` для получения чанков с метаданными\n   - Метод `detect_language()` для определения языка текста\n\n4. **Включает фабричную функцию `create_chunker()`** - упрощает создание экземпляров чанкеров.\n\n**Ключевые особенности:**\n\n- **Автоматическое определение языка** с использованием библиотеки `langdetect`\n- **Языково-специфичные правила** для 8+ языков (английский, русский, китайский, японский и др.)\n- **Гибкая настройка** через параметры (размер чанка, перекрытие, язык)\n- **Метаданные для каждого чанка** - информация о языке, размере, границах предложений\n- **Поддержка перекрытия (overlap)** между чанками для сохранения контекста\n\n**Как интегрировать с SemanticIndexer:**\n\n1. Импортировать чанкер в индексер:\n   \n\n2. Создать экземпляр чанкера:\n   \n\n3. Использовать для разбиения документов:\n   \n\n**Важные замечания:**\n\n- Код использует `langdetect` для определения языка - для production может потребоваться более надежное решение\n- `SemanticChunker` в текущей реализации использует упрощенный подход; для настоящего семантического чанкинга потребуется интеграция с моделями эмбеддингов\n- Все строки и комментарии в коде на английском, как требуется\n- Система легко расширяема - можно добавить новые чанкеры, унаследовав от `BaseChunker`",
    "frontend_json": {
      "success": true,
      "code_blocks": [
        {
          "code": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional\nimport re\nfrom langdetect import detect, DetectorFactory\nfrom langdetect.lang_detect_exception import LangDetectException\n\n# Ensure consistent language detection\nDetectorFactory.seed = 0\n\n\nclass BaseChunker(ABC):\n    \"\"\"Abstract base class for all chunkers.\"\"\"\n    \n    @abstractmethod\n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into chunks.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters for specific chunkers\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Get metadata for a chunk.\n        \n        Args:\n            chunk: Text chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            Dictionary with chunk metadata\n        \"\"\"\n        pass\n\n\nclass LanguageAwareChunker(BaseChunker):\n    \"\"\"Chunker that adapts to different languages.\"\"\"\n    \n    # Language-specific sentence boundary patterns\n    SENTENCE_PATTERNS = {\n        'en': r'(?<=[.!?])\\s+',  # English: period, exclamation, question mark\n        'ru': r'(?<=[.!?])\\s+',  # Russian: same punctuation\n        'de': r'(?<=[.!?])\\s+',  # German\n        'fr': r'(?<=[.!?])\\s+',  # French\n        'es': r'(?<=[.!?])\\s+',  # Spanish\n        'zh': r'(?<=[。！？])\\s*',  # Chinese: full-width punctuation\n        'ja': r'(?<=[。！？])\\s*',  # Japanese\n        'ko': r'(?<=[.!?。！？])\\s+',  # Korean: mixed punctuation\n    }\n    \n    # Default chunk size in characters (approximate)\n    DEFAULT_CHUNK_SIZES = {\n        'en': 1000,\n        'ru': 1000,\n        'de': 1000,\n        'fr': 1000,\n        'es': 1000,\n        'zh': 500,   # Chinese characters are more dense\n        'ja': 500,\n        'ko': 800,\n    }\n    \n    def __init__(self, default_language: str = 'en'):\n        \"\"\"\n        Initialize language-aware chunker.\n        \n        Args:\n            default_language: Default language code if detection fails\n        \"\"\"\n        self.default_language = default_language\n    \n    def detect_language(self, text: str) -> str:\n        \"\"\"\n        Detect language of text.\n        \n        Args:\n            text: Text to analyze\n            \n        Returns:\n            Language code (ISO 639-1)\n        \"\"\"\n        if not text or len(text.strip()) < 10:\n            return self.default_language\n        \n        try:\n            # Take first 500 chars for faster detection\n            sample = text[:500]\n            lang = detect(sample)\n            return lang if lang in self.SENTENCE_PATTERNS else self.default_language\n        except (LangDetectException, Exception):\n            return self.default_language\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into chunks based on language.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: \n                - language: Force specific language code\n                - chunk_size: Override default chunk size\n                - overlap: Number of characters to overlap between chunks\n                \n        Returns:\n            List of text chunks\n        \"\"\"\n        # Get parameters\n        language = kwargs.get('language') or self.detect_language(text)\n        chunk_size = kwargs.get('chunk_size') or self.DEFAULT_CHUNK_SIZES.get(language, 1000)\n        overlap = kwargs.get('overlap', 100)\n        \n        # Get sentence pattern for this language\n        sentence_pattern = self.SENTENCE_PATTERNS.get(language, self.SENTENCE_PATTERNS['en'])\n        \n        # Split into sentences\n        sentences = re.split(sentence_pattern, text)\n        \n        chunks = []\n        current_chunk = \"\"\n        \n        for sentence in sentences:\n            # If adding this sentence would exceed chunk size and we have content\n            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n                chunks.append(current_chunk.strip())\n                \n                # Start new chunk with overlap from previous\n                if overlap > 0 and chunks:\n                    last_chunk = chunks[-1]\n                    overlap_text = last_chunk[-overlap:] if len(last_chunk) > overlap else last_chunk\n                    current_chunk = overlap_text + \" \" + sentence\n                else:\n                    current_chunk = sentence\n            else:\n                if current_chunk:\n                    current_chunk += \" \" + sentence\n                else:\n                    current_chunk = sentence\n        \n        # Add the last chunk if not empty\n        if current_chunk.strip():\n            chunks.append(current_chunk.strip())\n        \n        return chunks\n    \n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Get metadata for a chunk.\n        \n        Args:\n            chunk: Text chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            Dictionary with chunk metadata\n        \"\"\"\n        language = kwargs.get('language') or self.detect_language(chunk)\n        \n        return {\n            'language': language,\n            'length_chars': len(chunk),\n            'length_words': len(chunk.split()),\n            'chunker_type': 'language_aware',\n            'has_sentence_boundary': bool(re.search(r'[.!?。！？]$', chunk.strip())),\n        }\n\n\nclass SemanticChunker(BaseChunker):\n    \"\"\"Chunker that tries to preserve semantic boundaries.\"\"\"\n    \n    def __init__(self, threshold: float = 0.7):\n        \"\"\"\n        Initialize semantic chunker.\n        \n        Args:\n            threshold: Similarity threshold for semantic boundaries\n        \"\"\"\n        self.threshold = threshold\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into semantic chunks.\n        \n        Note: This is a simplified version. In production, you would\n        integrate with a semantic similarity model.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        # For now, use paragraph-based chunking as a proxy for semantic chunks\n        paragraphs = text.split('\\n\\n')\n        \n        chunks = []\n        current_chunk = \"\"\n        max_chunk_size = kwargs.get('chunk_size', 1500)\n        \n        for paragraph in paragraphs:\n            paragraph = paragraph.strip()\n            if not paragraph:\n                continue\n                \n            if len(current_chunk) + len(paragraph) <= max_chunk_size:\n                if current_chunk:\n                    current_chunk += \"\\n\\n\" + paragraph\n                else:\n                    current_chunk = paragraph\n            else:\n                if current_chunk:\n                    chunks.append(current_chunk)\n                current_chunk = paragraph\n        \n        if current_chunk:\n            chunks.append(current_chunk)\n        \n        return chunks\n    \n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Get metadata for a semantic chunk.\n        \n        Args:\n            chunk: Text chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            Dictionary with chunk metadata\n        \"\"\"\n        return {\n            'chunker_type': 'semantic',\n            'length_chars': len(chunk),\n            'paragraph_count': chunk.count('\\n\\n') + 1,\n            'is_coherent': len(chunk.split('\\n\\n')) == 1,  # Single paragraph\n        }\n\n\nclass MultilingualChunker:\n    \"\"\"Unified interface for multilingual chunking.\"\"\"\n    \n    def __init__(self, chunker_type: str = 'language_aware', **chunker_kwargs):\n        \"\"\"\n        Initialize multilingual chunker.\n        \n        Args:\n            chunker_type: Type of chunker ('language_aware', 'semantic', or 'fixed')\n            **chunker_kwargs: Arguments for the specific chunker\n        \"\"\"\n        self.chunker_type = chunker_type\n        \n        if chunker_type == 'language_aware':\n            self.chunker = LanguageAwareChunker(**chunker_kwargs)\n        elif chunker_type == 'semantic':\n            self.chunker = SemanticChunker(**chunker_kwargs)\n        elif chunker_type == 'fixed':\n            self.chunker = FixedSizeChunker(**chunker_kwargs)\n        else:\n            raise ValueError(f\"Unknown chunker type: {chunker_type}\")\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into chunks using the configured chunker.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters for the chunker\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        return self.chunker.chunk(text, **kwargs)\n    \n    def chunk_with_metadata(self, text: str, **kwargs) -> List[Dict[str, Any]]:\n        \"\"\"\n        Split text into chunks and get metadata for each chunk.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            List of dictionaries with 'text' and 'metadata' keys\n        \"\"\"\n        chunks = self.chunk(text, **kwargs)\n        result = []\n        \n        for i, chunk in enumerate(chunks):\n            metadata = self.chunker.get_chunk_metadata(chunk, **kwargs)\n            metadata['chunk_index'] = i\n            metadata['total_chunks'] = len(chunks)\n            \n            result.append({\n                'text': chunk,\n                'metadata': metadata\n            })\n        \n        return result\n    \n    def detect_language(self, text: str) -> str:\n        \"\"\"\n        Detect language of text.\n        \n        Args:\n            text: Text to analyze\n            \n        Returns:\n            Language code\n        \"\"\"\n        if hasattr(self.chunker, 'detect_language'):\n            return self.chunker.detect_language(text)\n        else:\n            # Fallback to LanguageAwareChunker's detection\n            detector = LanguageAwareChunker()\n            return detector.detect_language(text)\n\n\nclass FixedSizeChunker(BaseChunker):\n    \"\"\"Simple fixed-size chunker for backward compatibility.\"\"\"\n    \n    def __init__(self, chunk_size: int = 1000, overlap: int = 100):\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        chunk_size = kwargs.get('chunk_size', self.chunk_size)\n        overlap = kwargs.get('overlap', self.overlap)\n        \n        chunks = []\n        start = 0\n        \n        while start < len(text):\n            end = start + chunk_size\n            \n            if end < len(text):\n                # Try to break at sentence boundary\n                sentence_end = text.rfind('.', start, end)\n                if sentence_end > start + chunk_size * 0.7:  # Only if we're close to end\n                    end = sentence_end + 1\n            \n            chunks.append(text[start:end].strip())\n            \n            # Move start with overlap\n            start = end - overlap if end - overlap > start else end\n        \n        return chunks\n    \n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        return {\n            'chunker_type': 'fixed_size',\n            'length_chars': len(chunk),\n        }\n\n\n# Factory function for easy integration\ndef create_chunker(chunker_type: str = 'language_aware', **kwargs) -> MultilingualChunker:\n    \"\"\"\n    Factory function to create a chunker.\n    \n    Args:\n        chunker_type: Type of chunker\n        **kwargs: Arguments for the chunker\n        \n    Returns:\n        MultilingualChunker instance\n    \"\"\"\n    return MultilingualChunker(chunker_type, **kwargs)",
          "filepath": "app/services/chunking/multilingual_chunker.py",
          "language": "python",
          "context": "MultilingualChunker class"
        },
        {
          "code": "from app.services.chunking.multilingual_chunker import create_chunker",
          "filepath": "app/services/chunking/multilingual_chunker.py",
          "language": "python",
          "context": null
        },
        {
          "code": "chunker = create_chunker('language_aware', default_language='ru')",
          "filepath": "app/services/chunking/multilingual_chunker.py",
          "language": "python",
          "context": null
        },
        {
          "code": "chunks_with_metadata = chunker.chunk_with_metadata(document_text)\n   for chunk_data in chunks_with_metadata:\n       text = chunk_data['text']\n       metadata = chunk_data['metadata']\n       # Индексировать text с metadata",
          "filepath": "app/services/chunking/multilingual_chunker.py",
          "language": "python",
          "context": null
        }
      ],
      "combined_code": "\n\n# ==================================================# filepath: app/services/chunking/multilingual_chunker.py\n# context: MultilingualChunker class\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional\nimport re\nfrom langdetect import detect, DetectorFactory\nfrom langdetect.lang_detect_exception import LangDetectException\n\n# Ensure consistent language detection\nDetectorFactory.seed = 0\n\n\nclass BaseChunker(ABC):\n    \"\"\"Abstract base class for all chunkers.\"\"\"\n    \n    @abstractmethod\n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into chunks.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters for specific chunkers\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Get metadata for a chunk.\n        \n        Args:\n            chunk: Text chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            Dictionary with chunk metadata\n        \"\"\"\n        pass\n\n\nclass LanguageAwareChunker(BaseChunker):\n    \"\"\"Chunker that adapts to different languages.\"\"\"\n    \n    # Language-specific sentence boundary patterns\n    SENTENCE_PATTERNS = {\n        'en': r'(?<=[.!?])\\s+',  # English: period, exclamation, question mark\n        'ru': r'(?<=[.!?])\\s+',  # Russian: same punctuation\n        'de': r'(?<=[.!?])\\s+',  # German\n        'fr': r'(?<=[.!?])\\s+',  # French\n        'es': r'(?<=[.!?])\\s+',  # Spanish\n        'zh': r'(?<=[。！？])\\s*',  # Chinese: full-width punctuation\n        'ja': r'(?<=[。！？])\\s*',  # Japanese\n        'ko': r'(?<=[.!?。！？])\\s+',  # Korean: mixed punctuation\n    }\n    \n    # Default chunk size in characters (approximate)\n    DEFAULT_CHUNK_SIZES = {\n        'en': 1000,\n        'ru': 1000,\n        'de': 1000,\n        'fr': 1000,\n        'es': 1000,\n        'zh': 500,   # Chinese characters are more dense\n        'ja': 500,\n        'ko': 800,\n    }\n    \n    def __init__(self, default_language: str = 'en'):\n        \"\"\"\n        Initialize language-aware chunker.\n        \n        Args:\n            default_language: Default language code if detection fails\n        \"\"\"\n        self.default_language = default_language\n    \n    def detect_language(self, text: str) -> str:\n        \"\"\"\n        Detect language of text.\n        \n        Args:\n            text: Text to analyze\n            \n        Returns:\n            Language code (ISO 639-1)\n        \"\"\"\n        if not text or len(text.strip()) < 10:\n            return self.default_language\n        \n        try:\n            # Take first 500 chars for faster detection\n            sample = text[:500]\n            lang = detect(sample)\n            return lang if lang in self.SENTENCE_PATTERNS else self.default_language\n        except (LangDetectException, Exception):\n            return self.default_language\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into chunks based on language.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: \n                - language: Force specific language code\n                - chunk_size: Override default chunk size\n                - overlap: Number of characters to overlap between chunks\n                \n        Returns:\n            List of text chunks\n        \"\"\"\n        # Get parameters\n        language = kwargs.get('language') or self.detect_language(text)\n        chunk_size = kwargs.get('chunk_size') or self.DEFAULT_CHUNK_SIZES.get(language, 1000)\n        overlap = kwargs.get('overlap', 100)\n        \n        # Get sentence pattern for this language\n        sentence_pattern = self.SENTENCE_PATTERNS.get(language, self.SENTENCE_PATTERNS['en'])\n        \n        # Split into sentences\n        sentences = re.split(sentence_pattern, text)\n        \n        chunks = []\n        current_chunk = \"\"\n        \n        for sentence in sentences:\n            # If adding this sentence would exceed chunk size and we have content\n            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n                chunks.append(current_chunk.strip())\n                \n                # Start new chunk with overlap from previous\n                if overlap > 0 and chunks:\n                    last_chunk = chunks[-1]\n                    overlap_text = last_chunk[-overlap:] if len(last_chunk) > overlap else last_chunk\n                    current_chunk = overlap_text + \" \" + sentence\n                else:\n                    current_chunk = sentence\n            else:\n                if current_chunk:\n                    current_chunk += \" \" + sentence\n                else:\n                    current_chunk = sentence\n        \n        # Add the last chunk if not empty\n        if current_chunk.strip():\n            chunks.append(current_chunk.strip())\n        \n        return chunks\n    \n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Get metadata for a chunk.\n        \n        Args:\n            chunk: Text chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            Dictionary with chunk metadata\n        \"\"\"\n        language = kwargs.get('language') or self.detect_language(chunk)\n        \n        return {\n            'language': language,\n            'length_chars': len(chunk),\n            'length_words': len(chunk.split()),\n            'chunker_type': 'language_aware',\n            'has_sentence_boundary': bool(re.search(r'[.!?。！？]$', chunk.strip())),\n        }\n\n\nclass SemanticChunker(BaseChunker):\n    \"\"\"Chunker that tries to preserve semantic boundaries.\"\"\"\n    \n    def __init__(self, threshold: float = 0.7):\n        \"\"\"\n        Initialize semantic chunker.\n        \n        Args:\n            threshold: Similarity threshold for semantic boundaries\n        \"\"\"\n        self.threshold = threshold\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into semantic chunks.\n        \n        Note: This is a simplified version. In production, you would\n        integrate with a semantic similarity model.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        # For now, use paragraph-based chunking as a proxy for semantic chunks\n        paragraphs = text.split('\\n\\n')\n        \n        chunks = []\n        current_chunk = \"\"\n        max_chunk_size = kwargs.get('chunk_size', 1500)\n        \n        for paragraph in paragraphs:\n            paragraph = paragraph.strip()\n            if not paragraph:\n                continue\n                \n            if len(current_chunk) + len(paragraph) <= max_chunk_size:\n                if current_chunk:\n                    current_chunk += \"\\n\\n\" + paragraph\n                else:\n                    current_chunk = paragraph\n            else:\n                if current_chunk:\n                    chunks.append(current_chunk)\n                current_chunk = paragraph\n        \n        if current_chunk:\n            chunks.append(current_chunk)\n        \n        return chunks\n    \n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Get metadata for a semantic chunk.\n        \n        Args:\n            chunk: Text chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            Dictionary with chunk metadata\n        \"\"\"\n        return {\n            'chunker_type': 'semantic',\n            'length_chars': len(chunk),\n            'paragraph_count': chunk.count('\\n\\n') + 1,\n            'is_coherent': len(chunk.split('\\n\\n')) == 1,  # Single paragraph\n        }\n\n\nclass MultilingualChunker:\n    \"\"\"Unified interface for multilingual chunking.\"\"\"\n    \n    def __init__(self, chunker_type: str = 'language_aware', **chunker_kwargs):\n        \"\"\"\n        Initialize multilingual chunker.\n        \n        Args:\n            chunker_type: Type of chunker ('language_aware', 'semantic', or 'fixed')\n            **chunker_kwargs: Arguments for the specific chunker\n        \"\"\"\n        self.chunker_type = chunker_type\n        \n        if chunker_type == 'language_aware':\n            self.chunker = LanguageAwareChunker(**chunker_kwargs)\n        elif chunker_type == 'semantic':\n            self.chunker = SemanticChunker(**chunker_kwargs)\n        elif chunker_type == 'fixed':\n            self.chunker = FixedSizeChunker(**chunker_kwargs)\n        else:\n            raise ValueError(f\"Unknown chunker type: {chunker_type}\")\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        \"\"\"\n        Split text into chunks using the configured chunker.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters for the chunker\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        return self.chunker.chunk(text, **kwargs)\n    \n    def chunk_with_metadata(self, text: str, **kwargs) -> List[Dict[str, Any]]:\n        \"\"\"\n        Split text into chunks and get metadata for each chunk.\n        \n        Args:\n            text: Input text to chunk\n            **kwargs: Additional parameters\n            \n        Returns:\n            List of dictionaries with 'text' and 'metadata' keys\n        \"\"\"\n        chunks = self.chunk(text, **kwargs)\n        result = []\n        \n        for i, chunk in enumerate(chunks):\n            metadata = self.chunker.get_chunk_metadata(chunk, **kwargs)\n            metadata['chunk_index'] = i\n            metadata['total_chunks'] = len(chunks)\n            \n            result.append({\n                'text': chunk,\n                'metadata': metadata\n            })\n        \n        return result\n    \n    def detect_language(self, text: str) -> str:\n        \"\"\"\n        Detect language of text.\n        \n        Args:\n            text: Text to analyze\n            \n        Returns:\n            Language code\n        \"\"\"\n        if hasattr(self.chunker, 'detect_language'):\n            return self.chunker.detect_language(text)\n        else:\n            # Fallback to LanguageAwareChunker's detection\n            detector = LanguageAwareChunker()\n            return detector.detect_language(text)\n\n\nclass FixedSizeChunker(BaseChunker):\n    \"\"\"Simple fixed-size chunker for backward compatibility.\"\"\"\n    \n    def __init__(self, chunk_size: int = 1000, overlap: int = 100):\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n    \n    def chunk(self, text: str, **kwargs) -> List[str]:\n        chunk_size = kwargs.get('chunk_size', self.chunk_size)\n        overlap = kwargs.get('overlap', self.overlap)\n        \n        chunks = []\n        start = 0\n        \n        while start < len(text):\n            end = start + chunk_size\n            \n            if end < len(text):\n                # Try to break at sentence boundary\n                sentence_end = text.rfind('.', start, end)\n                if sentence_end > start + chunk_size * 0.7:  # Only if we're close to end\n                    end = sentence_end + 1\n            \n            chunks.append(text[start:end].strip())\n            \n            # Move start with overlap\n            start = end - overlap if end - overlap > start else end\n        \n        return chunks\n    \n    def get_chunk_metadata(self, chunk: str, **kwargs) -> Dict[str, Any]:\n        return {\n            'chunker_type': 'fixed_size',\n            'length_chars': len(chunk),\n        }\n\n\n# Factory function for easy integration\ndef create_chunker(chunker_type: str = 'language_aware', **kwargs) -> MultilingualChunker:\n    \"\"\"\n    Factory function to create a chunker.\n    \n    Args:\n        chunker_type: Type of chunker\n        **kwargs: Arguments for the chunker\n        \n    Returns:\n        MultilingualChunker instance\n    \"\"\"\n    return MultilingualChunker(chunker_type, **kwargs)\n\n# filepath: app/services/chunking/multilingual_chunker.py\n\nfrom app.services.chunking.multilingual_chunker import create_chunker\n\n# filepath: app/services/chunking/multilingual_chunker.py\n\nchunker = create_chunker('language_aware', default_language='ru')\n\n# filepath: app/services/chunking/multilingual_chunker.py\n\nchunks_with_metadata = chunker.chunk_with_metadata(document_text)\n   for chunk_data in chunks_with_metadata:\n       text = chunk_data['text']\n       metadata = chunk_data['metadata']\n       # Индексировать text с metadata",
      "explanation": "Этот код создает мультиязычную систему чанкирования (разбиения текста на фрагменты) с унифицированным интерфейсом для интеграции в существующий `SemanticIndexer`.\n\n**Что делает код:**\n\n1. **Определяет абстрактный базовый класс `BaseChunker`** - задает общий интерфейс для всех типов чанкеров с методами `chunk()` и `get_chunk_metadata()`.\n\n2. **Реализует несколько специализированных чанкеров:**\n   - `LanguageAwareChunker` - адаптируется к разным языкам, используя языково-специфичные правила для определения границ предложений и оптимальных размеров чанков\n   - `SemanticChunker` - пытается сохранять семантические границы (в упрощенной версии использует разбиение по абзацам)\n   - `FixedSizeChunker` - простой чанкер с фиксированным размером для обратной совместимости\n\n3. **Создает основной класс `MultilingualChunker`** - предоставляет унифицированный интерфейс для работы с любым типом чанкера, включая:\n   - Метод `chunk()` для разбиения текста\n   - Метод `chunk_with_metadata()` для получения чанков с метаданными\n   - Метод `detect_language()` для определения языка текста\n\n4. **Включает фабричную функцию `create_chunker()`** - упрощает создание экземпляров чанкеров.\n\n**Ключевые особенности:**\n\n- **Автоматическое определение языка** с использованием библиотеки `langdetect`\n- **Языково-специфичные правила** для 8+ языков (английский, русский, китайский, японский и др.)\n- **Гибкая настройка** через параметры (размер чанка, перекрытие, язык)\n- **Метаданные для каждого чанка** - информация о языке, размере, границах предложений\n- **Поддержка перекрытия (overlap)** между чанками для сохранения контекста\n\n**Как интегрировать с SemanticIndexer:**\n\n1. Импортировать чанкер в индексер:\n   \n\n2. Создать экземпляр чанкера:\n   \n\n3. Использовать для разбиения документов:\n   \n\n**Важные замечания:**\n\n- Код использует `langdetect` для определения языка - для production может потребоваться более надежное решение\n- `SemanticChunker` в текущей реализации использует упрощенный подход; для настоящего семантического чанкинга потребуется интеграция с моделями эмбеддингов\n- Все строки и комментарии в коде на английском, как требуется\n- Система легко расширяема - можно добавить новые чанкеры, унаследовав от `BaseChunker`",
      "error": null,
      "model_used": "deepseek-chat",
      "tokens_used": 0
    }
  }
}