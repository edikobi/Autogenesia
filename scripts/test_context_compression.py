# scripts/test_context_compression.py
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∂–∞—Ç–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ Agent Mode.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –î–µ–ª–∞–µ—Ç –†–ï–ê–õ–¨–ù–´–ï API –≤—ã–∑–æ–≤—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
- –î–ª—è Gemini 3.0 Pro –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞—Å—Ç–æ—è—â–∏–µ thought_signature
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–Ω–∏–∂–µ–Ω–Ω—ã–µ –ª–∏–º–∏—Ç—ã (threshold=1000, target=500) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
"""

import asyncio
import sys
import os
import logging
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import cfg
from app.llm.api_client import call_llm_with_tools
from app.utils.token_counter import TokenCounter
from app.history.context_manager import (
    IntraSessionCompressor,
    get_compressor,
    is_context_overflow_error,
    CompressionResult,
    CompressionMode,
)

# ============================================================================
# LOGGING SETUP
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================================================
# TOKEN COUNTER
# ============================================================================

_token_counter = TokenCounter()

def count_tokens(text: str) -> int:
    """–ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ."""
    return _token_counter.count(text)

def count_messages_tokens(messages: List[Dict]) -> int:
    """–ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö."""
    total = 0
    for msg in messages:
        content = msg.get("content", "")
        if isinstance(content, str):
            total += count_tokens(content)
        elif isinstance(content, list):
            for part in content:
                if isinstance(part, dict) and "text" in part:
                    total += count_tokens(part["text"])
    return total

# ============================================================================
# TEST CONFIGURATION
# ============================================================================

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
TEST_MODELS = {
    "gpt_codex": cfg.MODEL_GPT_5_1_Codex_MAX,
    "deepseek_reasoner": cfg.MODEL_DEEPSEEK_REASONER,
    "gemini_3_pro": cfg.MODEL_GEMINI_3_PRO,
}

# –¢–µ—Å—Ç–æ–≤—ã–µ –ª–∏–º–∏—Ç—ã (—Å–∏–ª—å–Ω–æ —Å–Ω–∏–∂–µ–Ω—ã)
TEST_LIMITS = {
    cfg.MODEL_DEEPSEEK_REASONER: {"threshold": 1000, "target": 500},
    cfg.MODEL_GEMINI_3_PRO: {"threshold": 1000, "target": 500},
}

# –ü—Ä–æ—Å—Ç—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞
TEST_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "get_file_info",
            "description": "Get information about a file",
            "parameters": {
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "File name"}
                },
                "required": ["filename"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "analyze_code",
            "description": "Analyze code structure",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "Code to analyze"}
                },
                "required": ["code"]
            }
        }
    }
]

# ============================================================================
# TEST COMPRESSOR WITH LOW LIMITS
# ============================================================================

class TestCompressor(IntraSessionCompressor):
    """–ö–æ–º–ø—Ä–µ—Å—Å–æ—Ä —Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º–∏ –ª–∏–º–∏—Ç–∞–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    
    def __init__(self, model: str, threshold: int, target: int):
        super().__init__(model)
        self.threshold_tokens = threshold
        self.target_tokens = target
        self._proactive_config = {"threshold": threshold, "target": target}
        logger.info(f"TestCompressor for {model}: threshold={threshold}, target={target}")


def get_test_compressor(model: str) -> IntraSessionCompressor:
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –ª–∏–º–∏—Ç–∞–º–∏."""
    if model in TEST_LIMITS:
        limits = TEST_LIMITS[model]
        return TestCompressor(model, limits["threshold"], limits["target"])
    return get_compressor(model)

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def create_tool_result(iteration: int) -> str:
    """–°–æ–∑–¥–∞—ë—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ (~200-300 —Ç–æ–∫–µ–Ω–æ–≤)."""
    return f"""
# File: module_{iteration}.py
# Analysis results for iteration {iteration}

class Handler{iteration}:
    '''Handles processing for component {iteration}.'''
    
    def __init__(self, config: dict):
        self.config = config
        self.cache = {{}}
        self.initialized = False
        
    def process(self, data: list) -> dict:
        '''Process incoming data and return results.'''
        results = {{}}
        for item in data:
            key = f"item_{{item['id']}}"
            results[key] = self._transform(item)
        return results
        
    def _transform(self, item: dict) -> dict:
        '''Internal transformation logic.'''
        return {{
            "original": item,
            "processed": True,
            "timestamp": "2024-01-{iteration:02d}"
        }}

def helper_function_{iteration}(x: int, y: int) -> int:
    '''Calculate result for iteration {iteration}.'''
    return x * y + {iteration}

# Constants
MAX_ITEMS_{iteration} = 1000
DEFAULT_TIMEOUT_{iteration} = 30
"""


def log_messages_summary(messages: List[Dict], label: str = "") -> int:
    """–õ–æ–≥–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—è–º."""
    total_tokens = count_messages_tokens(messages)
    tool_count = len([m for m in messages if m.get("role") == "tool"])
    assistant_count = len([m for m in messages if m.get("role") == "assistant"])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ thought_signature
    thought_sig_count = len([
        m for m in messages 
        if m.get("role") == "assistant" and m.get("thought_signature")
    ])
    
    logger.info(f"{label} {len(messages)} msgs, {tool_count} tool, {assistant_count} assistant, ~{total_tokens} tokens")
    if thought_sig_count > 0:
        logger.info(f"{label} thought_signature present in {thought_sig_count} messages")
    
    return total_tokens


def extract_assistant_fields(response: Dict) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –≤–∞–∂–Ω—ã–µ –ø–æ–ª—è –∏–∑ –æ—Ç–≤–µ—Ç–∞ assistant."""
    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —á—Ç–æ content –Ω–µ –ø—É—Å—Ç–æ–π (Gemini —Ç—Ä–µ–±—É–µ—Ç parts)
    content = response.get("content") or ""
    
    # –ï—Å–ª–∏ content –ø—É—Å—Ç–æ–π –∏ –Ω–µ—Ç tool_calls ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º placeholder
    if not content and not response.get("tool_calls"):
        content = "I'll continue analyzing."
    
    fields = {
        "role": "assistant",
        "content": content,
    }
    
    # Tool calls
    if response.get("tool_calls"):
        fields["tool_calls"] = response["tool_calls"]
    
    # Reasoning fields (–≤–∞–∂–Ω–æ –¥–ª—è Gemini –∏ DeepSeek!)
    for field in ["thought_signature", "reasoning_content", "reasoning_details"]:
        if response.get(field):
            fields[field] = response[field]
    
    return fields

# ============================================================================
# CORE TEST: BUILD CONTEXT WITH REAL API CALLS
# ============================================================================

async def build_real_context(
    model: str,
    num_iterations: int = 5,
    target_tokens: int = 2000,
) -> List[Dict[str, Any]]:
    """
    –°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –†–ï–ê–õ–¨–ù–´–ï API –≤—ã–∑–æ–≤—ã.
    
    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —á—Ç–æ thought_signature –∏ –¥—Ä—É–≥–∏–µ –ø–æ–ª—è ‚Äî –Ω–∞—Å—Ç–æ—è—â–∏–µ.
    
    Args:
        model: ID –º–æ–¥–µ–ª–∏
        num_iterations: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
        target_tokens: –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç API
    """
    messages = [
        {
            "role": "system",
            "content": "You are a code analysis assistant. When asked to analyze files, use the provided tools. Be concise in responses."
        },
        {
            "role": "user",
            "content": "Analyze the Python modules in my project. Start by getting info about the first few files."
        }
    ]
    
    iteration = 0
    
    while iteration < num_iterations or count_messages_tokens(messages) < target_tokens:
        iteration += 1
        current_tokens = count_messages_tokens(messages)
        
        logger.info(f"  Iteration {iteration}: {current_tokens} tokens, {len(messages)} messages")
        
        # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
        if iteration > 15:
            logger.warning("  Max iterations reached, stopping")
            break
        
        try:
            # –î–µ–ª–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π API –≤—ã–∑–æ–≤
            response = await call_llm_with_tools(
                model=model,
                messages=messages,
                tools=TEST_TOOLS,
                temperature=0,
                max_tokens=300,
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø–æ–ª—è (–≤–∫–ª—é—á–∞—è thought_signature!)
            assistant_msg = extract_assistant_fields(response)
            messages.append(assistant_msg)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏
            has_thought_sig = "thought_signature" in assistant_msg
            has_tool_calls = "tool_calls" in assistant_msg
            logger.info(f"    Got response: thought_sig={has_thought_sig}, tool_calls={has_tool_calls}")
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å tool calls ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            tool_calls = response.get("tool_calls", [])
            if tool_calls:
                for tc in tool_calls:
                    tool_result = {
                        "role": "tool",
                        "tool_call_id": tc.get("id", f"call_{iteration}"),
                        "name": tc.get("function", {}).get("name", "unknown"),
                        "content": create_tool_result(iteration)
                    }
                    messages.append(tool_result)
            else:
                # –ú–æ–¥–µ–ª—å –Ω–µ –≤—ã–∑–≤–∞–ª–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π assistant message –∏–º–µ–µ—Ç content
                if messages[-1].get("role") == "assistant" and not messages[-1].get("content"):
                    messages[-1]["content"] = "I understand. Let me continue."
                
                # –î–æ–±–∞–≤–ª—è–µ–º user message —á—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å
                messages.append({
                    "role": "user",
                    "content": f"Now analyze module_{iteration}.py using the get_file_info tool."
                })            
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –≤—ã–∑–æ–≤–∞–º–∏
            is_gemini = "gemini" in model.lower()
            delay = 3.0 if is_gemini else 0.5
            await asyncio.sleep(delay)
            
        except Exception as e:
            logger.error(f"    API error at iteration {iteration}: {e}")
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ context overflow
            if is_context_overflow_error(e):
                logger.warning("    Context overflow detected, stopping build")
                break
            raise
    
    logger.info(f"  Built context: {len(messages)} messages, ~{count_messages_tokens(messages)} tokens")
    return messages


# ============================================================================
# TEST CASES
# ============================================================================

async def test_no_proactive_compression(model: str, model_name: str) -> bool:
    """
    –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ –ë–ï–ó –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è (GPT-5.1).
    –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–∂–∞—Ç–∏–µ –ù–ï –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –¥–∞–∂–µ –ø—Ä–∏ –±–æ–ª—å—à–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.
    """
    logger.info("=" * 70)
    logger.info(f"TEST: {model_name} - NO proactive compression")
    logger.info("=" * 70)
    
    # –°—Ç—Ä–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω—ã–µ –≤—ã–∑–æ–≤—ã
    logger.info("Building context with real API calls...")
    messages = await build_real_context(model, num_iterations=3, target_tokens=1500)
    
    original_count = len(messages)
    original_tokens = log_messages_summary(messages, "BEFORE:")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä (–æ–±—ã—á–Ω—ã–π, –Ω–µ —Ç–µ—Å—Ç–æ–≤—ã–π!)
    compressor = get_compressor(model)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–∞ –º–æ–¥–µ–ª—å –ù–ï –≤ —Å–ø–∏—Å–∫–µ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è
    logger.info(f"Needs proactive compression: {compressor.needs_proactive_compression}")
    
    # –ü—Ä–æ–±—É–µ–º —Å–∂–∞—Ç—å
    compressed_messages, result = await compressor.check_and_compress(messages)
    
    if result:
        logger.warning(f"‚ö†Ô∏è Unexpected compression: {result.original_tokens} ‚Üí {result.compressed_tokens}")
        return False
    
    logger.info("‚úÖ No compression happened (as expected)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
    if len(compressed_messages) != original_count:
        logger.error(f"‚ùå Message count changed: {original_count} ‚Üí {len(compressed_messages)}")
        return False
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π API –≤—ã–∑–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    logger.info("Making final API call...")
    try:
        response = await call_llm_with_tools(
            model=model,
            messages=compressed_messages,
            tools=TEST_TOOLS,
            temperature=0,
            max_tokens=200,
        )
        logger.info(f"‚úÖ Final API call successful!")
        logger.info(f"   Response: {response.get('content', '')[:100]}...")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Final API call failed: {e}")
        return False


async def test_proactive_compression(model: str, model_name: str) -> bool:
    """
    –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ –° –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–º —Å–∂–∞—Ç–∏–µ–º (DeepSeek Reasoner, Gemini 3.0 Pro).
    
    1. –°—Ç—Ä–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω—ã–µ API –≤—ã–∑–æ–≤—ã (–ø–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–µ thought_signature)
    2. –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∂–∞—Ç–∏–µ
    3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ API —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è
    """
    logger.info("=" * 70)
    logger.info(f"TEST: {model_name} - Proactive compression")
    logger.info("=" * 70)
    
    # –°—Ç—Ä–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω—ã–µ –≤—ã–∑–æ–≤—ã
    logger.info("Building context with real API calls...")
    messages = await build_real_context(model, num_iterations=5, target_tokens=2000)
    
    original_count = len(messages)
    original_tokens = log_messages_summary(messages, "BEFORE:")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
    if original_tokens < 1000:
        logger.warning(f"‚ö†Ô∏è Context too small ({original_tokens} < 1000), compression won't trigger")
        logger.info("   Adding more messages...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –µ—â—ë —Å–æ–æ–±—â–µ–Ω–∏–π
        messages = await build_real_context(model, num_iterations=8, target_tokens=2500)
        original_tokens = log_messages_summary(messages, "EXTENDED:")
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä —Å –Ω–∏–∑–∫–∏–º–∏ –ª–∏–º–∏—Ç–∞–º–∏
    compressor = get_test_compressor(model)
    logger.info(f"Test limits: threshold={compressor.threshold_tokens}, target={compressor.target_tokens}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∂–∞—Ç–∏–µ
    compressed_messages, result = await compressor.check_and_compress(messages)
    
    if not result:
        logger.error(f"‚ùå Compression did not happen! Tokens: {original_tokens}")
        return False
    
    logger.info(f"‚úÖ Compression happened!")
    logger.info(f"   Original: {result.original_tokens} tokens, {result.messages_before} messages")
    logger.info(f"   Compressed: {result.compressed_tokens} tokens, {result.messages_after} messages")
    logger.info(f"   Saved: {result.tokens_saved} tokens ({100 - result.compression_ratio*100:.1f}%)")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∂–∞—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    logger.info("Compressed context structure:")
    for i, msg in enumerate(compressed_messages[:6]):
        role = msg.get("role", "?")
        content = str(msg.get("content", ""))[:50].replace("\n", " ")
        has_ts = "‚úìTS" if msg.get("thought_signature") else ""
        has_tc = "‚úìTC" if msg.get("tool_calls") else ""
        logger.info(f"   [{i}] {role}: {content}... {has_ts} {has_tc}")
    
    if len(compressed_messages) > 6:
        logger.info(f"   ... and {len(compressed_messages) - 6} more messages")
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –¢–ï–°–¢: API –≤—ã–∑–æ–≤ –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è
    logger.info("")
    logger.info("üî• CRITICAL TEST: API call after compression...")
    
    try:
        response = await call_llm_with_tools(
            model=model,
            messages=compressed_messages,
            tools=TEST_TOOLS,
            temperature=0,
            max_tokens=300,
        )
        
        logger.info(f"‚úÖ API call SUCCESSFUL after compression!")
        logger.info(f"   Response: {response.get('content', '')[:100]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–∂–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –¥–∏–∞–ª–æ–≥
        if response.get("tool_calls"):
            logger.info("   Model made tool calls, testing continuation...")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
            assistant_msg = extract_assistant_fields(response)
            compressed_messages.append(assistant_msg)
            
            # –î–æ–±–∞–≤–ª—è–µ–º tool results
            for tc in response["tool_calls"]:
                compressed_messages.append({
                    "role": "tool",
                    "tool_call_id": tc.get("id", "test"),
                    "name": tc.get("function", {}).get("name", "unknown"),
                    "content": "File analysis complete. Found 5 functions and 2 classes."
                })
            
            # –ï—â—ë –æ–¥–∏–Ω –≤—ã–∑–æ–≤
            response2 = await call_llm_with_tools(
                model=model,
                messages=compressed_messages,
                tools=TEST_TOOLS,
                temperature=0,
                max_tokens=200,
            )
            
            logger.info(f"‚úÖ Continuation also successful!")
            logger.info(f"   Response: {response2.get('content', '')[:80]}...")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå API call FAILED after compression!")
        logger.error(f"   Error: {e}")
        
        if is_context_overflow_error(e):
            logger.error("   This is a context overflow error")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ thought_signature –æ—à–∏–±–∫—É
        if "thought_signature" in str(e).lower():
            logger.error("   ‚ö†Ô∏è thought_signature issue detected!")
            logger.error("   Checking compressed messages for thought_signature...")
            
            for i, msg in enumerate(compressed_messages):
                if msg.get("role") == "assistant" and msg.get("tool_calls"):
                    has_ts = "YES" if msg.get("thought_signature") else "NO"
                    logger.error(f"      [{i}] assistant with tool_calls: thought_signature={has_ts}")
        
        return False


async def test_emergency_compression(model: str, model_name: str) -> bool:
    """
    –¢–µ—Å—Ç –∞–≤–∞—Ä–∏–π–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è.
    """
    logger.info("=" * 70)
    logger.info(f"TEST: {model_name} - Emergency compression")
    logger.info("=" * 70)
    
    # –°—Ç—Ä–æ–∏–º –±–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    logger.info("Building large context...")
    messages = await build_real_context(model, num_iterations=6, target_tokens=2500)
    
    original_tokens = log_messages_summary(messages, "BEFORE EMERGENCY:")
    
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∞–≤–∞—Ä–∏–π–Ω–æ–µ —Å–∂–∞—Ç–∏–µ
    compressor = get_test_compressor(model)
    compressed_messages, result = await compressor.emergency_compress(messages)
    
    logger.info(f"Emergency compression result:")
    logger.info(f"   Original: {result.original_tokens} tokens")
    logger.info(f"   Compressed: {result.compressed_tokens} tokens")
    logger.info(f"   Saved: {result.tokens_saved} tokens")
    
    log_messages_summary(compressed_messages, "AFTER EMERGENCY:")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ API —Ä–∞–±–æ—Ç–∞–µ—Ç
    logger.info("Testing API after emergency compression...")
    try:
        response = await call_llm_with_tools(
            model=model,
            messages=compressed_messages,
            tools=TEST_TOOLS,
            temperature=0,
            max_tokens=200,
        )
        logger.info(f"‚úÖ API call successful after emergency compression!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå API call failed: {e}")
        return False

# ============================================================================
# MAIN TEST RUNNER
# ============================================================================

async def run_all_tests():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤."""
    
    print("\n" + "=" * 70)
    print("üß™ CONTEXT COMPRESSION TEST SUITE (Real API Calls)")
    print(f"   Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70 + "\n")
    
    results = {}
    
    # =========================================================================
    # TEST 1: GPT-5.1 Codex Max - –ù–ï –¥–æ–ª–∂–Ω–∞ —Å–∂–∏–º–∞—Ç—å—Å—è
    # =========================================================================
    print("\n" + "üîµ" * 35)
    print("PHASE 1: GPT-5.1 Codex Max (NO compression)")
    print("üîµ" * 35 + "\n")
    
    try:
        results["gpt_codex"] = await test_no_proactive_compression(
            TEST_MODELS["gpt_codex"],
            "GPT-5.1 Codex Max"
        )
    except Exception as e:
        logger.error(f"Test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        results["gpt_codex"] = False
    
    await asyncio.sleep(2)
    
    # =========================================================================
    # TEST 2: DeepSeek Reasoner - –î–û–õ–ñ–ù–ê —Å–∂–∏–º–∞—Ç—å—Å—è
    # =========================================================================
    print("\n" + "üü°" * 35)
    print("PHASE 2: DeepSeek V3.2 Reasoner (WITH compression)")
    print("üü°" * 35 + "\n")
    
    try:
        results["deepseek_reasoner"] = await test_proactive_compression(
            TEST_MODELS["deepseek_reasoner"],
            "DeepSeek V3.2 Reasoner"
        )
    except Exception as e:
        logger.error(f"Test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        results["deepseek_reasoner"] = False
    
    await asyncio.sleep(2)
    
    # =========================================================================
    # TEST 3: Gemini 3.0 Pro - –î–û–õ–ñ–ù–ê —Å–∂–∏–º–∞—Ç—å—Å—è (—Å thought_signature!)
    # =========================================================================
    print("\n" + "üü¢" * 35)
    print("PHASE 3: Gemini 3.0 Pro (WITH compression + thought_signature)")
    print("üü¢" * 35 + "\n")
    
    try:
        results["gemini_3_pro"] = await test_proactive_compression(
            TEST_MODELS["gemini_3_pro"],
            "Gemini 3.0 Pro"
        )
    except Exception as e:
        logger.error(f"Test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        results["gemini_3_pro"] = False
    
    await asyncio.sleep(2)
    
    # =========================================================================
    # TEST 4: Emergency Compression
    # =========================================================================
    print("\n" + "üî¥" * 35)
    print("PHASE 4: Emergency Compression (DeepSeek)")
    print("üî¥" * 35 + "\n")
    
    try:
        results["emergency"] = await test_emergency_compression(
            TEST_MODELS["deepseek_reasoner"],
            "DeepSeek V3.2 Reasoner"
        )
    except Exception as e:
        logger.error(f"Test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        results["emergency"] = False
    
    # =========================================================================
    # SUMMARY
    # =========================================================================
    print("\n" + "=" * 70)
    print("üìä TEST RESULTS SUMMARY")
    print("=" * 70)
    
    for test_name, passed in results.items():
        status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
        print(f"   {test_name:25} : {status}")
    
    total_passed = sum(1 for v in results.values() if v)
    total_tests = len(results)
    
    print("-" * 70)
    print(f"   TOTAL: {total_passed}/{total_tests} tests passed")
    
    if total_passed == total_tests:
        print("\nüéâ ALL TESTS PASSED! Compression system is reliable.")
    else:
        print("\n‚ö†Ô∏è  Some tests failed. Check logs above for details.")
    
    print("=" * 70 + "\n")
    
    return all(results.values())

# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("üöÄ Context Compression Test Suite")
    print("   Using REAL API calls for all models")
    print("=" * 70)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á–∏
    print("\nüîë Checking API keys...")
    
    keys_status = {
        "ROUTERAI_API_KEY": bool(cfg.ROUTERAI_API_KEY),
        "DEEPSEEK_API_KEY": bool(cfg.DEEPSEEK_API_KEY),
        "OPENROUTER_API_KEY": bool(cfg.OPENROUTER_API_KEY),
    }
    
    all_keys_ok = True
    for key, present in keys_status.items():
        status = "‚úÖ" if present else "‚ùå"
        print(f"   {status} {key}")
        if not present:
            all_keys_ok = False
    
    if not all_keys_ok:
        print("\n‚ö†Ô∏è  Some API keys are missing!")
        print("   Set them in .env file.")
        sys.exit(1)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª–∏
    print("\nüìã Test models:")
    for key, model in TEST_MODELS.items():
        display_name = cfg.get_model_display_name(model)
        print(f"   {key}: {display_name}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª–∏–º–∏—Ç—ã
    print("\n‚öôÔ∏è  Test compression limits:")
    for model, limits in TEST_LIMITS.items():
        display_name = cfg.get_model_display_name(model)
        print(f"   {display_name}:")
        print(f"      threshold: {limits['threshold']} tokens")
        print(f"      target: {limits['target']} tokens")
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
    print("\nüí∞ NOTE: This test makes ~20-30 real API calls.")
    print("   Estimated cost: ~$0.10-0.50 depending on models.")
    
    print("\n" + "-" * 70)
    input("Press Enter to start tests...")
    
    success = asyncio.run(run_all_tests())
    
    sys.exit(0 if success else 1)