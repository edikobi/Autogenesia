{
  "version": "1.0",
  "advices": {
    "ADV-G01": {
      "title": "Bug Hunting Methodology",
      "content": "# ADV-G01: Bug Hunting Methodology\n\n## Core Principle\nA bug you see is often a SYMPTOM. Your job is to find the DISEASE and ALL its manifestations.\n\n## Thinking Framework\n\n### Phase 1: Localization (NOT fixing yet!)\nGoal: Find the EXACT point where data becomes incorrect.\n\n**Self-check questions:**\n- Where exactly does the error occur? (file, line, function)\n- What data comes IN to this point?\n- What data is expected OUT?\n- At what moment do actual and expected diverge?\n\n**Tool priority:**\n1. `read_file()` — read the ENTIRE file with error, not just the function\n2. `search_code()` — find all calls to the problematic function\n\n### Phase 2: Data Flow Tracing\nGoal: Understand WHERE the incorrect data came from.\n\n**Backward tracing technique:**\n1. Find the variable with wrong value\n2. Find where it gets this value (assignment)\n3. Find the source of that value (parameter? return from another function?)\n4. Repeat until you reach external source (API, DB, user input)\n\n**Key insight:** The bug is often 2-3 levels UP from where the error manifests.\n\n### Phase 3: Pattern Search (CRITICAL!)\nGoal: Find ALL places with the same bug pattern.\n\n**Mandatory checks:**\n- [ ] Similar code in the same file?\n- [ ] Similar code in related files (same module)?\n- [ ] Same pattern used elsewhere in project?\n\n**Use search_code() with:**\n- Function name that has the bug\n- Variable name that gets corrupted\n- Code pattern that causes the issue\n\n### Phase 4: Dependency Impact\nGoal: Understand who else will be affected by your fix.\n\n**Questions:**\n- Who calls the function you're fixing?\n- Will the contract change (signature, return type, exceptions)?\n- Do callers need updates?\n\n### Phase 5: Fix Formulation\nOnly AFTER phases 1-4!\n\n**Checklist before writing instruction:**\n- [ ] ROOT CAUSE stated explicitly (not just 'X was wrong')\n- [ ] All related locations found and listed\n- [ ] Dependencies checked\n- [ ] Fix doesn't break other code\n\n## Anti-patterns\n- Fixing first found location without checking others\n- Assuming cause without data tracing\n- Ignoring similar code in other files\n- Changing signature without checking callers"
    },
    "ADV-G02": {
      "title": "Feature Integration Thinking",
      "content": "# ADV-G02: Feature Integration Thinking\n\n## Core Principle\nNew code must INTEGRATE, not just EXIST. An isolated feature is technical debt from day one.\n\n## Thinking Framework\n\n### Phase 1: Pattern Discovery\nGoal: Understand HOW this project does things.\n\n**Before writing ANY code, answer:**\n- How are similar features structured in this project?\n- What naming conventions are used?\n- Where do similar components live (folders)?\n- What's the typical file structure for this type of feature?\n\n**Tool priority:**\n1. `search_code()` — find similar existing features\n2. `read_file()` — study their structure in detail\n\n### Phase 2: Integration Point Mapping\nGoal: Identify WHERE new code connects to existing system.\n\n**Connection types to identify:**\n- **Entry points:** How will the app reach this feature? (routes, CLI, events)\n- **Dependencies:** What existing code will this feature USE?\n- **Consumers:** What existing code might USE this feature?\n- **Configuration:** Does it need config values, env vars?\n\n**Key question:** Draw the dependency arrows. Do they all point in consistent direction?\n\n### Phase 3: Contract Design\nGoal: Define the PUBLIC interface before implementation.\n\n**Design decisions:**\n- What's the minimal public API surface?\n- What should be internal/private?\n- What types/interfaces define the contract?\n- How will other code discover and use this?\n\n**Rule:** If you can't explain the public API in 2-3 sentences, it's too complex.\n\n### Phase 4: Dependency Direction Check\nGoal: Ensure new code doesn't violate architectural boundaries.\n\n**Checks:**\n- [ ] Higher-level modules don't depend on lower-level details\n- [ ] No circular dependencies introduced\n- [ ] Core business logic doesn't depend on infrastructure\n\n### Phase 5: Implementation Order\nGoal: Plan the sequence of file creation/modification.\n\n**Order principle:** Create DEPENDENCIES before DEPENDENTS.\n1. First: standalone utilities, types, interfaces\n2. Then: core logic that uses those\n3. Last: integration points (routes, handlers, wiring)\n\n## Anti-patterns\n- Creating feature in isolation, then 'connecting' it\n- Ignoring existing patterns in favor of 'better' approach\n- Making everything public 'just in case'\n- Creating circular dependencies for convenience"
    },
    "ADV-G03": {
      "title": "Safe Refactoring Framework",
      "content": "# ADV-G03: Safe Refactoring Framework\n\n## Core Principle\nRefactoring changes STRUCTURE without changing BEHAVIOR. If behavior changes, it's not refactoring — it's modification.\n\n## Thinking Framework\n\n### Phase 1: Scope Definition\nGoal: Draw a clear boundary around what you're changing.\n\n**Define explicitly:**\n- Which files/classes/functions are IN scope?\n- Which are explicitly OUT of scope?\n- What behavior must remain IDENTICAL?\n\n**Tool priority:**\n1. `search_code()` — find all usages of code you're refactoring\n2. `read_file()` — understand full context of each usage\n\n### Phase 2: Contract Identification\nGoal: Identify the PUBLIC contracts that must not change.\n\n**Contracts include:**\n- Function/method signatures\n- Return types and value ranges\n- Exception types thrown\n- Side effects (what external state changes)\n- Timing/ordering guarantees\n\n**Rule:** If ANY consumer depends on a behavior, it's part of the contract.\n\n### Phase 3: Change Planning\nGoal: Break refactoring into ATOMIC, reversible steps.\n\n**Good refactoring step:**\n- Changes one thing\n- Can be tested immediately\n- Can be reverted if problems found\n- Doesn't require other steps to 'work'\n\n**Step ordering:**\n1. Preparatory changes (add new structure alongside old)\n2. Migration changes (move consumers to new structure)\n3. Cleanup changes (remove old structure)\n\n### Phase 4: Impact Verification\nGoal: Ensure all consumers still work.\n\n**For each consumer found in Phase 1:**\n- [ ] Does it still compile/parse?\n- [ ] Does it still get expected values?\n- [ ] Do its tests still pass?\n\n### Phase 5: Rollback Planning\nGoal: Know how to undo if things go wrong.\n\n**Before executing:**\n- Can each step be reverted independently?\n- What's the 'abort point' where reverting becomes complex?\n- How will you detect if something broke?\n\n## Anti-patterns\n- 'While I'm here, I'll also...' scope creep\n- Changing behavior 'slightly' during refactoring\n- Big-bang refactoring without intermediate states\n- Refactoring without knowing all consumers"
    },
    "ADV-G04": {
      "title": "Security Audit Checklist",
      "content": "# ADV-G04: Security Audit Checklist\n\n## Core Principle\nSecurity is about TRUST BOUNDARIES. Every time data crosses a boundary, it must be validated.\n\n## Thinking Framework\n\n### Phase 1: Trust Boundary Mapping\nGoal: Identify where untrusted data enters the system.\n\n**Common entry points:**\n- User input (forms, API requests, file uploads)\n- External API responses\n- Database reads (data may have been corrupted)\n- Configuration files (may be tampered)\n- Environment variables\n\n**Tool priority:**\n1. `search_code()` — find input handling, request parsing\n2. `read_file()` — examine validation logic\n\n### Phase 2: Injection Vector Analysis\nGoal: Find places where untrusted data is used in dangerous contexts.\n\n**Dangerous contexts:**\n- SQL queries (SQL injection)\n- Shell commands (command injection)\n- File paths (path traversal)\n- HTML output (XSS)\n- Deserialization (object injection)\n- URL construction (SSRF)\n\n**For each, ask:** Is input sanitized/escaped BEFORE use?\n\n### Phase 3: Authentication & Authorization\nGoal: Verify identity checks are complete and correct.\n\n**Check:**\n- [ ] Every protected endpoint verifies authentication\n- [ ] Authorization checks happen BEFORE action, not after\n- [ ] Failed auth returns generic error (no information leak)\n- [ ] Session/token handling is secure (expiry, invalidation)\n\n### Phase 4: Data Exposure Analysis\nGoal: Find unintentional information leaks.\n\n**Look for:**\n- Error messages with stack traces or internal details\n- Logs containing sensitive data\n- API responses with more data than needed\n- Debug endpoints left in production code\n- Hardcoded secrets (keys, passwords)\n\n### Phase 5: Defense Layers\nGoal: Verify defense-in-depth is applied.\n\n**Principle:** Single security check is single point of failure.\n\n**Check for:**\n- Input validation at entry point AND before use\n- Output encoding at data boundary AND at rendering\n- Multiple auth checks for sensitive operations\n\n## Anti-patterns\n- Trusting data because 'our frontend validates it'\n- Security by obscurity (hiding instead of fixing)\n- Validating input format but not content\n- Checking auth at route but not in business logic"
    },
    "ADV-G05": {
      "title": "Dependency Graph Analysis",
      "content": "# ADV-G05: Dependency Graph Analysis\n\n## Core Principle\nCode is a GRAPH of dependencies. Understanding the graph prevents cascading failures.\n\n## Thinking Framework\n\n### Phase 1: Node Identification\nGoal: List all components involved in the change.\n\n**Components include:**\n- Files that will be modified\n- Files that import modified files\n- Files that modified files import\n- Configuration that affects behavior\n\n**Tool priority:**\n1. `search_code()` — find all imports/usages\n2. `read_file()` — examine import statements in found files\n\n### Phase 2: Edge Mapping\nGoal: Draw the dependency arrows.\n\n**For each component:**\n- What does it DEPEND ON? (imports, uses)\n- What DEPENDS ON it? (imported by, used by)\n- What's the direction of data flow?\n\n**Visualization technique:**\n```\nLayer 0 (no deps): config, constants, types\nLayer 1 (deps on L0): utilities, helpers\nLayer 2 (deps on L1): services, business logic  \nLayer 3 (deps on L2): handlers, controllers\nLayer 4 (deps on L3): entry points, main\n```\n\n### Phase 3: Cycle Detection\nGoal: Find circular dependencies.\n\n**Circular dependency signs:**\n- Import A from B, import B from A\n- Import at function level (not top of file)\n- Late binding to avoid import errors\n\n**If found:** Break cycle by extracting shared code to lower layer.\n\n### Phase 4: Impact Radius Estimation\nGoal: Predict how far changes will propagate.\n\n**Questions:**\n- If I change X's interface, what must update?\n- If I change X's behavior, what might break?\n- What's the worst case propagation path?\n\n**Rule of thumb:** Changes in Layer 0-1 have wide impact. Changes in Layer 3-4 have narrow impact.\n\n### Phase 5: Change Strategy\nGoal: Plan changes to minimize disruption.\n\n**Strategies:**\n- Change lower layers first (they're depended upon)\n- Add new interface alongside old, migrate, then remove old\n- Use adapter pattern to isolate changes\n\n## Anti-patterns\n- Assuming 'small change' means 'small impact'\n- Ignoring transitive dependencies\n- Creating dependency on higher layer for convenience\n- Breaking circular dependency by adding MORE dependencies"
    },
    
    "ADV-G06": {
      "title": "Error Recovery Framework",
      "content": "# ADV-G06: Error Recovery Framework\n\n## Core Principle\nErrors are SIGNALS about mismatches between intent and implementation. Your job is to decode the signal and fix the root mismatch.\n\n## Thinking Framework\n\n### Phase 1: Error Classification\nGoal: Understand WHAT TYPE of error you're dealing with.\n\n**Error categories:**\n- **Syntax errors:** Code doesn't parse (missing brackets, typos)\n- **Import errors:** Module not found, circular imports\n- **Type errors:** Wrong types passed to functions\n- **Runtime errors:** Code runs but crashes (null reference, division by zero)\n- **Logic errors:** Code runs but produces wrong result\n- **Test failures:** Code runs but doesn't meet requirements\n- **Validation errors:** Code violates project standards\n\n**For each category, ask:**\n- Is this error BLOCKING (prevents any execution)?\n- Is this error TRANSIENT (might succeed on retry)?\n- Is this error SYSTEMATIC (will always occur)?\n\n### Phase 2: Signal Decoding\nGoal: Extract MAXIMUM information from error message.\n\n**What to extract:**\n- **Exact location:** File path, line number, function name\n- **Failed operation:** What was the code trying to do?\n- **Actual vs expected:** What did it get vs what it needed?\n- **Context:** What was the input/state when error occurred?\n\n**Tool priority:**\n1. Read the FULL error message (not just first line)\n2. `read_file()` — examine the exact line that failed\n3. `search_code()` — find related code that might cause this\n\n### Phase 3: Root Cause Analysis\nGoal: Find the FUNDAMENTAL reason, not the symptom.\n\n**Tracing technique:**\n1. Start at error location\n2. Ask: 'WHY did this fail?' (e.g., 'variable was None')\n3. Ask: 'WHY was it None?' (e.g., 'function returned None')\n4. Ask: 'WHY did function return None?' (e.g., 'missing error handling')\n5. Continue until you reach a DESIGN DECISION or MISSING KNOWLEDGE\n\n**Root cause types:**\n- **Misunderstood requirement:** Implemented wrong thing\n- **Incomplete specification:** Missing edge case handling\n- **Integration mismatch:** Wrong assumption about dependency\n- **Missing knowledge:** Didn't know about library constraint\n\n### Phase 4: Fix Strategy Selection\nGoal: Choose the RIGHT fix, not just the QUICK fix.\n\n**Strategy options:**\n\n**A. Minimal fix (for simple errors):**\n- Typo correction\n- Missing import addition\n- Syntax error fix\n- When to use: Error is localized, cause is obvious\n\n**B. Defensive fix (for runtime errors):**\n- Add validation before operation\n- Add error handling (try/catch)\n- Add null checks\n- When to use: Error is from external/unpredictable source\n\n**C. Redesign fix (for logic errors):**\n- Change algorithm\n- Restructure data flow\n- Split complex function\n- When to use: Current design fundamentally can't handle case\n\n**D. Requirement clarification (for test failures):**\n- Ask user for clarification\n- Review original requirement\n- Adjust implementation to match intent\n- When to use: Unclear what 'correct' behavior should be\n\n### Phase 5: Regression Prevention\nGoal: Ensure THIS error never happens again.\n\n**Prevention techniques:**\n- **Add test case:** Capture this scenario in tests\n- **Add validation:** Check input earlier in flow\n- **Add assertion:** Make assumption explicit in code\n- **Add documentation:** Explain constraint/requirement\n\n**Question:** If same input comes again, will it fail?\n\n## Iteration Strategy\n\nWhen errors occur DURING Agent Mode execution:\n\n**Iteration 1-2 (Quick fixes):**\n- Fix syntax errors\n- Add missing imports\n- Fix obvious typos\n- Goal: Get code to RUN\n\n**Iteration 3-4 (Defensive coding):**\n- Add error handling\n- Add input validation\n- Fix edge cases\n- Goal: Get code to HANDLE edge cases\n\n**Iteration 5+ (Redesign if needed):**\n- If still failing, STOP and analyze\n- Is the approach fundamentally wrong?\n- Do we need different algorithm/structure?\n- Goal: Get code to be ROBUST\n\n**Red flag:** If fixing one error creates new error in different place → design issue, not implementation issue.\n\n## Anti-patterns\n- Fixing error message instead of error cause\n- Adding try/catch without understanding what can fail\n- Assuming error is in generated code (could be in existing code)\n- Giving up after 2-3 iterations without systematic analysis"
    },
    
    "ADV-G07": {
      "title": "Validator Feedback Interpretation Framework",
      "content": "# ADV-G07: Validator Feedback Interpretation Framework\n\n## Core Principle\nValidator feedback is a diagnostic signal about the alignment between requirements, generated code, and system constraints. Your goal is to decode this signal to identify the root cause of misalignment.\n\n## Thinking Framework\n\n### Phase 1: Feedback Decomposition\nGoal: Break down the feedback into independent, atomic issues.\n\n**Key questions:**\n- Is this a syntax error (prevents execution)?\n- Is this a logical error (incorrect behavior)?\n- Is this an integration error (violates existing contracts)?\n- Is this a stylistic/architectural concern (works but poorly)?\n- Is this a misunderstanding of requirements by the Validator?\n\n**Tool priority:**\n1. Re-examine the generated code in full context.\n2. Map each issue to the original user requirement.\n\n### Phase 2: Severity and Priority Assessment\nGoal: Determine which issues must be fixed immediately and which can be deferred or challenged.\n\n**Critical issues (must fix):**\n- Syntax errors that prevent execution.\n- Violations of core user requirements.\n- Security vulnerabilities or data integrity issues.\n\n**Important issues (should fix in this cycle):**\n- Logical errors in primary functionality.\n- Breaking changes to existing contracts.\n\n**Minor issues (can defer or discuss):**\n- Stylistic preferences.\n- Optimizations without correctness impact.\n- Ambiguous requirements leading to subjective criticism.\n\n### Phase 3: Root Cause Analysis in the Generation Process\nGoal: Understand why the Generator produced this issue to provide a more precise correction.\n\n**Questions about your original instruction:**\n- Was the instruction ambiguous or incomplete?\n- Did I miss a critical requirement or constraint?\n- Was the provided context (semantic index) insufficient or misleading?\n- Was the task too complex for a single Generator iteration?\n\n**Outcome:** Determine whether to clarify the task, provide more context, or break it down.\n\n### Phase 4: Corrective Instruction Formulation\nGoal: Transform the analyzed feedback into a clear, actionable instruction for the Generator.\n\n**Instruction principles:**\n- Address the root cause, not just the symptom.\n- Rephrase the original task with refined understanding.\n- Specify the required changes in the context of the overall goal.\n- Reference relevant context from the project index.\n\n## Anti-patterns\n- Treating all feedback as equally urgent.\n- Blaming the Generator without examining your own instructions.\n- Applying local fixes without considering the broader system.\n- Ignoring feedback without substantive justification."
    },

    "ADV-G08": {
      "title": "Framework Contract Analysis",
      "content": "# ADV-G08: Framework Contract Analysis\n\n## Core Principle\nWhen code runs in a controlled environment (framework, library, runtime), errors may indicate a violation of implicit execution rules rather than logical bugs. The system expects to manage function calls and provide contextual arguments.\n\n## Analysis Protocol\n\n### Phase 1: Pattern Identification\n**Indicators for this error type:**\n- Missing argument names suggesting system-provided context: `ctx`, `self`, `request`, `event`, `state`, `loop`\n- Function decorated with framework-specific decorators (`@...`)\n- Error occurs when calling functions that belong to imported frameworks\n- Standard debugging suggests \"pass the argument\" but source code shows no obvious source\n\n**Decision:** If multiple indicators present, switch to framework contract analysis.\n\n### Phase 2: Environmental Context Discovery\n**Tool-based investigation sequence:**\n1. Use `read_file()` to examine the entire file containing the error\n   - Identify all decorators on the function\n   - Note framework imports\n   - Review file purpose (CLI command, view handler, event listener)\n\n2. Use `search_code()` to understand the ecosystem\n   - Find framework entry points in project (`main()`, `run()`, `start()`)\n   - Locate similar decorated functions and see how they're called\n   - Identify initialization patterns (where frameworks are configured)\n\n### Phase 3: Contract Hypothesis Formation\n**Mental model construction:**\n- Which system (framework/library) owns this function's lifecycle?\n- What is the expected invocation pattern for this type of function?\n- Where in the execution flow should the missing arguments originate?\n\n**Output:** A clear statement: \"Function X appears managed by framework Y, requiring invocation through Y's mechanisms.\"\n\n### Phase 4: Corrective Direction\n**Instruction principles for Generator:**\n- Focus investigation on framework entry points\n- Maintain existing function signatures and decorators\n- Identify how to trigger framework's natural execution flow\n- Reference discovered patterns from similar code\n\n## Integration with General Debugging\n- **Precedes ADV-G01 Phase 2:** Use this analysis when Phase 1 reveals pattern indicators\n- **Complements data flow analysis:** Examines control flow and ownership instead\n- **Returns to ADV-G01:** After identifying correct entry point, verify no regressions\n\n## Critical Checks\n- Never suggest modifying framework-managed function signatures\n- Never recommend removing essential decorators\n- Always verify against existing framework usage patterns in project"
    },

    "ADV-G09": {
      "title": "Code Generation Failure Recovery",
      "content": "# ADV-G09: Code Generation Failure Recovery\n\n## Core Principle\nA syntax error in generated code is rarely a 'typo' — it's usually a strategy failure. Don't ask the Generator to 'fix' a broken patch; force it to REWRITE the context.\n\n## Thinking Framework\n\n### Phase 1: Failure Pattern Recognition\nGoal: Identify if the Generator is struggling with the modification strategy.\n\n**Red Flags (Stop using PATCH immediately if you see):**\n- `IndentationError` or `unexpected indent`\n- Duplicate code blocks (e.g., two `def name():` or double decorators)\n- `SyntaxError` near the edges of a modification\n- Partial code artifacts (trailing `}` or `)`)\n\n**Diagnosis:**\nThe Generator tried to `PATCH` a file but couldn't correctly match the context or merge the text, likely because the file state in the context window differs slightly from reality or the patch logic is brittle.\n\n### Phase 2: Instruction Refinement (Self-Correction)\nGoal: Eliminate ambiguity that leads to lazy generation.\n\n**Check your previous instruction:**\n- Did you allow `PATCH` or `EDIT` mode implicitly?\n- Did you ask to \"change line X\" instead of \"implement logic Y\"?\n- Is the context window providing the *dirty* (broken) version of the file?\n\n**Action:**\nIf the file is already broken (has syntax errors), **NEVER** ask to patch it. The Generator will likely patch relative to the broken state, compounding the error.\n\n### Phase 3: Strategy Escalation\nGoal: Force a robust solution that ignores the broken state.\n\n**Escalation Ladder:**\n1. **Level 1 (Targeted Replace):** Switch from `PATCH` to `REPLACE_METHOD` / `REPLACE_CLASS`.\n   *Instruction:* \"Completely REWRITE the method `xyz`. Do not output a diff. Output the full new method body.\"\n\n2. **Level 2 (Full File Rewrite):** If the structure is heavily damaged.\n   *Instruction:* \"Regenerate the ENTIRE file `filename.py`. The current version has structural corruption.\"\n\n### Phase 4: Clean Context Enforcement\nGoal: Ensure the Generator ignores the 'bad' history.\n\n**Prompting Technique:**\n\"The current file contains syntax errors (duplicates/indentation). IGNORE the existing broken code in method `X`. Implement the Correct Logic from scratch based on requirements: [Requirements].\"\n\n## Recovery Checklist\n1. **Stop** trying to \"remove the extra line\".\n2. **Select** the enclosing scope (Function or Class).\n3. **Issue** a `REPLACE` command for that entire scope.\n4. **Verify** you provided all necessary imports/dependencies for the full rewrite.\n\n## Anti-patterns\n- Asking to \"fix the indentation\" via a patch (almost always fails).\n- Trying to remove a duplicate line by line number.\n- Ignoring `SyntaxError` hoping the next step will fix it."
    },


    "ADV-G10": {
      "title": "Instruction Compliance Recovery",
      "content": "# ADV-G10: Instruction Compliance Recovery\n\n## Core Principle\nWhen the Generator \"doesn't listen\", treat it as a spec-to-output alignment failure. The fastest fix is not rewriting code manually, but tightening the instruction so compliance becomes mechanically checkable.\n\n## When to Apply\nUse this advice if at least one is true:\n- The Generator changes files outside the requested scope.\n- The Generator applies only part of the requested modifications.\n- The Generator ignores \"Preserve\" constraints (removes/rewrites unrelated code).\n- The Generator produces the right idea but wrong integration point (wrong function/class/file).\n- The Generator keeps returning patches that don't match the existing file structure.\n\n## Strategy\n\n### Phase 1: Non-compliance Classification\nGoal: Identify the exact type of non-compliance.\n- **Scope drift:** touched extra files/sections.\n- **Partial execution:** missed a required change.\n- **Constraint violation:** broke \"Preserve\" rules.\n- **Format violation:** wrong output mode/structure.\n\nWrite down which of these happened in one sentence.\n\n### Phase 2: Convert Intent into Verifiable Acceptance Criteria\nGoal: Make correctness checkable without interpretation.\nAdd 3–7 short acceptance criteria that can be verified by reading the diff or running validator.\nExamples:\n- \"Only file X is modified\".\n- \"Method Y is fully replaced; no other methods changed\".\n- \"No duplicate decorators remain\".\n- \"Result must parse (no SyntaxError/IndentationError)\".\n\n### Phase 3: Reduce Degrees of Freedom (Clarity Tightening)\nGoal: Remove ambiguity that lets the Generator improvise.\nRefine the instruction using these levers:\n- **Explicit scope:** file + exact symbol (function/class) + boundaries (what starts/ends the change).\n- **Preserve list:** what must remain unchanged (imports, logging, public API, docstrings, behavior).\n- **Non-goals:** what must NOT be done (e.g., no refactoring, no renaming, no style cleanup) — keep this short.\n- **Dependency order:** if multiple files, specify order and why.\n- **Output contract:** what the Generator should output (full function/class vs minimal fragment) so it cannot omit context.\n\n### Phase 4: Add a Self-check Step (Generator-facing)\nGoal: Force the Generator to validate its own output against the instruction.\nAsk for a short checklist confirmation AFTER code generation:\n- \"List which files changed\".\n- \"Confirm preserve items were not changed\".\n- \"Confirm acceptance criteria satisfied\".\nThis does not require the Orchestrator to write code; it only adds a verification gate.\n\n### Phase 5: Escalation (When Iteration Fails)\nGoal: If the Generator keeps missing, increase instruction determinism.\n- Expand context: include the full current function/class (not just a snippet).\n- Upgrade from \"change these lines\" to \"replace the whole unit\" (function/class/file) when structure is unstable.\n- Split the task into smaller steps (one file or one symbol per generation) if multi-file alignment keeps failing.\n\n## Anti-patterns\n- Relying on line numbers without providing the full surrounding context.\n- Allowing broad wording like \"cleanup\" / \"refactor\" when only a bug fix is needed.\n- Treating repeated non-compliance as \"random\" instead of tightening acceptance criteria.\n- The Orchestrator manually writing implementation code instead of improving the instruction contract.\n"
    },
    
    "ADV-E01": {
      "title": "Caching Strategy Design",
      "content": "# ADV-E01: Caching Strategy Design\n\n## Core Principle\nCaching trades FRESHNESS for SPEED. Every cache decision is about acceptable staleness.\n\n## Thinking Framework\n\n### Phase 1: Cache Candidate Analysis\nGoal: Identify WHAT to cache and WHY.\n\n**Good cache candidates:**\n- Expensive computations (CPU-heavy)\n- Slow I/O operations (DB queries, API calls)\n- Frequently accessed, rarely changed data\n- Results that are identical for same inputs\n\n**Bad cache candidates:**\n- Data that changes frequently\n- Data that must be real-time\n- User-specific data with many variations\n- Results affected by side effects\n\n### Phase 2: Cache Location Selection\nGoal: Decide WHERE the cache lives.\n\n**Location options:**\n- In-memory (same process) — fastest, lost on restart\n- Shared memory (same machine) — survives restart\n- External store (separate service) — survives redeploy\n\n**Decision factors:**\n- How much data? (memory limits)\n- How many processes? (sharing needs)\n- What happens on cache loss? (rebuild cost)\n\n### Phase 3: Invalidation Strategy\nGoal: Decide WHEN cached data becomes stale.\n\n**Strategies:**\n- **Time-based (TTL):** Simple, but data may be stale\n- **Event-based:** Fresh data, but complex to implement\n- **Version-based:** Good for immutable data\n- **Manual:** Maximum control, maximum risk of staleness\n\n**Key question:** What's the MAXIMUM acceptable staleness?\n\n### Phase 4: Consistency Analysis\nGoal: Understand WHAT BREAKS if cache is wrong.\n\n**Scenarios to consider:**\n- Cache returns stale data during update\n- Cache miss during high load (thundering herd)\n- Cache becomes inconsistent with source\n- Partial cache invalidation leaves orphaned data\n\n### Phase 5: Failure Planning\nGoal: Design behavior when cache fails.\n\n**Questions:**\n- What if cache service is down?\n- What if cache returns corrupted data?\n- Can system function without cache (degraded mode)?\n\n## Anti-patterns\n- Caching without invalidation strategy\n- Caching user-specific data with global key\n- Caching mutable objects (shared state bugs)\n- Infinite TTL for changing data"
    },
    "ADV-E02": {
      "title": "Security Hardening Process",
      "content": "# ADV-E02: Security Hardening Process\n\n## Core Principle\nHardening is about REDUCING ATTACK SURFACE while maintaining functionality.\n\n## Thinking Framework\n\n### Phase 1: Attack Surface Inventory\nGoal: Map all ways an attacker could interact with system.\n\n**Inventory includes:**\n- Public endpoints (APIs, web pages)\n- Input fields and file upload points\n- Authentication mechanisms\n- Third-party integrations\n- Background jobs and schedulers\n\n**Tool priority:**\n1. `search_code()` — find route definitions, handlers\n2. `read_file()` — examine authentication/authorization logic\n\n### Phase 2: Threat Modeling\nGoal: For each surface, identify potential attacks.\n\n**Per entry point, ask:**\n- What can unauthenticated user do?\n- What can authenticated but unauthorized user do?\n- What happens with malformed input?\n- What happens with oversized input?\n- What information is leaked on error?\n\n### Phase 3: Defense Gap Analysis\nGoal: Find missing security controls.\n\n**Common gaps:**\n- Input validation present but incomplete\n- Authentication checked but authorization missing\n- Errors logged but with too much detail\n- Rate limiting absent or too permissive\n\n### Phase 4: Backward Compatibility Planning\nGoal: Ensure security changes don't break legitimate users.\n\n**Consider:**\n- Will stricter validation reject valid data?\n- Will new auth requirements lock out existing sessions?\n- Do clients need updates for new security headers?\n\n**Strategy:** Log violations before enforcing, analyze, then enforce.\n\n### Phase 5: Defense Layering\nGoal: Ensure no single point of security failure.\n\n**Layers to verify:**\n- Network layer (firewall, rate limiting)\n- Transport layer (TLS, certificate validation)\n- Application layer (input validation, output encoding)\n- Data layer (encryption, access controls)\n\n## Anti-patterns\n- Adding security that breaks existing clients\n- Implementing complex security for low-risk areas\n- Relying on single security check\n- Security through obscurity"
    },
    "ADV-E03": {
      "title": "Performance Optimization Approach",
      "content": "# ADV-E03: Performance Optimization Approach\n\n## Core Principle\nOptimize based on MEASUREMENT, not assumption. The bottleneck is rarely where you think.\n\n## Thinking Framework\n\n### Phase 1: Problem Definition\nGoal: Quantify the performance issue.\n\n**Define explicitly:**\n- What operation is slow? (specific endpoint, function)\n- How slow is it now? (measured, not perceived)\n- How fast should it be? (target with justification)\n- Under what conditions? (load, data size)\n\n**Rule:** 'It feels slow' is not a specification.\n\n### Phase 2: Bottleneck Identification\nGoal: Find WHERE time is spent.\n\n**Time categories:**\n- CPU (computation, serialization)\n- I/O (database, file system, network)\n- Wait (locks, queue, external service)\n- Memory (allocation, garbage collection)\n\n**Tool priority:**\n1. `read_file()` — examine slow code path\n2. `search_code()` — find all calls in the path\n\n### Phase 3: Root Cause Analysis\nGoal: Understand WHY the bottleneck exists.\n\n**Common causes by category:**\n\n**CPU:**\n- Inefficient algorithm (O(n²) instead of O(n))\n- Repeated computation (same calculation multiple times)\n- Expensive operations in loop\n\n**I/O:**\n- N+1 queries (query per item instead of batch)\n- Missing indexes (full table scan)\n- Uncompressed transfers\n\n**Wait:**\n- Lock contention (too coarse locking)\n- Sequential when could be parallel\n- Synchronous when could be async\n\n### Phase 4: Solution Design\nGoal: Fix root cause, not symptoms.\n\n**Optimization hierarchy (most to least impact):**\n1. Don't do it (can we eliminate the operation?)\n2. Do it less (can we reduce frequency?)\n3. Do it later (can we defer/batch?)\n4. Do it faster (algorithm improvement)\n5. Do it in parallel (if independent)\n\n### Phase 5: Verification\nGoal: Confirm optimization worked.\n\n**Measure AFTER optimization:**\n- Did target metric improve?\n- Did other metrics stay acceptable?\n- Does improvement hold under load?\n\n## Anti-patterns\n- Optimizing without measuring first\n- Optimizing rarely-executed code\n- Micro-optimizations vs algorithmic fixes\n- Caching as universal solution"
    },
    "ADV-E04": {
      "title": "Comprehensive Code Review",
      "content": "# ADV-E04: Comprehensive Code Review\n\n## Core Principle\nCode review is about RISK REDUCTION, not style enforcement.\n\n## Thinking Framework\n\n### Phase 1: Context Understanding\nGoal: Understand WHAT the code is supposed to do.\n\n**Before reading code:**\n- What problem does this solve?\n- What were the requirements?\n- What are the constraints?\n\n**Tool priority:**\n1. `read_file()` — read the complete file, not just changes\n2. `search_code()` — understand how it integrates\n\n### Phase 2: Correctness Verification\nGoal: Does code do what it claims?\n\n**Checklist:**\n- [ ] Logic matches stated purpose\n- [ ] All code paths lead to valid outcomes\n- [ ] Error conditions are handled\n- [ ] Edge cases are covered (null, empty, max values)\n- [ ] Concurrency is safe (if applicable)\n\n### Phase 3: Integration Analysis\nGoal: Does code work with rest of system?\n\n**Checklist:**\n- [ ] Interfaces match expectations of callers\n- [ ] Data formats match consumers\n- [ ] Dependencies are appropriate (no circular)\n- [ ] Changes don't break existing contracts\n\n### Phase 4: Risk Assessment\nGoal: What could go wrong?\n\n**Risk categories:**\n- **Functional:** Does wrong thing\n- **Performance:** Too slow under load\n- **Security:** Vulnerable to attack\n- **Operational:** Hard to debug/maintain\n\n**For each risk:** What's likelihood? What's impact?\n\n### Phase 5: Improvement Suggestions\nGoal: Make code better (optional, not blocking).\n\n**Categories:**\n- Readability (clearer names, better structure)\n- Maintainability (easier to change later)\n- Testability (easier to verify)\n- Performance (faster, but not blocking)\n\n**Rule:** Clearly separate 'must fix' from 'nice to have'.\n\n## Anti-patterns\n- Reviewing only changed lines, not context\n- Style nitpicking over functional issues\n- Approving without understanding\n- Blocking on preferences, not problems"
    },
    
    "ADV-E05": {
      "title": "Feature Addition Workflow",
      "content": "# ADV-E05: Feature Addition Workflow\n\n## Core Principle\nAdding a feature is not just writing code — it's INTEGRATING a new capability into an existing system without destabilizing it.\n\n## Thinking Framework\n\n### Phase 1: Requirement Extraction\nGoal: Understand WHAT is being requested and WHY.\n\n**Questions to answer:**\n- What problem does this feature solve?\n- What is the MINIMAL viable version?\n- What are the success criteria (how to know it works)?\n- What are the constraints (performance, compatibility)?\n\n**Clarification triggers:**\n- User says 'add X' without specifying behavior\n- Multiple interpretations possible\n- Edge cases not mentioned\n\n**Tool priority:**\n1. `search_code()` — find similar existing features\n2. `read_file()` — study how similar features work\n\n### Phase 2: Architecture Fit Assessment\nGoal: Determine WHERE this feature fits in existing structure.\n\n**Assessment questions:**\n- Does similar functionality already exist? (avoid duplication)\n- Which layer does this belong to? (UI, business logic, data)\n- What existing components will it interact with?\n- Does it fit existing patterns or need new pattern?\n\n**Integration points to identify:**\n- Entry points (how users/system trigger it)\n- Dependencies (what it needs from existing code)\n- Consumers (what might use this feature)\n- Configuration (env vars, settings it needs)\n\n### Phase 3: Incremental Implementation Planning\nGoal: Break feature into ATOMIC, testable steps.\n\n**Implementation order:**\n1. **Foundation:** Types, interfaces, data models\n2. **Core logic:** Business logic WITHOUT external dependencies\n3. **Integration:** Connect to database, APIs, file system\n4. **Entry points:** Add routes, CLI commands, event handlers\n5. **Error handling:** Add validation, error messages\n6. **Documentation:** Update README, add docstrings\n\n**Rule:** Each step should be independently testable.\n\n### Phase 4: Testing Strategy\nGoal: Verify feature works AND doesn't break existing code.\n\n**Test levels:**\n- **Unit tests:** Test core logic in isolation\n- **Integration tests:** Test interaction with dependencies\n- **Regression tests:** Verify existing features still work\n\n**Testing approach:**\n1. Write tests for NEW functionality\n2. Run EXISTING tests to ensure no breakage\n3. Add edge case tests\n4. Test error scenarios\n\n### Phase 5: Impact Verification\nGoal: Ensure feature integrates cleanly.\n\n**Checklist:**\n- [ ] No new circular dependencies introduced\n- [ ] No existing tests broken\n- [ ] No performance degradation\n- [ ] No security vulnerabilities introduced\n- [ ] Error messages are clear and actionable\n- [ ] Configuration is documented\n\n**Tool usage:**\n- Use `run_project_tests` to verify existing tests pass\n- Use validation to check imports and syntax\n\n## Common Scenarios\n\n### Scenario A: Add new endpoint/route\n1. Define request/response types\n2. Implement handler logic\n3. Add route registration\n4. Add validation middleware\n5. Add tests for success and error cases\n\n### Scenario B: Add new data processing function\n1. Define input/output types\n2. Implement core algorithm\n3. Add error handling for edge cases\n4. Add unit tests with various inputs\n5. Integrate into existing pipeline\n\n### Scenario C: Add new configuration option\n1. Add to config schema/class\n2. Add default value\n3. Update config loading logic\n4. Update code that uses this config\n5. Document in README/config file\n\n## Anti-patterns\n- Implementing entire feature in one step (hard to debug)\n- Adding feature without tests (can't verify it works)\n- Modifying existing code unnecessarily (scope creep)\n- Skipping requirement clarification (build wrong thing)\n- Not checking existing tests (break other features)"
    },
    
    "ADV-E06": {
      "title": "GUI Feature Integration",
      "content": "# ADV-E06: GUI Feature Integration\n\n## Core Principle\nA new UI feature must look and behave as if it was there from Day 1. Conformity to existing patterns is more important than local optimization.\n\n## Thinking Framework\n\n### Phase 1: Visual Inventory & Pattern Matching\nGoal: Identify reusable components and styles to ensure consistency.\n\n**Before writing code, scan the codebase:**\n- **Primitive matching:** Does a button/input/card like this already exist? Use it.\n- **Style inheritance:** How are margins/padding defined? (Utility classes? Theme variables? Hardcoded?)\n- **Layout strategy:** Does the parent container use Grid, Flex, Stack, or absolute positioning?\n\n**Rule:** Never define a new style or component if an existing one can be parameterized.\n\n### Phase 2: State Hierarchy Insertion\nGoal: Determine where the new data lives in the existing state tree.\n\n**Placement analysis:**\n- Does this feature need NEW state, or does it display EXISTING state?\n- If new state: Should it live in the component (local) or in the global store?\n- How will this state be injected? (Props? Context? Global access?)\n\n**Key check:** Avoid creating parallel state sources for the same data.\n\n### Phase 3: Layout Impact Analysis\nGoal: Predict how the new element affects surrounding elements.\n\n**Questions:**\n- Will adding this push other content off-screen?\n- How does this behave on window resize?\n- Does the parent container allow expansion?\n- Does this break the tab order or focus flow?\n\n### Phase 4: Interaction & Event Wiring\nGoal: Connect the visual element to the application logic.\n\n**Wiring plan:**\n- **Trigger:** What user action initiates the flow?\n- **Handler:** Does an existing handler exist, or do we need a new one?\n- **Feedback:** How does the UI signal that the action is processing?\n\n### Phase 5: Implementation Strategy\nGoal: Integrate incrementally.\n\n1. **Skeleton:** Insert the component with mock data to verify layout/positioning.\n2. **Styling:** Apply project-specific styles to match the look.\n3. **Wiring:** Connect real state and event handlers.\n4. **Cleanup:** Remove mock data and hardcoded values.\n\n## Anti-patterns\n- Creating a `CustomButton` when `AppButton` exists\n- Hardcoding colors (e.g., `#000`) instead of using theme tokens (e.g., `theme.primary`)\n- Placing a new component where it forces a scrollbar unintentionally\n- Copy-pasting a large block of code just to change one label\n- Ignoring the project's existing state management library"
    },

    "ADV-E07": {
      "title": "GUI Debugging & Repair",
      "content": "# ADV-E07: GUI Debugging & Repair\n\n## Core Principle\nUI bugs are either PROJECTION errors (wrong data displayed) or INTERACTION errors (events not firing). Isolate the layer before fixing.\n\n## Thinking Framework\n\n### Phase 1: Layer Isolation (The \"Where\")\nGoal: Determine if the bug is in the Logic, the State, or the View.\n\n**Diagnostic questions:**\n1. **Data Check:** Is the underlying variable/model correct? (Print/Log the state)\n   - *If data is wrong:* It's a Logic/State bug. Stop looking at UI code.\n   - *If data is right:* It's a View/Render bug. Continue.\n2. **Structure Check:** Is the component actually rendered in the DOM/Tree?\n3. **Visibility Check:** Is it rendered but hidden (size 0, wrong color, behind other layer)?\n\n### Phase 2: Event Flow Tracing\nGoal: Identify broken links in the interaction chain.\n\n**Chain of custody:**\n1. **Trigger:** Did the user action actually fire the event? (Log on click)\n2. **Propagation:** Did the event reach the handler?\n3. **Mutation:** Did the handler request a state change?\n4. **Reaction:** Did the state change trigger a re-render?\n\n**Identify the break point:** Where did the chain stop?\n\n### Phase 3: Layout & Styling Debugging\nGoal: Fix visual glitches without breaking responsiveness.\n\n**Common culprits:**\n- **Overflow:** Content is larger than container (check `overflow`, `wrap`, `clip`).\n- **Alignment:** Flex/Grid properties mismatch.\n- **Z-Index:** Component is buried under another.\n- **Constraints:** Parent has fixed size, child tries to expand.\n\n**Strategy:** Use outline/border debugging (color borders of containers) to see actual boundaries.\n\n### Phase 4: Component Boundary Verification\nGoal: Ensure data passes correctly between components.\n\n**Checklist:**\n- Are props/arguments passing the correct types?\n- Is the child component memoized/cached and refusing to update?\n- Is the context/provider available at this level of the tree?\n\n### Phase 5: Regression Prevention\nGoal: Fix locally, protect globally.\n\n**Before committing:**\n- Did fixing this alignment break the mobile view?\n- Did changing the color affect other buttons using the same class?\n- Does the tab key still navigate correctly?\n\n## Anti-patterns\n- Adding `!important` or fixed pixel sizes to force a visual fix (technical debt)\n- Changing global styles to fix one specific button\n- Assuming the click handler works without logging it\n- \"Blindly\" changing layout parameters until it looks right\n- Restarting the app hoping it's a transient glitch without finding root cause"
    },

    "ADV-N01": {
      "title": "Distributed System Design",
      "content": "# ADV-N01: Distributed System Design\n\n## Core Principle\nDistributed systems fail PARTIALLY. Design for degraded operation, not just happy path.\n\n## Thinking Framework\n\n### Phase 1: Component Identification\nGoal: Define the separate pieces and their responsibilities.\n\n**For each component, define:**\n- What single responsibility does it have?\n- What data does it own?\n- What operations does it expose?\n- What are its dependencies?\n\n**Rule:** If a component has multiple reasons to change, split it.\n\n### Phase 2: Communication Design\nGoal: Define how components talk to each other.\n\n**Communication patterns:**\n- **Synchronous (request/response):** Simple, but creates coupling\n- **Asynchronous (messages/events):** Decoupled, but complex\n- **Shared data:** Simple access, but coordination needed\n\n**For each interaction:**\n- What happens if receiver is slow?\n- What happens if receiver is down?\n- What happens if message is lost?\n\n### Phase 3: Consistency Strategy\nGoal: Decide what consistency guarantees you need.\n\n**Consistency spectrum:**\n- Strong (all see same data) — slow, simple to reason\n- Eventual (all converge to same data) — fast, complex to reason\n\n**Per data type, ask:**\n- What's the cost of stale read?\n- What's the cost of lost write?\n- What's acceptable inconsistency window?\n\n### Phase 4: Failure Mode Analysis\nGoal: Design behavior when things break.\n\n**Failure types:**\n- Component crash (process dies)\n- Network partition (can't reach component)\n- Slow response (timeout boundary)\n- Corrupted data (validation failure)\n\n**For each failure:**\n- How is it detected?\n- What's the fallback behavior?\n- How does system recover?\n\n### Phase 5: Operational Design\nGoal: Make system observable and manageable.\n\n**Requirements:**\n- How to know if component is healthy?\n- How to trace request across components?\n- How to deploy updates without downtime?\n\n## Anti-patterns\n- Assuming network is reliable\n- Treating remote call like local call\n- Single point of failure for critical path\n- Ignoring partial failure scenarios"
    },
    "ADV-N02": {
      "title": "Real-time Feature Architecture",
      "content": "# ADV-N02: Real-time Feature Architecture\n\n## Core Principle\nReal-time is about PERCEPTION of immediacy, not actual instant delivery.\n\n## Thinking Framework\n\n### Phase 1: Update Model Selection\nGoal: Choose how clients receive updates.\n\n**Models:**\n- **Polling:** Client asks periodically (simple, wasteful)\n- **Long polling:** Client waits for update (less waste, complex)\n- **Server-sent events:** Server pushes to client (one-way, reliable)\n- **WebSocket:** Bidirectional channel (flexible, complex)\n\n**Decision factors:**\n- How frequent are updates?\n- Do clients need to send data back?\n- How many concurrent connections?\n\n### Phase 2: State Synchronization\nGoal: Keep client and server state consistent.\n\n**Challenges:**\n- Client may miss updates while disconnected\n- Multiple updates may arrive out of order\n- Same update may arrive multiple times\n\n**Solutions:**\n- Sequence numbers (detect missing/reordering)\n- Idempotent updates (safe to replay)\n- Full state sync on reconnect\n\n### Phase 3: Conflict Resolution\nGoal: Handle simultaneous updates from multiple sources.\n\n**Strategies:**\n- **Last write wins:** Simple, data may be lost\n- **First write wins:** Prevents overwrite, may frustrate users\n- **Merge:** Combine changes, complex logic\n- **User resolution:** Ask user to choose, slow\n\n**Key question:** What happens if two users edit same thing at same time?\n\n### Phase 4: Connection Lifecycle\nGoal: Handle connection states gracefully.\n\n**States to handle:**\n- Connecting (show loading)\n- Connected (normal operation)\n- Reconnecting (show status, queue changes)\n- Disconnected (offline mode?)\n\n**For each transition:**\n- What does user see?\n- What happens to pending changes?\n- How to recover state?\n\n### Phase 5: Scalability Planning\nGoal: Design for growth in connections and messages.\n\n**Considerations:**\n- How many concurrent connections per server?\n- How to route updates to right connections?\n- How to fan out to many subscribers?\n\n## Anti-patterns\n- Treating WebSocket like stateless HTTP\n- Assuming updates always arrive in order\n- No offline handling strategy\n- Broadcasting everything to everyone"
    },
    "ADV-N03": {
      "title": "Data Pipeline Construction",
      "content": "# ADV-N03: Data Pipeline Construction\n\n## Core Principle\nPipelines are about RELIABLE TRANSFORMATION, not just moving data from A to B.\n\n## Thinking Framework\n\n### Phase 1: Stage Definition\nGoal: Break pipeline into discrete, testable stages.\n\n**Each stage should:**\n- Have single responsibility (extract, transform, or load)\n- Be independently deployable\n- Be independently testable\n- Have clear input/output contract\n\n**Stage types:**\n- **Source:** Reads from external system\n- **Transform:** Modifies data\n- **Sink:** Writes to external system\n\n### Phase 2: Error Handling Design\nGoal: Decide what happens when processing fails.\n\n**Failure handling strategies:**\n- **Stop:** Halt pipeline, manual intervention\n- **Skip:** Log error, continue with next item\n- **Retry:** Try again with backoff\n- **Dead letter:** Move to separate queue for later\n\n**Per stage, decide:**\n- What errors are transient (retry)?\n- What errors are permanent (skip/dead letter)?\n- What errors are critical (stop)?\n\n### Phase 3: Idempotency Design\nGoal: Make pipeline safe to re-run.\n\n**Idempotency means:** Running twice produces same result as running once.\n\n**Techniques:**\n- Use natural keys for upsert (insert or update)\n- Track processed items (don't reprocess)\n- Make transforms deterministic (same input = same output)\n\n**Test:** What happens if you run same data through twice?\n\n### Phase 4: Backpressure Handling\nGoal: Handle when downstream is slower than upstream.\n\n**Scenarios:**\n- Source produces faster than sink can write\n- Transform is slower than data arrival\n- Destination system is temporarily slow\n\n**Solutions:**\n- Buffer with size limit\n- Drop oldest when buffer full\n- Slow down source (flow control)\n- Scale up slow stage\n\n### Phase 5: Monitoring Design\nGoal: Know pipeline health at all times.\n\n**Key metrics:**\n- Items processed per time unit (throughput)\n- Time from input to output (latency)\n- Items failed vs succeeded (error rate)\n- Queue/buffer sizes (backpressure indicator)\n\n**Alerts:**\n- Throughput drops below threshold\n- Error rate exceeds threshold\n- Latency exceeds SLA\n\n## Anti-patterns\n- Monolithic pipeline (all stages in one process)\n- Silent failures (error swallowed without logging)\n- Non-idempotent transforms (unsafe to retry)\n- No backpressure handling (memory exhaustion)"
    },
    "ADV-N04": {
      "title": "Greenfield Project Design",
      "content": "# ADV-N04: Greenfield Project Design\n\n## Core Principle\nGreenfield projects offer freedom, but freedom without structure leads to chaos. Design for EVOLUTION, not just initial requirements.\n\n## Thinking Framework\n\n### Phase 1: Requirement Mining\nGoal: Extract COMPLETE picture of what system must do.\n\n**Questions to ask user:**\n- What is the PRIMARY purpose of this application?\n- Who are the users? (end users, admins, APIs)\n- What are the CORE workflows? (3-5 main use cases)\n- What are the constraints? (performance, scale, deployment)\n- What does SUCCESS look like? (measurable criteria)\n\n**Output:** Written list of requirements, prioritized (must-have vs nice-to-have)\n\n### Phase 2: Architecture Selection\nGoal: Choose HIGH-LEVEL structure that fits requirements.\n\n**Architecture patterns:**\n\n**Monolith (single process):**\n- When: Small-medium app, single team, simple deployment\n- Structure: Layered (presentation, business, data)\n- Example: Flask/FastAPI app with SQLite\n\n**Modular monolith:**\n- When: Medium app, multiple teams, clear boundaries\n- Structure: Modules with defined interfaces\n- Example: Django with apps, or Node.js with service layer\n\n**Microservices:**\n- When: Large app, multiple teams, independent scaling\n- Structure: Separate services with APIs\n- Example: Docker containers with orchestration\n\n**Serverless:**\n- When: Event-driven, variable load, minimal ops\n- Structure: Functions as services\n- Example: AWS Lambda, Google Cloud Functions\n\n**Decision factors:**\n- Team size and structure\n- Expected scale\n- Deployment infrastructure\n- Operational complexity tolerance\n\n### Phase 3: Technology Stack Decision\nGoal: Select frameworks, languages, databases.\n\n**Stack components:**\n- **Language:** Python, JavaScript, Go, etc.\n- **Framework:** Django, FastAPI, Express, etc.\n- **Database:** PostgreSQL, MongoDB, etc.\n- **Cache:** Redis, Memcached\n- **Message queue:** RabbitMQ, Kafka\n- **Deployment:** Docker, Kubernetes, serverless\n\n**Decision criteria:**\n- Team expertise\n- Requirement fit\n- Ecosystem maturity\n- Operational complexity\n\n### Phase 4: Project Structure Design\nGoal: Organize code for clarity and growth.\n\n**Structure principles:**\n- Separation of concerns (layers)\n- Clear module boundaries\n- Scalable organization\n- Easy to navigate\n\n**Example structure (monolith):**\n~~~\nproject/\n├── src/\n│   ├── api/           # HTTP endpoints\n│   ├── services/      # Business logic\n│   ├── models/        # Data models\n│   ├── repositories/  # Data access\n│   └── utils/         # Helpers\n├── tests/\n├── docs/\n├── config/\n└── requirements.txt\n~~~\n\n### Phase 5: Error Handling Strategy\nGoal: Plan how to handle failures.\n\n**Error handling approach:**\n- **Validation:** Check inputs early\n- **Exceptions:** Use typed exceptions\n- **Logging:** Log errors with context\n- **Recovery:** Graceful degradation\n- **Monitoring:** Alert on critical errors\n\n**Error categories to handle:**\n- Input validation errors\n- Database errors\n- External API errors\n- Resource exhaustion\n- Concurrency issues\n\n### Phase 6: Testing Approach\nGoal: Plan testing strategy from start.\n\n**Testing pyramid:**\n- **Unit tests:** 70% - Test individual functions\n- **Integration tests:** 20% - Test component interaction\n- **End-to-end tests:** 10% - Test complete workflows\n\n**Testing setup:**\n- Test framework selection\n- Test database setup\n- Mock external services\n- CI/CD integration\n- Coverage targets\n\n## Implementation Sequence\n\n**Phase 1: Foundation (Week 1)**\n1. Create project structure\n2. Set up version control\n3. Configure development environment\n4. Create README with vision\n\n**Phase 2: Core (Week 2-3)**\n1. Implement data models\n2. Implement core business logic\n3. Add comprehensive tests\n4. Set up error handling\n\n**Phase 3: Integration (Week 4)**\n1. Add API/entry points\n2. Add database integration\n3. Add logging and monitoring\n4. Add configuration management\n\n**Phase 4: Polish (Week 5+)**\n1. Add documentation\n2. Performance optimization\n3. Security review\n4. Deployment setup\n\n## Anti-patterns\n- Starting with database schema before understanding requirements\n- Choosing technology based on hype, not fit\n- Building without tests from start\n- Over-engineering for scale that won't happen\n- Ignoring error handling until end\n- Creating monolith when microservices needed (or vice versa)"
    },
  
    "ADV-N05": {
      "title": "GUI Application Architecture",
      "content": "# ADV-N05: GUI Application Architecture\n\n## Core Principle\nGUI is a PROJECTION of application state. Design the state model first, then derive the presentation layer. Avoid encoding business logic in visual components.\n\n## Thinking Framework\n\n### Phase 1: State Model Design\nGoal: Define WHAT data drives the interface, independent of HOW it looks.\n\n**Questions to answer:**\n- What is the minimal state needed to render the entire UI?\n- Which state is local to a component vs shared across the app?\n- What are the state transitions? (user actions, async operations, errors)\n- Which state is persistent vs ephemeral?\n\n**State categories:**\n- **Application state:** Global data (user session, app config)\n- **View state:** Current screen, navigation stack\n- **Component state:** Local UI state (expanded/collapsed, selected item)\n- **Form state:** Input values, validation status\n- **Async state:** Loading indicators, error messages\n\n**Output:** Written state schema before creating any visual components.\n\n### Phase 2: Component Hierarchy Planning\nGoal: Organize UI into a TREE of responsibilities.\n\n**Hierarchy design principles:**\n- Components should have single responsibility\n- Data flows DOWN (parent to child)\n- Events flow UP (child to parent)\n- Shared state lives in common ancestor\n\n**Component types:**\n- **Container components:** Manage state, handle logic\n- **Presentation components:** Pure rendering, no state\n- **Layout components:** Positioning, spacing, structure\n- **Utility components:** Reusable primitives (buttons, inputs)\n\n**Decision questions:**\n- What are the major screen sections?\n- Which components are reused across screens?\n- Where does each piece of state naturally belong?\n- Which components should be stateless?\n\n### Phase 3: Interaction Flow Mapping\nGoal: Define HOW user actions transform state.\n\n**Flow elements:**\n- **User events:** Clicks, input, gestures\n- **State updates:** What changes in response\n- **Side effects:** API calls, navigation, storage\n- **Feedback:** Visual confirmation, error display\n\n**Critical paths to map:**\n- Primary user workflows (3-5 most common tasks)\n- Error scenarios (validation failures, network errors)\n- Edge cases (empty states, loading states)\n\n**Anti-patterns to avoid:**\n- Components directly modifying state they don't own\n- Business logic scattered across multiple components\n- Event handlers duplicated in multiple places\n\n### Phase 4: Visual Consistency Strategy\nGoal: Ensure coherent design without hardcoding values.\n\n**Consistency mechanisms:**\n- **Design tokens:** Colors, spacing, typography as constants\n- **Component variants:** Defined styles (primary/secondary button)\n- **Layout patterns:** Standard spacing, alignment rules\n- **Theming support:** Light/dark mode, accessibility\n\n**Organization approach:**\n- Central theme/style configuration\n- Components reference theme values\n- Avoid inline style values\n- Use semantic names (not visual descriptions)\n\n**Example structure (conceptual):**\n~~~\ntheme/\n  ├── colors.py       # Color palette\n  ├── spacing.py      # Size constants\n  └── typography.py   # Font definitions\ncomponents/\n  ├── base/          # Primitive components\n  └── composite/     # Complex components\n~~~\n\n### Phase 5: Separation Verification\nGoal: Ensure clean boundaries between layers.\n\n**Layer boundaries:**\n- **State layer:** Pure data, no UI dependencies\n- **Logic layer:** State transformations, validation\n- **Presentation layer:** Rendering, event handling\n- **Style layer:** Visual appearance\n\n**Verification checklist:**\n- [ ] Can state logic be tested without rendering UI?\n- [ ] Can components be rendered with mock data?\n- [ ] Can visual styles change without touching logic?\n- [ ] Can layout change without rewriting components?\n\n**Red flags:**\n- UI components containing business rules\n- State manipulation inside render methods\n- Hardcoded pixel values in component code\n- Direct API calls from UI components\n\n### Phase 6: Accessibility and Edge Cases\nGoal: Design for ALL users and states.\n\n**Accessibility considerations:**\n- Keyboard navigation support\n- Screen reader compatibility\n- Color contrast requirements\n- Text scaling behavior\n\n**Edge state design:**\n- Empty states (no data to display)\n- Loading states (waiting for data)\n- Error states (operation failed)\n- Disabled states (action unavailable)\n\n**Questions:**\n- Can all actions be performed via keyboard?\n- Are loading indicators visible?\n- Are error messages actionable?\n- Does UI handle window resize gracefully?\n\n## Implementation Sequence\n\n**Phase 1: State Foundation**\n1. Define state models (data classes/types)\n2. Implement state management (store/context)\n3. Write state transformation logic\n4. Add state persistence if needed\n\n**Phase 2: Component Structure**\n1. Create primitive components (buttons, inputs)\n2. Create layout components (containers, grids)\n3. Create composite components (forms, lists)\n4. Create screen-level components\n\n**Phase 3: Integration**\n1. Connect components to state\n2. Wire event handlers\n3. Add navigation logic\n4. Implement error handling\n\n**Phase 4: Polish**\n1. Apply consistent theming\n2. Add loading/empty states\n3. Implement accessibility features\n4. Test edge cases\n\n## Anti-patterns\n- Starting with visual mockup before defining state model\n- Creating monolithic components that do everything\n- Passing raw state objects deep into component tree\n- Mixing business logic with rendering code\n- Hardcoding styles instead of using theme system\n- Ignoring loading/error states until 'later'\n- Building for ideal case only, ignoring edge cases"
    }
  
  }
}